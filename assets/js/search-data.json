{
  
    
        "post0": {
            "title": "How to get started on Kaggle Competitions",
            "content": "1. Understand the Data . The first step when you face a new data set is to take some time to know the data. In Kaggle competitions, you&#39;ll come across something like the sample below. . . On the competition&#39;s page, you can check the project description on Overview and you&#39;ll find useful information about the data set on the tab Data. In Kaggle competitions, it&#39;s common to have the training and test sets provided in separate files. On the same tab, there&#39;s usually a summary of the features you&#39;ll be working with and some basic statistics. It&#39;s crucial to understand which problem needs to be addressed and the data set we have at hand. . You can use the Kaggle notebooks to execute your projects, as they are similar to Jupyter Notebooks. . 2. Import the necessary libraries and data set . 2.1. Libraries . The libraries used in this project are the following. . import pandas as pd # Data analysis tool import numpy as np # Package for scientific computing from sklearn.model_selection import train_test_split # Splits arrays or matrices into random train and test subsets from sklearn.model_selection import KFold # Cross-validator from sklearn.model_selection import cross_validate # Evaluate metrics by cross-validation from sklearn.model_selection import GridSearchCV # Search over specified parameter values for an estimator from sklearn.compose import ColumnTransformer # Applies transformers to columns of DataFrames from sklearn.pipeline import Pipeline # Helps building a chain of transforms and estimators from sklearn.impute import SimpleImputer # Imputation transformer for completing missing values from sklearn.preprocessing import OneHotEncoder # Encode categorical features from sklearn.metrics import mean_absolute_error # One of many statistical measures of error from xgboost import XGBRegressor # Our model estimator . 2.2. Data set . The next step is to read the data set into a pandas DataFrame and obtain target vector y, which will be the column SalePrice, and predictors X, which, for now, will be the remaining columns. . X_full = pd.read_csv(&#39;https://raw.githubusercontent.com/rmpbastos/data_sets/main/housing_price_train.csv&#39;, index_col=&#39;Id&#39;) X_test_full = pd.read_csv(&#39;https://raw.githubusercontent.com/rmpbastos/data_sets/main/housing_price_test.csv&#39;, index_col=&#39;Id&#39;) # Obtain target vectors and predictors X = X_full.copy() y = X.SalePrice X.drop([&#39;SalePrice&#39;], axis=1, inplace=True) . To get an overview of the data, let&#39;s check the first rows and the size of the data set. . X.head() . MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating HeatingQC CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition . Id . 1 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2003 | 2003 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 196.0 | Gd | TA | PConc | Gd | TA | No | GLQ | 706 | Unf | 0 | 150 | 856 | GasA | Ex | Y | SBrkr | 856 | 854 | 0 | 1710 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 8 | Typ | 0 | NaN | Attchd | 2003.0 | RFn | 2 | 548 | TA | TA | Y | 0 | 61 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | . 2 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | FR2 | Gtl | Veenker | Feedr | Norm | 1Fam | 1Story | 6 | 8 | 1976 | 1976 | Gable | CompShg | MetalSd | MetalSd | None | 0.0 | TA | TA | CBlock | Gd | TA | Gd | ALQ | 978 | Unf | 0 | 284 | 1262 | GasA | Ex | Y | SBrkr | 1262 | 0 | 0 | 1262 | 0 | 1 | 2 | 0 | 3 | 1 | TA | 6 | Typ | 1 | TA | Attchd | 1976.0 | RFn | 2 | 460 | TA | TA | Y | 298 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | . 3 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2001 | 2002 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 162.0 | Gd | TA | PConc | Gd | TA | Mn | GLQ | 486 | Unf | 0 | 434 | 920 | GasA | Ex | Y | SBrkr | 920 | 866 | 0 | 1786 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 6 | Typ | 1 | TA | Attchd | 2001.0 | RFn | 2 | 608 | TA | TA | Y | 0 | 42 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | . 4 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | Crawfor | Norm | Norm | 1Fam | 2Story | 7 | 5 | 1915 | 1970 | Gable | CompShg | Wd Sdng | Wd Shng | None | 0.0 | TA | TA | BrkTil | TA | Gd | No | ALQ | 216 | Unf | 0 | 540 | 756 | GasA | Gd | Y | SBrkr | 961 | 756 | 0 | 1717 | 1 | 0 | 1 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Detchd | 1998.0 | Unf | 3 | 642 | TA | TA | Y | 0 | 35 | 272 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | . 5 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | FR2 | Gtl | NoRidge | Norm | Norm | 1Fam | 2Story | 8 | 5 | 2000 | 2000 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 350.0 | Gd | TA | PConc | Gd | TA | Av | GLQ | 655 | Unf | 0 | 490 | 1145 | GasA | Ex | Y | SBrkr | 1145 | 1053 | 0 | 2198 | 1 | 0 | 2 | 1 | 4 | 1 | Gd | 9 | Typ | 1 | TA | Attchd | 2000.0 | RFn | 3 | 836 | TA | TA | Y | 192 | 84 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | . X.shape . (1460, 79) . y.shape . (1460,) . We have 1,460 rows and 79 columns. Later on, we&#39;ll check these columns to verify which of them will be meaningful to the model. . In the next step, we&#39;ll split the data into training and validation sets. . 3. Training and validation data . It is crucial to break our data into a set for training the model and another one to validate the results. It&#39;s worth mentioning that we should never use the test data here. Our test set stays untouched until we are satisfied with our model&#39;s performance. . What we&#39;re going to do is taking the predictors X and target vector y and breaking them into training and validation sets. For that, we&#39;ll use scikit-learn&#39;s train_test_split. . X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0) . Checking the shape of our training and validation sets, we get the following. . print(f&#39;Shape of X_train_full: {X_train_full.shape}&#39;) print(f&#39;Shape of X_valid_full: {X_valid_full.shape}&#39;) print(f&#39;Shape of y_train: {y_train.shape}&#39;) print(f&#39;Shape of y_valid: {y_valid.shape}&#39;) . Shape of X_train_full: (1168, 79) Shape of X_valid_full: (292, 79) Shape of y_train: (1168,) Shape of y_valid: (292,) . 4. Analyze and prepare the data . Now, we start analyzing the data by checking some information about the features. . X.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1460 entries, 1 to 1460 Data columns (total 79 columns): # Column Non-Null Count Dtype -- -- 0 MSSubClass 1460 non-null int64 1 MSZoning 1460 non-null object 2 LotFrontage 1201 non-null float64 3 LotArea 1460 non-null int64 4 Street 1460 non-null object 5 Alley 91 non-null object 6 LotShape 1460 non-null object 7 LandContour 1460 non-null object 8 Utilities 1460 non-null object 9 LotConfig 1460 non-null object 10 LandSlope 1460 non-null object 11 Neighborhood 1460 non-null object 12 Condition1 1460 non-null object 13 Condition2 1460 non-null object 14 BldgType 1460 non-null object 15 HouseStyle 1460 non-null object 16 OverallQual 1460 non-null int64 17 OverallCond 1460 non-null int64 18 YearBuilt 1460 non-null int64 19 YearRemodAdd 1460 non-null int64 20 RoofStyle 1460 non-null object 21 RoofMatl 1460 non-null object 22 Exterior1st 1460 non-null object 23 Exterior2nd 1460 non-null object 24 MasVnrType 1452 non-null object 25 MasVnrArea 1452 non-null float64 26 ExterQual 1460 non-null object 27 ExterCond 1460 non-null object 28 Foundation 1460 non-null object 29 BsmtQual 1423 non-null object 30 BsmtCond 1423 non-null object 31 BsmtExposure 1422 non-null object 32 BsmtFinType1 1423 non-null object 33 BsmtFinSF1 1460 non-null int64 34 BsmtFinType2 1422 non-null object 35 BsmtFinSF2 1460 non-null int64 36 BsmtUnfSF 1460 non-null int64 37 TotalBsmtSF 1460 non-null int64 38 Heating 1460 non-null object 39 HeatingQC 1460 non-null object 40 CentralAir 1460 non-null object 41 Electrical 1459 non-null object 42 1stFlrSF 1460 non-null int64 43 2ndFlrSF 1460 non-null int64 44 LowQualFinSF 1460 non-null int64 45 GrLivArea 1460 non-null int64 46 BsmtFullBath 1460 non-null int64 47 BsmtHalfBath 1460 non-null int64 48 FullBath 1460 non-null int64 49 HalfBath 1460 non-null int64 50 BedroomAbvGr 1460 non-null int64 51 KitchenAbvGr 1460 non-null int64 52 KitchenQual 1460 non-null object 53 TotRmsAbvGrd 1460 non-null int64 54 Functional 1460 non-null object 55 Fireplaces 1460 non-null int64 56 FireplaceQu 770 non-null object 57 GarageType 1379 non-null object 58 GarageYrBlt 1379 non-null float64 59 GarageFinish 1379 non-null object 60 GarageCars 1460 non-null int64 61 GarageArea 1460 non-null int64 62 GarageQual 1379 non-null object 63 GarageCond 1379 non-null object 64 PavedDrive 1460 non-null object 65 WoodDeckSF 1460 non-null int64 66 OpenPorchSF 1460 non-null int64 67 EnclosedPorch 1460 non-null int64 68 3SsnPorch 1460 non-null int64 69 ScreenPorch 1460 non-null int64 70 PoolArea 1460 non-null int64 71 PoolQC 7 non-null object 72 Fence 281 non-null object 73 MiscFeature 54 non-null object 74 MiscVal 1460 non-null int64 75 MoSold 1460 non-null int64 76 YrSold 1460 non-null int64 77 SaleType 1460 non-null object 78 SaleCondition 1460 non-null object dtypes: float64(3), int64(33), object(43) memory usage: 912.5+ KB . From the summary above, we can observe that some columns have missing values. Let&#39;s take a closer look. . 4.1. Missing Values . missing_values = X.isnull().sum() missing_values = missing_values[missing_values &gt; 0].sort_values(ascending=False) print(missing_values) . PoolQC 1453 MiscFeature 1406 Alley 1369 Fence 1179 FireplaceQu 690 LotFrontage 259 GarageYrBlt 81 GarageType 81 GarageFinish 81 GarageQual 81 GarageCond 81 BsmtFinType2 38 BsmtExposure 38 BsmtFinType1 37 BsmtCond 37 BsmtQual 37 MasVnrArea 8 MasVnrType 8 Electrical 1 dtype: int64 . Some features have missing values counting for the majority of their entries. Checking the competition page, we find more details about the values for each feature, which will help us handle missing data. . For instance, in the columns PoolQC, MiscFeature, Alley, Fence, and FireplaceQu, the missing values mean that the house doesn&#39;t count with that specific feature, so, we&#39;ll fill the missing values with &quot;NA&quot;. All the null values in columns starting with Garage and Bsmt are related to houses that don&#39;t have a garage or basement, respectively. We&#39;ll fill those and the remaining null values with &quot;NA&quot; or the mean value, considering if the features are categorical or numerical. . 4.2. Preprocessing the categorical variables . Most machine learning models only work with numerical variables. Therefore, if we feed the model with categorical variables without preprocessing them first, we&#39;ll get an error. . There are several ways to deal with categorical values. Here, we&#39;ll use One-Hot Encoding, which will create new columns indicating the presence or absence of each value in the original data. . One issue of One-Hot Encoding is dealing with variables with numerous unique categories since it will create a new column for each unique category. Thus, this project will only include categorical variables with no more than 15 unique values. . categorical_cols = [col for col in X_train_full.columns if X_train_full[col].nunique() &lt;= 15 and X_train_full[col].dtype == &#39;object&#39;] # Select numeric values numeric_cols = [col for col in X_train_full.columns if X_train_full[col].dtype in [&#39;int64&#39;, &#39;float64&#39;]] # Keep selected columns my_columns = categorical_cols + numeric_cols X_train = X_train_full[my_columns].copy() X_valid = X_valid_full[my_columns].copy() X_test = X_test_full[my_columns].copy() . 4.3. Create a pipeline . Pipelines are a great way to keep the data modeling and preprocessing more organized and easier to understand. Creating a pipeline, we&#39;ll handle the missing values and the preprocessing covered in the previous two steps. . As defined above, numerical missing entries will be filled with the mean value while missing categorical variables will be filled with &quot;NA&quot;. Furthermore, categorical columns will also be preprocessed with One-Hot Encoding. . We are using SimpleImputer to fill in missing values and ColumnTransformer will help us to apply the numerical and categorical preprocessors in a single transformer. . numerical_transformer = SimpleImputer(strategy=&#39;mean&#39;) # Preprocessing categorical values categorical_transformer = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;NA&#39;)), (&#39;onehot&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;)) ]) # Pack the preprocessors together preprocessor = ColumnTransformer(transformers=[ (&#39;num&#39;, numerical_transformer, numeric_cols), (&#39;cat&#39;, categorical_transformer, categorical_cols) ]) . 5. Define a model . Now that we have bundled our preprocessors in a pipeline, we can define a model. In this article, we are working with XGBoost, one of the most effective machine learning algorithms, that presents great results in many Kaggle competitions. As a metric of evaluation, we are using the Mean Absolute Error. . model = XGBRegressor(verbosity=0, random_state=0) # Pack preprocessing and modeling together in a pipeline my_pipeline = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;model&#39;, model) ]) # Preprocessing of training data, fit model my_pipeline.fit(X_train, y_train) # Preprocessing of validation data, get predictions preds = my_pipeline.predict(X_valid) print(&#39;MAE:&#39;, mean_absolute_error(y_valid, preds)) . MAE: 16706.181988441782 . 6. Cross-validation . Using Cross-Validation can yield better results. Instead of simply using the training and test sets, cross-validation will run our model on different subsets of the data to get multiple measures of model quality. . We&#39;ll use the cross-validator KFold in its default setup to split the training data into 5 folds. Then, each fold will be used once as validation while the remaining folds will form the training set. After that, cross-validate will evaluate the metrics. In this case, we&#39;re using the Mean Absolute Error. . kfold = KFold(shuffle=True, random_state=0) # Evaluating the Mean Absolute Error scores = cross_validate(my_pipeline, X_train, y_train, scoring=&#39;neg_mean_absolute_error&#39;, cv=kfold) # Multiply by -1 since sklearn calculates negative MAE print(&#39;Average MAE score:&#39;, (scores[&#39;test_score&#39;] * -1).mean()) . Average MAE score: 16168.894833206665 . With cross-validation we could improve our score, reducing the error. In the next step, we&#39;ll try to further improve the model, optimizing some hyperparameters. . 7. Hyperparameter tuning . XGBoost in its default setup usually yields great results, but it also has plenty of hyperparameters that can be optimized to improve the model. Here, we&#39;ll use a method called GridSearchCV which will search over specified parameter values and return the best ones. Once again, we&#39;ll utilize the pipeline and the cross-validator KFold defined above. . GridSearchCV will perform an exhaustive search over parameters, which can demand a lot of computational power and take a lot of time to be finished. We can speed up the process a little bit by setting the parameter n_jobs to -1, which means that the machine will use all processors on the task. . &quot;&quot;&quot; To pass parameter in a pipeline, we should add the names of the steps and the parameter name separated by a ‘__’. Ex: Instead of &#39;n_estimators&#39;, we should set &#39;model__n_estimators&#39;. https://github.com/scikit-learn/scikit-learn/issues/18472 &quot;&quot;&quot; # parameters to be searched over param_grid = {&#39;model__n_estimators&#39;: [10, 50, 100, 200, 400, 600], &#39;model__max_depth&#39;: [2, 3, 5, 7, 10], &#39;model__min_child_weight&#39;: [0.0001, 0.001, 0.01], &#39;model__learning_rate&#39;: [0.01, 0.1, 0.5, 1]} # find the best parameter kfold = KFold(shuffle=True, random_state=0) grid_search = GridSearchCV(my_pipeline, param_grid, scoring=&#39;neg_mean_absolute_error&#39;, cv=kfold, n_jobs=-1) grid_result = grid_search.fit(X_train, y_train) . print(&#39;Best result:&#39;, round((grid_result.best_score_ * -1), 2), &#39;for&#39;, grid_result.best_params_) . Best result: 15750.17 for {&#39;model__learning_rate&#39;: 0.1, &#39;model__max_depth&#39;: 3, &#39;model__min_child_weight&#39;: 0.0001, &#39;model__n_estimators&#39;: 400} . 8. Generate test predictions . After tuning some hyperparameters, it&#39;s time to go over the modeling process again to make predictions on the test set. We&#39;ll define our final model based on the optimized values provided by GridSearchCV. . final_model = XGBRegressor(n_estimators=400, max_depth=3, min_child_weight=0.0001, learning_rate=0.1, verbosity=0, random_state=0 ) # Create a pipeline final_pipeline = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;final_model&#39;, final_model) ]) # Fit the model final_pipeline.fit(X_train, y_train) # Get predictions on the test set final_prediction = final_pipeline.predict(X_test) . 9. Submit your results . We&#39;re almost there! The machine learning modeling is done, but we still need to submit our results to have our score recorded. . This step is quite simple. We need to create a .csv file containing the predictions. This file consists of a DataFrame with two columns. In this case, one column for &quot;Id&quot; and the other one for the test predictions on the target feature. . output = pd.DataFrame({&#39;Id&#39;: X_test.index, &#39;SalePrice&#39;: final_prediction}) output.to_csv(&#39;submission.csv&#39;, index=False) . 10. Join the competition . Finally, we just need to join the competition. Please follow the steps below, according to Kaggle&#39;s instructions. . Start by accessing the competition page and clicking on Join Competition. | In your Kaggle notebook, click on the blue Save Version button in the top right corner of the window. | A pop-up window will show up. Select the option Save and Run All and then click on the blue Save button. | A new pop-up shows up in the bottom left corner while your notebook is running. When it stops running, click on the number to the right of the Save Version button. You should click on the ellipsis (...) to the right of the most recent notebook version, and select Open in Viewer. This brings you into view mode of the same page. | Now, click on the Output tab on the right of the screen. Then, click on the blue Submit button to submit your results to the leaderboard. | . After submitting, you can check your score and position on the leaderboard. . . Conclusion . This article was intended to be instructive, helping data science beginners to structure their first projects on Kaggle in simple steps. With this straightforward approach, I&#39;ve got a score of 14,778.87, which ranked this project in the Top 7%. . After further studying, you can go back on past projects and try to enhance their performance, using new skills you&#39;ve learned. To improve this project, we could investigate and treat the outliers more closely, apply a different approach to missing values, or do some feature engineering, for instance. . My advice to beginners is to keep it simple when starting out. Instead of aiming at the &quot;perfect&quot; model, focus on completing the project, applying your skills correctly, and learning from your mistakes, understanding where and why you messed things up. The data science community is on constant expansion and there&#39;s plenty of more experienced folks willing to help on websites like Kaggle or Stack Overflow. Try to learn from their past mistakes as well! With practice and discipline, it&#39;s just a matter of time to start building more elaborate projects and climb up the ranking of Kaggle&#39;s competitions. .",
            "url": "https://jmmerrell.github.io/ws/poop/stuff/2021/08/11/youtube_views_xgboost.html",
            "relUrl": "/poop/stuff/2021/08/11/youtube_views_xgboost.html",
            "date": " • Aug 11, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "import os from numpy.core.fromnumeric import shape from numpy.lib.function_base import diff import pandas as pd import re from datetime import datetime, timedelta import numpy as np import matplotlib.pyplot as plt from pandas.core.indexes.base import Index import emoji from scipy import stats import fasttext from collections import Counter from pprint import pprint from IPython.display import display_html from itertools import chain,cycle os.chdir(&quot;C: Users merre Desktop ws data youtube_jadoinstuff&quot;) files= [&quot;shanieology.txt&quot;,&quot;simau.txt&quot;,&quot;soundslikepizza.txt&quot;,&quot;azerrz.txt&quot;,&quot;BigShade.txt&quot;,&quot;black_gryph0n.txt&quot; ,&quot;brian_hull.txt&quot;,&quot;brizzy_voices.txt&quot;,&quot;brock_baker.txt&quot;,&quot;charlie_hopkinson.txt&quot;,&quot;danny_padilla_&amp;_mason_sperling.txt&quot; ,&quot;ja_doin_stuff.txt&quot;,&quot;joshiiwuh.txt&quot;,&quot;knep.txt&quot;,&quot;ori.txt&quot;,&quot;redfireball555.txt&quot;,&quot;scheiffer_bates.txt&quot;,&quot;daniel_ferguson.txt&quot;, &quot;BigShade.txt&quot;,&quot;best_in_class.txt&quot;,&quot;maxamili.txt&quot;,&quot;mikey_bolts.txt&quot;] df = pd.DataFrame(columns=[&#39;video_id&#39;,&#39;title&#39;,&#39;title_len&#39;,&#39;words&#39;,&#39;upper_pct&#39;,&#39;emoji_count&#39;,&#39;upload_date&#39;,&#39;upload_time&#39;, &#39;upload_day&#39;,&#39;upload_time_of_day&#39;,&#39;viewCount&#39;,&#39;likeCount&#39;,&#39;dislikeCount&#39;,&#39;favoriteCount&#39;, &#39;commentCount&#39;,&#39;duration&#39;,&#39;definition&#39;,&#39;caption&#39;,&#39;licensedContent&#39;,&#39;thumbnail_url&#39;, &#39;thumbnail_w&#39;, &#39;thumbnail_h&#39;, &#39;tags&#39;,&#39;num_tags&#39;,&#39;categoryId&#39;,&#39;liveBroadcastContent&#39;, &#39;topicCategories&#39;, &#39;channel&#39;, &#39;channel_subs&#39;, &#39;channel_views&#39;, &#39;channel_videos&#39;,&#39;desc&#39;]) #Loop through all the youtuber&#39;s data files and combine into on data frame for file in files: df_add= pd.read_csv(file) df = df.append(df_add.drop([&#39;Unnamed: 0&#39;],axis=1)) #Read in the files that have the thumbnail data, and combine with the youtuber data df_thumb = pd.read_csv(&quot;thumbnail_data_20210801_23-03-21.638854.txt&quot;).append(pd.read_csv(&quot;thumbnail_data_20210802_21-39-16.451172.txt&quot;)) df_all = pd.merge(df.drop(&#39;defaultAudioLanguage&#39;,axis=1),df_thumb.drop([&#39;thumbnail_url&#39;],axis=1),on=&quot;video_id&quot;,how=&quot;inner&quot;).drop([&#39;emoji_count&#39;],axis=1) df_all[&#39;all_text&#39;] = df_all[&#39;title&#39;].astype(str) + df_all[&#39;tags&#39;].astype(str) + df_all[&#39;desc&#39;].astype(str) . print(df_all.isna().mean().round(4)) #looks like &quot;topicCategories&quot; may need imputation and/or further exploration unique, counts = np.unique(np.array([y for x in df[&#39;topicCategories&#39;] for y in re.findall(r&#39;/[ w_-]+ &#39;&#39;,str(x).upper()) ]), return_counts=True) dic= dict(zip(unique, counts)) dic2=dict(sorted(dic.items(),key= lambda x:x[1],reverse=True)) print(dic2) #Impute missing data #Several topics are very similar so I&#39;m combining them into new fields. df_all[&quot;topic_entertain&quot;] = [1 if any([str(x).upper().find(y) &gt;-1 for y in [&#39;ENTERTAINMENT&#39;]]) else 0 for x in df_all[&#39;topicCategories&#39;]] df_all[&quot;topic_video_game&quot;] = [1 if any([str(x).upper().find(y) &gt;-1 for y in [&#39;_GAME&#39;]]) else 0 for x in df_all[&#39;topicCategories&#39;]] df_all[&quot;topic_music&quot;] = [1 if any([str(x).upper().find(y) &gt;-1 for y in [&#39;MUSIC&#39;]]) else 0 for x in df_all[&#39;topicCategories&#39;]] df_all[&quot;topic_film_tv&quot;] = [1 if any([str(x).upper().find(y) &gt;-1 for y in [&#39;FILM&#39;,&#39;TELEVISION&#39;]]) else 0 for x in df_all[&#39;topicCategories&#39;]] #Now we can drop defaultAudioLanguage since we have a replacement field for that df_all = df_all.drop(&#39;topicCategories&#39;,axis=1) #We can now remove all rows that have missiong values since the rows with the most missing values are gone df_all = df_all.dropna() df_all = df_all.reset_index() all_labels = list(df_all[&quot;labels&quot;]) all_labels = [ x.split(&#39;,&#39;) for x in df_all[&quot;labels&quot;]] all_labels = [[re.sub(r&#39;[^a-zA-Z u00C0- u00FF s]&#39;, &quot; &quot;,i).strip(&#39; t n r&#39;).upper() for i in ii] for ii in all_labels] all_labels_flat = list(set([item for elem in all_labels for item in elem])) df_all[&#39;labels_words&#39;] = all_labels . video_id 0.0000 title 0.0000 title_len 0.0000 words 0.0000 upper_pct 0.0000 upload_date 0.0000 upload_time 0.0000 upload_day 0.0000 upload_time_of_day 0.0000 viewCount 0.0000 likeCount 0.0000 dislikeCount 0.0000 favoriteCount 0.0000 commentCount 0.0057 duration 0.0000 definition 0.0000 caption 0.0000 licensedContent 0.0000 thumbnail_url 0.0000 thumbnail_w 0.0000 thumbnail_h 0.0000 tags 0.0000 num_tags 0.0000 categoryId 0.0000 liveBroadcastContent 0.0000 topicCategories 0.0598 channel 0.0000 channel_subs 0.0000 channel_views 0.0000 channel_videos 0.0000 desc 0.0041 labels 0.0000 faces 0.0000 texts 0.0000 adult 0.0000 medical 0.0000 racy 0.0000 spoof 0.0000 violence 0.0000 all_text 0.0000 dtype: float64 {&#34;/ENTERTAINMENT&#39;&#34;: 970, &#34;/VIDEO_GAME_CULTURE&#39;&#34;: 545, &#34;/ACTION-ADVENTURE_GAME&#39;&#34;: 497, &#34;/ACTION_GAME&#39;&#34;: 442, &#34;/FILM&#39;&#34;: 265, &#34;/ROLE-PLAYING_VIDEO_GAME&#39;&#34;: 234, &#34;/MUSIC&#39;&#34;: 124, &#34;/SPORTS_GAME&#39;&#34;: 111, &#34;/STRATEGY_VIDEO_GAME&#39;&#34;: 60, &#34;/HIP_HOP_MUSIC&#39;&#34;: 57, &#34;/POP_MUSIC&#39;&#34;: 44, &#34;/TELEVISION_PROGRAM&#39;&#34;: 43, &#34;/SIMULATION_VIDEO_GAME&#39;&#34;: 14, &#34;/HOBBY&#39;&#34;: 13, &#34;/FOOD&#39;&#34;: 11, &#34;/HUMOUR&#39;&#34;: 8, &#34;/TECHNOLOGY&#39;&#34;: 7, &#34;/CHRISTIAN_MUSIC&#39;&#34;: 6, &#34;/ELECTRONIC_MUSIC&#39;&#34;: 6, &#34;/SOUL_MUSIC&#39;&#34;: 6, &#34;/PET&#39;&#34;: 4, &#34;/RHYTHM_AND_BLUES&#39;&#34;: 4, &#34;/ROCK_MUSIC&#39;&#34;: 4, &#34;/TOURISM&#39;&#34;: 4, &#34;/BASKETBALL&#39;&#34;: 3, &#34;/PUZZLE_VIDEO_GAME&#39;&#34;: 3, &#34;/REGGAE&#39;&#34;: 3, &#34;/CASUAL_GAME&#39;&#34;: 2, &#34;/INDEPENDENT_MUSIC&#39;&#34;: 2, &#34;/PHYSICAL_FITNESS&#39;&#34;: 2, &#34;/SOCIETY&#39;&#34;: 2, &#34;/ASSOCIATION_FOOTBALL&#39;&#34;: 1, &#34;/HEALTH&#39;&#34;: 1, &#34;/PERFORMING_ARTS&#39;&#34;: 1, &#34;/SPORT&#39;&#34;: 1, &#34;/VEHICLE&#39;&#34;: 1} . #Scans the title for any emojis in general df_all[&quot;emoji_count&quot;]= [(emoji.emoji_count(x)&gt;0)*1 for x in df_all[&quot;title&quot;]] ###Create variables to see how well title reflects the description, thumbnail and tags of the video #The text read in by the ggogle vision text detection is messy. #Need to clean up and create new variable &quot;thumb_words&quot;, thumb_word_count&quot; df_all[&#39;labels_word_count&#39;]=[len(df_all[&#39;labels_words&#39;][i]) for i in range(len(df_all))] discard_list = [&quot;&quot;,&quot;B&quot;,&#39;C&#39;,&#39;D&#39;,&#39;&#39;,&#39;F&#39;,&#39;G&#39;,&#39;H&#39;,&#39;&#39;,&#39;J&#39;,&#39;K&#39;,&#39;L&#39;,&#39;M&#39;,&#39;N&#39;,&#39;&#39;,&#39;P&#39;,&#39;Q&#39;,&#39;R&#39;,&#39;S&#39;,&#39;T&#39;,&#39;V&#39;,&#39;W&#39;,&#39;X&#39;,&#39;Z&#39;] df_all[&#39;thumb_words&#39;]=[list(set(re.sub(r&#39;[^a-zA-Z u00C0- u00FF]&#39;, &quot; &quot;,str(re.findall(r&#39;[^ n][ s ? ! &quot; &#39;]+[a-zA-Z u00C0- u00FF]+[ s . ? ! &quot; &#39;]?&#39;,str(df_all[&#39;texts&#39;][i])))).upper().split(&quot; &quot;))) for i in range(len(df_all))] for i in range(len(df_all)): test_list = df_all[&#39;thumb_words&#39;][i] remove_list = discard_list df_all[&#39;thumb_words&#39;][i] = [i for i in test_list if i not in remove_list] df_all[&#39;thumb_word_count&#39;]=[len(df_all[&#39;thumb_words&#39;][i]) for i in range(len(df_all))] df_all[&#39;title_in_desc&#39;]=[((df_all[&#39;desc&#39;][i].upper().find(df_all[&#39;title&#39;][i].upper()))&gt;-1)*1 for i in range(len(df_all))] df_all[&#39;thumb_words_in_title&#39;]=[ sum([(str(df_all[&#39;title&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;thumb_words&#39;][z]]) for z in range(len(df_all[&#39;title&#39;]))] df_all[&#39;thumb_words_in_tags&#39;]=[ sum([(str(df_all[&#39;tags&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;thumb_words&#39;][z]]) for z in range(len(df_all[&#39;tags&#39;]))] df_all[&#39;label_words_in_title&#39;]=&#39;&#39; df_all[&#39;label_words_in_tags&#39;]=&#39;&#39; for z in range(len(df_all[&#39;title&#39;])): df_all[&#39;label_words_in_title&#39;][z]=sum([(str(df_all[&#39;title&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;labels_words&#39;][z]]) df_all[&#39;label_words_in_tags&#39;][z]=sum([(str(df_all[&#39;tags&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;labels_words&#39;][z]]) ##See how many faces are in the thumbnails df_all[&#39;faces_surprised&#39;]=[sum([(str(y)==&#39;suprised&#39;)*1 for y in re.findall(r&#39;[a-zA-Z u00C0- u00FF]+&#39;,df_all[&#39;faces&#39;][z])]) for z in range(len(df_all[&#39;faces&#39;]))] df_all[&#39;faces_angry&#39;]=[sum([(str(y)==&#39;angry&#39;)*1 for y in re.findall(r&#39;[a-zA-Z u00C0- u00FF]+&#39;,df_all[&#39;faces&#39;][z])]) for z in range(len(df_all[&#39;faces&#39;]))] df_all[&#39;faces_happy&#39;]=[sum([(str(y)==&#39;happy&#39;)*1 for y in re.findall(r&#39;[a-zA-Z u00C0- u00FF]+&#39;,df_all[&#39;faces&#39;][z])]) for z in range(len(df_all[&#39;faces&#39;]))] df_all[&#39;faces_other&#39;]=[sum([(str(y)==&#39;other&#39;)*1 for y in re.findall(r&#39;[a-zA-Z u00C0- u00FF]+&#39;,df_all[&#39;faces&#39;][z])]) for z in range(len(df_all[&#39;faces&#39;]))] ###Create varibales to see how much people like the videos df_all[&#39;likes_views_ratio&#39;]=df_all[&#39;likeCount&#39;]/df_all[&#39;viewCount&#39;] df_all[&#39;likes_subs_ratio&#39;]=df_all[&#39;likeCount&#39;]/df_all[&#39;channel_subs&#39;] df_all[&#39;comment_views_ratio&#39;]=df_all[&#39;commentCount&#39;]/df_all[&#39;viewCount&#39;] try: df_all[&#39;comment_likes_ratio&#39;]=df_all[&#39;commentCount&#39;]/df_all[&#39;likeCount&#39;] except: df_all[&#39;comment_likes_ratio&#39;]=df_all[&#39;commentCount&#39;].mean()/df_all[&#39;likeCount&#39;].mean() df_all[&#39;comment_subs_ratio&#39;]=df_all[&#39;commentCount&#39;]/df_all[&#39;channel_subs&#39;] df_all[&#39;views_favorite_ratio&#39;]=df_all[&#39;favoriteCount&#39;]/df_all[&#39;viewCount&#39;] df_all[&#39;like_percent&#39;]=df_all[&#39;likeCount&#39;]/(df_all[&#39;likeCount&#39;]+df_all[&#39;dislikeCount&#39;]) max_date=max(df_all[&#39;upload_date&#39;]) df_all[&#39;days_since_upload&#39;]= (pd.to_datetime(max_date) - pd.to_datetime(df_all[&#39;upload_date&#39;])).dt.days +1 df_all[&#39;views_per_day&#39;] = df_all[&#39;viewCount&#39;]/df_all[&#39;days_since_upload&#39;] df_all[&#39;views_per_day_per_sub&#39;] = df_all[&#39;viewCount&#39;]/df_all[&#39;days_since_upload&#39;]/df_all[&#39;channel_subs&#39;] df_all[&#39;views_per_sub&#39;] = df_all[&#39;viewCount&#39;]/df_all[&#39;channel_subs&#39;] . C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:18: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:29: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:30: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . # plt.plot( df_all[&#39;days_since_upload&#39;],df_all[&#39;viewCount&#39;]) # plt.xlabel(&#39;Time (hr)&#39;) # plt.ylabel(&#39;Position (km)&#39;) df_model = df_all[[&#39;title_len&#39;, &#39;words&#39;, &#39;upper_pct&#39;, &#39;upload_day&#39;, &#39;upload_time_of_day&#39;, &#39;viewCount&#39;,&#39;views_per_sub&#39;, &#39;likeCount&#39;, &#39;dislikeCount&#39;, &#39;favoriteCount&#39;, &#39;commentCount&#39;, &#39;duration&#39;, &#39;definition&#39;, &#39;caption&#39;, &#39;licensedContent&#39;, &#39;thumbnail_h&#39;, &#39;num_tags&#39;, &#39;categoryId&#39;, &#39;liveBroadcastContent&#39;, &#39;channel_subs&#39;, &#39;channel_views&#39;, &#39;channel_videos&#39;, &#39;adult&#39;, &#39;medical&#39;, &#39;racy&#39;, &#39;spoof&#39;, &#39;violence&#39;, &#39;emoji_count&#39;, &#39;thumb_word_count&#39;, &#39;title_in_desc&#39;, &#39;thumb_words_in_title&#39;, &#39;thumb_words_in_tags&#39;, &#39;label_words_in_title&#39;, &#39;label_words_in_tags&#39;, &#39;days_since_upload&#39;, &#39;views_per_day&#39;, &#39;views_per_day_per_sub&#39;, &#39;likes_views_ratio&#39;,&#39;comment_views_ratio&#39;, &#39;likes_subs_ratio&#39;, &#39;comment_likes_ratio&#39;,&#39;comment_subs_ratio&#39;, &#39;views_favorite_ratio&#39;, &#39;like_percent&#39;,&#39;faces_surprised&#39;,&#39;faces_happy&#39;,&#39;faces_angry&#39;,&#39;faces_other&#39;, &#39;topic_entertain&#39;,&#39;topic_video_game&#39;,&#39;topic_music&#39;,&#39;topic_film_tv&#39;]] df_model.describe() # plt.scatter(df_all[&#39;views_like_ratio&#39;],df_all[&#39;views_per_sub&#39;]) # plt.show() # plt.subplot(2,3,1) # if use subplot #placed a log transformation on number of views to show a more normal distribution #There are some extreme outliers in views, so we will drop these few outliers to not inflate the RMSE of the predicted views np.power(df_model[&#39;views_per_sub&#39;].astype(float)[(stats.zscore(np.power(df_model[&#39;views_per_sub&#39;].astype(float),1/50)) &lt; 2.5)],1/50).hist(bins=30) plt.show() df_model2 = df_model[np.abs(stats.zscore(np.power(df_model[&#39;views_per_sub&#39;].astype(float),1/50))) &lt; 2.5] df_model2 = df_model2.reset_index() cat_x_vars=[&#39;upload_day&#39;, &#39;upload_time_of_day&#39;, &#39;caption&#39;,&#39;thumbnail_h&#39;, &#39;categoryId&#39;, &#39;title_in_desc&#39;, &#39;adult&#39;, &#39;medical&#39;, &#39;racy&#39;, &#39;spoof&#39;, &#39;violence&#39;,&#39;topic_entertain&#39;,&#39;topic_video_game&#39;,&#39;topic_music&#39;,&#39;topic_film_tv&#39;] for i in df_all.columns: temp_df = pd.DataFrame(df_all[i].value_counts(normalize=True).sort_values(ascending=False)) if shape(temp_df)[0] &lt;= 50 and i in cat_x_vars: print(temp_df) # df_all[&#39;duration&#39;][(np.abs(stats.zscore(df_all[&#39;duration&#39;])) &lt; 3)].hist(bins=20) # plt.title(&#39;duration&#39;) # df_all[&#39;channel_subs&#39;][(np.abs(stats.zscore(df_all[&#39;channel_subs&#39;].astype(float))) &lt; 3)].hist(bins=20) # plt.title(&#39;channel_subs&#39;) # plt.show() # df_all.dtypes ##For the CategoryIDs field these are what the values correspond to # ID Category name # 1 Film &amp; Animation # 2 Autos &amp; Vehicles # 10 Music # 15 Pets &amp; Animals # 17 Sports # 19 Travel &amp; Events # 20 Gaming # 22 People &amp; Blogs # 23 Comedy # 24 Entertainment # 25 News &amp; Politics # 26 Howto &amp; Style # 27 Education # 28 Science &amp; Technology # 29 Nonprofits &amp; Activism ##For the upload_day field 0=Monday and 6=Sunday . upload_day 4 0.204060 5 0.170224 6 0.153045 1 0.136387 0 0.117647 3 0.110359 2 0.108277 upload_time_of_day afternoon 0.591879 night 0.243623 morning 0.096304 late_night 0.068194 caption False 0.947944 True 0.052056 thumbnail_h 720 0.783967 360 0.216033 categoryId 24 0.453930 20 0.279021 23 0.181676 10 0.042686 22 0.025508 1 0.015617 26 0.001041 28 0.000521 adult VERY_UNLIKELY 0.625195 UNLIKELY 0.351900 POSSIBLE 0.016658 LIKELY 0.004685 VERY_LIKELY 0.001562 medical VERY_UNLIKELY 0.611140 UNLIKELY 0.382613 POSSIBLE 0.006247 racy VERY_UNLIKELY 0.566372 UNLIKELY 0.289433 POSSIBLE 0.108277 LIKELY 0.028110 VERY_LIKELY 0.007808 spoof UNLIKELY 0.343571 VERY_UNLIKELY 0.246746 LIKELY 0.152525 VERY_LIKELY 0.130661 POSSIBLE 0.126497 violence UNLIKELY 0.580427 VERY_UNLIKELY 0.397189 POSSIBLE 0.019261 LIKELY 0.003123 topic_entertain 0 0.500781 1 0.499219 topic_video_game 0 0.636648 1 0.363352 topic_music 0 0.917751 1 0.082249 topic_film_tv 0 0.849558 1 0.150442 title_in_desc 0 0.890682 1 0.109318 . import seaborn as sns ######Prepare the data for random Forest Model df_model3 = df_model2 #Combine level of some categorical fields for i in range(len(df_model3)): if df_model3[&quot;adult&quot;][i] in [&#39;VERY_UNLIKELY&#39;,&#39;UNLIKELY&#39;]: df_model3[&quot;adult&quot;][i] = &#39;UNLIKELY&#39; else: df_model3[&quot;adult&quot;][i] = &#39;LIKLEY&#39; if df_model3[&quot;medical&quot;][i] in [&#39;VERY_UNLIKELY&#39;,&#39;UNLIKELY&#39;]: df_model3[&quot;medical&quot;][i] = &#39;UNLIKELY&#39; else: df_model3[&quot;medical&quot;][i] = &#39;LIKLEY&#39; if df_model3[&quot;racy&quot;][i] in [&#39;VERY_UNLIKELY&#39;,&#39;UNLIKELY&#39;]: df_model3[&quot;racy&quot;][i] = &#39;UNLIKELY&#39; else: df_model3[&quot;racy&quot;][i] = &#39;LIKLEY&#39; if df_model3[&quot;spoof&quot;][i] in [&#39;VERY_UNLIKELY&#39;,&#39;UNLIKELY&#39;]: df_model3[&quot;spoof&quot;][i] = &#39;UNLIKELY&#39; else: df_model3[&quot;spoof&quot;][i] = &#39;LIKLEY&#39; if df_model3[&quot;violence&quot;][i] in [&#39;VERY_UNLIKELY&#39;,&#39;UNLIKELY&#39;]: df_model3[&quot;violence&quot;][i] = &#39;UNLIKELY&#39; else: df_model3[&quot;violence&quot;][i] = &#39;LIKLEY&#39; if df_model3[&quot;categoryId&quot;][i] in [1,26,28]: df_model3[&quot;categoryId&quot;][i] = 99 print(df_model3.columns) #List of categorical explanatory variables cat_x_vars=[&#39;upload_day&#39;, &#39;upload_time_of_day&#39;, &#39;caption&#39;,&#39;thumbnail_h&#39;, &#39;categoryId&#39;, &#39;title_in_desc&#39;, &#39;adult&#39;, &#39;racy&#39;, &#39;spoof&#39;,&#39;topic_entertain&#39;,&#39;topic_video_game&#39;,&#39;topic_music&#39;,&#39;topic_film_tv&#39;] cat_features = pd.get_dummies(df_model3[cat_x_vars]) # , &#39;licensedContent&#39;, &#39;liveBroadcastContent&#39; for i in df_model3.columns: temp_df = pd.DataFrame(df_model3[i].value_counts(normalize=True).sort_values(ascending=False)) if shape(temp_df)[0] &lt;= 50 and i in cat_x_vars: print(temp_df) #List of numeric explanatory variables num_x_vars=[&#39;title_len&#39;, &#39;upper_pct&#39;, &#39;duration&#39;, &#39;num_tags&#39;, &#39;emoji_count&#39;, &#39;thumb_word_count&#39;, &#39;like_percent&#39;, &#39;thumb_words_in_title&#39;, &#39;thumb_words_in_tags&#39;, &#39;label_words_in_title&#39;, &#39;label_words_in_tags&#39;,&#39;faces_surprised&#39;,&#39;faces_happy&#39;,&#39;faces_angry&#39;,&#39;faces_other&#39;, &#39;days_since_upload&#39;, &#39;comment_likes_ratio&#39;,&#39;likes_views_ratio&#39;,&#39;comment_views_ratio&#39;,&#39;channel_subs&#39;] #, , &#39;channel_videos&#39;,&#39;comment_views_ratio&#39;, &#39;views_favorite_ratio&#39;, &#39;like_percent&#39;,&#39;views_like_ratio&#39;, num_features=df_model3[num_x_vars] for var in num_x_vars: num_features[var] = num_features[var].astype(float) print(num_features.describe()) plt.figure(figsize=(20,20)) sns.heatmap(num_features.corr(),annot=True,cmap=&quot;RdYlGn&quot;,annot_kws={&quot;size&quot;:15}) #independent variable pred_features =df_model3[&#39;views_per_sub&#39;].astype(float) # &#39;views_per_day&#39;, # &#39;views_per_day_per_sub&#39;, &#39;views_per_sub&#39;, . A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . Index([&#39;index&#39;, &#39;title_len&#39;, &#39;words&#39;, &#39;upper_pct&#39;, &#39;upload_day&#39;, &#39;upload_time_of_day&#39;, &#39;viewCount&#39;, &#39;views_per_sub&#39;, &#39;likeCount&#39;, &#39;dislikeCount&#39;, &#39;favoriteCount&#39;, &#39;commentCount&#39;, &#39;duration&#39;, &#39;definition&#39;, &#39;caption&#39;, &#39;licensedContent&#39;, &#39;thumbnail_h&#39;, &#39;num_tags&#39;, &#39;categoryId&#39;, &#39;liveBroadcastContent&#39;, &#39;channel_subs&#39;, &#39;channel_views&#39;, &#39;channel_videos&#39;, &#39;adult&#39;, &#39;medical&#39;, &#39;racy&#39;, &#39;spoof&#39;, &#39;violence&#39;, &#39;emoji_count&#39;, &#39;thumb_word_count&#39;, &#39;title_in_desc&#39;, &#39;thumb_words_in_title&#39;, &#39;thumb_words_in_tags&#39;, &#39;label_words_in_title&#39;, &#39;label_words_in_tags&#39;, &#39;days_since_upload&#39;, &#39;views_per_day&#39;, &#39;views_per_day_per_sub&#39;, &#39;likes_views_ratio&#39;, &#39;comment_views_ratio&#39;, &#39;likes_subs_ratio&#39;, &#39;comment_likes_ratio&#39;, &#39;comment_subs_ratio&#39;, &#39;views_favorite_ratio&#39;, &#39;like_percent&#39;, &#39;faces_surprised&#39;, &#39;faces_happy&#39;, &#39;faces_angry&#39;, &#39;faces_other&#39;, &#39;topic_entertain&#39;, &#39;topic_video_game&#39;, &#39;topic_music&#39;, &#39;topic_film_tv&#39;, &#39;high_views&#39;], dtype=&#39;object&#39;) upload_day 4 0.204233 5 0.169841 6 0.152910 1 0.131746 0 0.119048 3 0.112169 2 0.110053 upload_time_of_day afternoon 0.592063 night 0.244444 morning 0.097884 late_night 0.065608 caption False 0.94709 True 0.05291 thumbnail_h 720 0.783069 360 0.216931 categoryId 24 0.456614 20 0.278836 23 0.180423 10 0.041799 22 0.025926 99 0.016402 adult UNLIKELY 0.97672 LIKLEY 0.02328 racy UNLIKELY 0.856085 LIKLEY 0.143915 spoof UNLIKELY 0.590476 LIKLEY 0.409524 title_in_desc 0 0.894709 1 0.105291 topic_entertain 0 0.503175 1 0.496825 topic_video_game 0 0.634921 1 0.365079 topic_music 0 0.917989 1 0.082011 topic_film_tv 0 0.847619 1 0.152381 . A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . title_len upper_pct duration num_tags emoji_count count 1890.000000 1890.000000 1890.000000 1890.000000 1890.000000 mean 51.248148 0.268345 10.570767 20.492593 0.047619 std 19.630070 0.330699 18.785139 11.973555 0.213015 min 5.000000 0.000000 0.116667 1.000000 0.000000 25% 37.000000 0.000000 3.516667 12.000000 0.000000 50% 49.000000 0.125000 6.283333 20.000000 0.000000 75% 64.000000 0.428571 11.075000 26.000000 0.000000 max 100.000000 1.000000 426.416667 75.000000 1.000000 thumb_word_count like_percent thumb_words_in_title count 1890.000000 1890.000000 1890.000000 mean 5.153968 0.974640 2.145503 std 7.767880 0.058392 2.262193 min 0.000000 0.000000 0.000000 25% 1.000000 0.977948 0.000000 50% 4.000000 0.987324 2.000000 75% 6.000000 0.993155 3.000000 max 119.000000 1.000000 15.000000 thumb_words_in_tags label_words_in_title label_words_in_tags count 1890.000000 1890.000000 1890.000000 mean 2.139153 0.104233 0.521164 std 2.412295 0.344714 0.748973 min 0.000000 0.000000 0.000000 25% 0.000000 0.000000 0.000000 50% 2.000000 0.000000 0.000000 75% 3.000000 0.000000 1.000000 max 16.000000 4.000000 5.000000 faces_surprised faces_happy faces_angry faces_other count 1890.000000 1890.000000 1890.000000 1890.000000 mean 0.141270 0.532275 0.029101 0.973016 std 0.396705 1.070449 0.168133 1.700998 min 0.000000 0.000000 0.000000 0.000000 25% 0.000000 0.000000 0.000000 0.000000 50% 0.000000 0.000000 0.000000 1.000000 75% 0.000000 1.000000 0.000000 1.000000 max 5.000000 10.000000 1.000000 15.000000 days_since_upload comment_likes_ratio likes_views_ratio count 1890.000000 1.890000e+03 1890.000000 mean 666.209524 6.265499e-02 0.058660 std 588.307997 2.776292e-17 0.039813 min 1.000000 6.265499e-02 0.000000 25% 232.250000 6.265499e-02 0.036111 50% 493.000000 6.265499e-02 0.050972 75% 877.750000 6.265499e-02 0.068545 max 3216.000000 6.265499e-02 0.352941 comment_views_ratio channel_subs count 1890.000000 1.890000e+03 mean 0.012812 1.350863e+06 std 0.022330 1.778690e+06 min 0.000000 3.870000e+02 25% 0.002642 1.170000e+04 50% 0.005187 4.060000e+05 75% 0.011605 2.257500e+06 max 0.236364 6.810000e+06 . df_model3[&#39;high_views&#39;] = pd.qcut(df_model3[&#39;views_per_sub&#39;], [0,0.5, 1], labels=[0,1]) df_model3[&#39;views_per_sub&#39;] = df_model3[&#39;views_per_sub&#39;].astype(float) # print(df_model3.groupby(&#39;high_views&#39;).agg({&#39;views_per_sub&#39;: [&#39;mean&#39;]})) from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.metrics import confusion_matrix, accuracy_score import shap import xgboost as xgb import matplotlib.pylab as pl # pred_features = np.power(df_model2[&#39;viewCount&#39;].astype(float),1/6) pred_features = df_model3[&#39;high_views&#39;].astype(int) # data = pd.concat([cat_features,num_features],axis=1) X_data=pd.concat([cat_features,num_features],axis=1) y_data=pred_features def split_data_train_model(labels, data): # 20% examples in test data train, test, train_labels, test_labels = train_test_split(X_data, y_data, test_size=0.2) # training data fit return test, test_labels, regressor # x_test, x_test_labels, regressor = split_data_train_model(y_data, X_data) # regressor = RandomForestRegressor(n_estimators=250) # regressor.fit(X_train, y_train) # predictions = regressor.predict(X_train) # reg_xgb = xgb.XGBRegressor() # objective =&#39;reg:linear&#39; # reg_xgb.fit(X_train,y_train,verbose=True,early_stopping_rounds=10,eval_set=[(X_test,y_test)]) ###optimize parameters for XGBoost X_train, X_test, y_train, y_test = train_test_split(X_data,y_data,test_size=0.2) # [&#39;neg_mean_squared_error&#39;,&#39;max_error&#39;,&#39;neg_mean_squared_log_error&#39;,&#39;r2&#39;,&#39;neg_mean_gamma_deviance&#39;] #round1 # [5,6,7,8,9,10,11,12,13,14,15] # [44,45,46,47] # [49,50,51,52] # Index([&#39;title_in_desc&#39;, &#39;topic_entertain&#39;, &#39;topic_video_game&#39;, &#39;topic_music&#39;,3 # &#39;topic_film_tv&#39;, &#39;upload_day_0&#39;, &#39;upload_day_1&#39;, &#39;upload_day_2&#39;,7 # &#39;upload_day_3&#39;, &#39;upload_day_4&#39;, &#39;upload_day_5&#39;, &#39;upload_day_6&#39;,11 # &#39;upload_time_of_day_afternoon&#39;, &#39;upload_time_of_day_late_night&#39;,13 # &#39;upload_time_of_day_morning&#39;, &#39;upload_time_of_day_night&#39;,15 # &#39;caption_False&#39;, &#39;caption_True&#39;, &#39;thumbnail_h_360&#39;, &#39;thumbnail_h_720&#39;,19 # &#39;categoryId_10&#39;, &#39;categoryId_20&#39;, &#39;categoryId_22&#39;, &#39;categoryId_23&#39;,23 # &#39;categoryId_24&#39;, &#39;categoryId_99&#39;, &#39;adult_LIKLEY&#39;, &#39;adult_UNLIKELY&#39;,27 # &#39;medical_LIKLEY&#39;, &#39;medical_UNLIKELY&#39;, &#39;racy_LIKLEY&#39;, &#39;racy_UNLIKELY&#39;,31 # &#39;spoof_LIKLEY&#39;, &#39;spoof_UNLIKELY&#39;, &#39;violence_LIKLEY&#39;,34 # &#39;violence_UNLIKELY&#39;, &#39;title_len&#39;, &#39;upper_pct&#39;, &#39;duration&#39;, &#39;num_tags&#39;,36 # &#39;emoji_count&#39;, &#39;thumb_word_count&#39;, &#39;like_percent&#39;,39 # &#39;thumb_words_in_title&#39;, &#39;thumb_words_in_tags&#39;, &#39;label_words_in_title&#39;,42 # &#39;label_words_in_tags&#39;, &#39;faces_surprised&#39;, &#39;faces_happy&#39;, &#39;faces_angry&#39;,46 # &#39;faces_other&#39;, &#39;days_since_upload&#39;, &#39;comment_likes_ratio&#39;,49 # &#39;likes_views_ratio&#39;, &#39;comment_views_ratio&#39;, &#39;channel_subs&#39;52 # param_grid={ # &#39;interaction_constraints&#39;:[[],[[49,50,51,52]],[[44,45,46,47]],[[5,6,7,8,9,10,11,12,13,14,15]]], # &#39;n_estimators&#39;:[250,500,750], # &#39;max_depth&#39;:[4,6,8], # &#39;learning_rate&#39;: [.1,.3,.5], # &quot;min_child_weight&quot;: [ .5,1, 1.5], # &#39;gamma&#39;: [0,.5,1], # &#39;reg_lambda&#39;: [0,5,10], # } # #round2 # param_grid={ # &#39;interaction_constraints&#39;:[[[49,50,51,52],[44,45,46,47]],[44,45,46,47]], # &#39;n_estimators&#39;:[25,100,200], # &#39;max_depth&#39;:[3,4], # &#39;learning_rate&#39;: [.4,.5], # &quot;min_child_weight&quot;: [ 0,.1,.4], # &#39;gamma&#39;: [1,2,5], # &#39;reg_lambda&#39;: [8,13], # } # #round3 # param_grid={ # &#39;interaction_constraints&#39;:[[[49,50,51,52],[44,45,46,47]]], # &#39;n_estimators&#39;:[80,100,120], # &#39;max_depth&#39;:[4], # &#39;learning_rate&#39;: [.4], # &quot;min_child_weight&quot;: [ 0,.1,.4], # &#39;gamma&#39;: [1], # &#39;reg_lambda&#39;: [13,17], # } # #round4 # param_grid={ # &#39;interaction_constraints&#39;:[[[49,50,51,52],[44,45,46,47]]], # &#39;n_estimators&#39;:[80], # &#39;max_depth&#39;:[4], # &#39;learning_rate&#39;: [.4], # &quot;min_child_weight&quot;: [ 0], # &#39;gamma&#39;: [1], # &#39;reg_lambda&#39;: [25,27], # } # optimal_params = GridSearchCV( # estimator=xgb.XGBClassifier(objective=&quot;binary:logistic&quot;), # param_grid=param_grid, # verbose=1, # n_jobs=-1, # cv=3 # ) # optimal_params.fit(X_train,y_train,verbose=True,early_stopping_rounds=10,eval_metric=&quot;auc&quot;,eval_set=[(X_test,y_test)]) # print(optimal_params.best_params_) # #Optimized hyperparameters # {&#39;gamma&#39;: 1, &#39;interaction_constraints&#39;: [[49, 50, 51, 52], [44, 45, 46, 47]], # &#39;learning_rate&#39;: 0.4, &#39;max_depth&#39;: 4, &#39;min_child_weight&#39;: 0, &#39;n_estimators&#39;: 80, &#39;reg_lambda&#39;: 27} # interaction_constraints=[[49, 50, 51, 52], [44, 45, 46, 47]], def run_xgboost(): X_train, X_test, y_train, y_test = train_test_split(X_data,y_data,test_size=0.2) reg_xgb = xgb.XGBClassifier(objective=&quot;binary:logistic&quot;,n_estimators=80, max_depth=4,learning_rate=.4,min_child_weight=0 ,gamma=1,reg_lambda=27,check_additivity=False) reg_xgb.fit(X_train,y_train,verbose=False,early_stopping_rounds=10,eval_set=[(X_test,y_test)]) explainer = shap.TreeExplainer(reg_xgb) shap_values = explainer.shap_values(X_train.reset_index().drop(&#39;index&#39;,axis=1)) predictions = reg_xgb.predict(X_test, ntree_limit = 0) df_preds = pd.concat([X_test.reset_index(),pd.DataFrame(y_test,columns=[&quot;high_views&quot;]).reset_index(),pd.DataFrame(predictions,columns=[&quot;preds&quot;])],axis=1).drop(&#39;index&#39;,axis=1) return X_train,shap_values,df_preds for i in range(100): if i ==0: X_train,shap_values,df_preds = run_xgboost() df_all_X_train = X_train.reset_index().drop(&#39;index&#39;,axis=1) df_all_preds = df_preds all_shap_values = shap_values else: X_train,shap_values,df_preds = run_xgboost() df_all_X_train = pd.concat([df_all_X_train,X_train.reset_index().drop(&#39;index&#39;,axis=1)], ignore_index=True) df_all_preds = pd.concat([df_all_preds,df_preds], ignore_index=True) all_shap_values=np.append(all_shap_values,shap_values,axis=0) . The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. . [20:46:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:46:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. . ntree_limit is deprecated, use `iteration_range` or model slicing instead. . [20:47:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [20:47:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [20:47:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. . from sklearn.metrics import confusion_matrix, accuracy_score accuracy = accuracy_score(df_all_preds[&#39;high_views&#39;], df_all_preds[&#39;preds&#39;]) confusionMatrix = confusion_matrix(df_all_preds[&#39;high_views&#39;], df_all_preds[&#39;preds&#39;]) print(f&#39;Accuracy Score: {accuracy}&#39;) print(f&#39;Confusion Matrix: n {confusionMatrix}&#39;) shap.summary_plot(all_shap_values,df_all_X_train,max_display=30) vals= np.abs(all_shap_values).mean(0) feature_importance = pd.DataFrame(list(zip(df_all_X_train.columns,vals)),columns=[&#39;col_name&#39;,&#39;feature_importance_vals&#39;]) feature_importance.sort_values(by=[&#39;feature_importance_vals&#39;],ascending=False,inplace=True) most_imortant = feature_importance[&quot;col_name&quot;][:30] for i in most_imortant: shap.dependence_plot(i,all_shap_values,df_all_X_train,show=False) x=df_all_X_train[i] y=[item[df_all_X_train.columns.get_loc(i)] for item in all_shap_values] if i in num_x_vars: mymodel=np.poly1d(np.polyfit(x,y,7)) myline = np.linspace(0,10000,1000000) pl.plot(myline,mymodel(myline), &#39;-k&#39; , linewidth=2) pl.legend(loc=&#39;upper left&#39;) pl.xlim(x.quantile(q=.005)*.75-.01,x.quantile(q=.995)*1.2) pl.ylim(np.quantile(y,.005)*.75-.01,np.quantile(y,.995)*1.2) pl.show() else: m, b = pl.polyfit( x, y , 1 ) pl.plot(x, m * x+b , &#39;-k&#39; , linewidth=2,label=&#39;y=&#39;+str(round(m,3))+&#39;x+&#39;+str(round(b,3))) pl.legend(loc=&#39;upper left&#39;) pl.xlim(min(x.quantile(q=.01)*.75,-.2),max(x.quantile(q=.99)*1.2,1)) pl.ylim(min(np.quantile(y,.005)*.75,-.01),max(np.quantile(y,.995)*1.2,.01)) pl.show() . Accuracy Score: 0.8525132275132276 Confusion Matrix: [[16055 2894] [ 2681 16170]] . No handles with labels found to put in legend. . No handles with labels found to put in legend. . No handles with labels found to put in legend. . No handles with labels found to put in legend. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. No handles with labels found to put in legend. . No handles with labels found to put in legend. . No handles with labels found to put in legend. . No handles with labels found to put in legend. . No handles with labels found to put in legend. . Polyfit may be poorly conditioned No handles with labels found to put in legend. . No handles with labels found to put in legend. . No handles with labels found to put in legend. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. No handles with labels found to put in legend. . No handles with labels found to put in legend. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . No handles with labels found to put in legend. . Polyfit may be poorly conditioned No handles with labels found to put in legend. .",
            "url": "https://jmmerrell.github.io/ws/2021/08/11/youtube_jadoinstuff_project.html",
            "relUrl": "/2021/08/11/youtube_jadoinstuff_project.html",
            "date": " • Aug 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "import os from numpy.core.fromnumeric import shape from numpy.lib.function_base import diff import pandas as pd import re from datetime import datetime, timedelta import numpy as np import matplotlib.pyplot as plt from pandas.core.indexes.base import Index import emoji from scipy import stats import fasttext from collections import Counter from pprint import pprint from IPython.display import display_html from itertools import chain,cycle from deep_translator import GoogleTranslator #create a function to identify which character are a country flag emoji def num_flag_emoji(text): num_country_emoji =0 for c in text: if &quot; U0001F1E6 U0001F1E8&quot; &lt;= c &lt;= &quot; U0001F1FF U0001F1FC&quot; or c in [&quot; U0001F3F4 U000e0067 U000e0062 U000e0065 U000e006e U000e0067 U000e007f&quot;, &quot; U0001F3F4 U000e0067 U000e0062 U000e0073 U000e0063 U000e0074 U000e007f&quot;, &quot; U0001F3F4 U000e0067 U000e0062 U000e0077 U000e006c U000e0073 U000e007f&quot;]: num_country_emoji += 1 return num_country_emoji os.chdir(&quot;C: Users merre Desktop ws data youtube_guerito&quot;) files= [&quot;jan_el_wero.txt&quot;, &quot;nate&#39;s_adventures.txt&quot;, &quot;nathan_seastrand.txt&quot;, &quot;rusos_reaccionan.txt&quot;, &quot;superholly.txt&quot;, &quot;vlog_güero.txt&quot;, &quot;wero_wero_tv.txt&quot;, &quot;american_boy.txt&quot;, &quot;dustin_luke.txt&quot;, &quot;el_gringo.txt&quot;, &quot;el_güerito.txt&quot;, &quot;ford_quarterman.txt&quot;, &quot;gringa_reacciona.txt&quot;] df = pd.DataFrame(columns=[&#39;video_id&#39;,&#39;title&#39;,&#39;title_len&#39;,&#39;words&#39;,&#39;upper_pct&#39;,&#39;emoji_count&#39;,&#39;upload_date&#39;,&#39;upload_time&#39;, &#39;upload_day&#39;,&#39;upload_time_of_day&#39;,&#39;viewCount&#39;,&#39;likeCount&#39;,&#39;dislikeCount&#39;,&#39;favoriteCount&#39;, &#39;commentCount&#39;,&#39;duration&#39;,&#39;definition&#39;,&#39;caption&#39;,&#39;licensedContent&#39;,&#39;thumbnail_url&#39;, &#39;thumbnail_w&#39;, &#39;thumbnail_h&#39;, &#39;tags&#39;,&#39;num_tags&#39;,&#39;categoryId&#39;,&#39;liveBroadcastContent&#39;,&#39;defaultAudioLanguage&#39;, &#39;topicCategories&#39;, &#39;channel&#39;, &#39;channel_subs&#39;, &#39;channel_views&#39;, &#39;channel_videos&#39;,&#39;desc&#39;]) #Loop through all the youtuber&#39;s data files and combine into on data frame for file in files: df_add= pd.read_csv(file) df = df.append(df_add.drop([&#39;Unnamed: 0&#39;],axis=1)) #Read in the files that have the thumbnail data, and combine with the youtuber data df_thumb = pd.read_csv(&quot;thumbnail_data_20210716_13-23-46.486750.txt&quot;).append(pd.read_csv(&quot;thumbnail_data_20210716_02-31-27.088616.txt&quot;)) df_all = pd.merge(df,df_thumb.drop(&#39;thumbnail_url&#39;,axis=1),on=&quot;video_id&quot;,how=&quot;inner&quot;).drop([&#39;emoji_count&#39;],axis=1) df_all[&#39;all_text&#39;] = df_all[&#39;title&#39;].astype(str) + df_all[&#39;tags&#39;].astype(str) + df_all[&#39;desc&#39;].astype(str) . print(df_all.isna().mean().round(4)) #looks like &quot;defaultAudioLanguage&quot; and &quot;topicCategories&quot; may need imputation and/or further exploration print(df[&#39;defaultAudioLanguage&#39;].value_counts(normalize=True)) print(df[&#39;topicCategories&#39;].value_counts(normalize=True)) #There are too many levels for the &#39;topicCategories&#39; variable and not realated to other variables, so imputation will be difficult #Therefore I will toss &#39;topicCategories&#39; from the study df_all = df_all.drop(&#39;topicCategories&#39;,axis=1) #There are two main languages used in the &#39;defaultAudioLanguage&#39; Spanish and English, so the follwoing model will predict the #language for the missing data using the tags of the youtube videos, tags with more english words in them will mean an English video #This was the most accurate model for predicting video language. Combined the title, tags, and description into all text column #Also used the most common words in the text to feed the model and have 95% accuracy in predictions PRETRAINED_MODEL_PATH = &quot;C: Users merre Desktop lid.176.bin&quot; model = fasttext.load_model(PRETRAINED_MODEL_PATH) df_all[&#39;predict_lang&#39;]=&#39;other&#39; ii=0 #finds which language is most probable, English or Spanish for i in df_all[&quot;all_text&quot;].replace(&#39;[&#39;,&#39;&#39;).replace(&#39;,&#39;,&#39;&#39;).replace(&#39;]&#39;,&#39;&#39;): sup =[item.replace(&#39; n&#39;, &#39;&#39;).replace(&quot;&#39;&quot;,&#39;&#39;) for item in re.findall(r&#39;[ s ? ! &quot; &#39;]+[a-zA-Z u00C0- u00FF]+[ s . ? ! &quot; &#39;]+&#39;,i)] sup=[j[0] for j in Counter(sup).most_common(15)] model_preds= list(model.predict(str(sup),k=4)) if str(model_preds[0][0].replace(&#39;&#39;&#39;__label__&#39;&#39;&#39;,&#39;&#39;)) in [&#39;en&#39;,&#39;es&#39;]: which_pred = 0 elif str(model_preds[0][1].replace(&#39;&#39;&#39;__label__&#39;&#39;&#39;,&#39;&#39;)) in [&#39;en&#39;,&#39;es&#39;]: which_pred =1 elif str(model_preds[0][2].replace(&#39;&#39;&#39;__label__&#39;&#39;&#39;,&#39;&#39;)) in [&#39;en&#39;,&#39;es&#39;]: which_pred =2 elif str(model_preds[0][3].replace(&#39;&#39;&#39;__label__&#39;&#39;&#39;,&#39;&#39;)) in [&#39;en&#39;,&#39;es&#39;]: which_pred =3 else: which_pred=4 if which_pred==4: df_all[&#39;predict_lang&#39;][ii]=&#39;es&#39; else: df_all[&#39;predict_lang&#39;][ii]=model_preds[0][which_pred].replace(&#39;&#39;&#39;__label__&#39;&#39;&#39;,&#39;&#39;) ii +=1 df_all[&#39;new_lang&#39;]=&#39;es&#39; #Use the predictions to replace the missing data for i in range(len(df_all[&#39;predict_lang&#39;])): if str(df_all[&#39;defaultAudioLanguage&#39;][i]).find(&#39;es&#39;) &gt;= 0: df_all[&#39;new_lang&#39;][i]= &#39;es&#39; elif str(df_all[&#39;defaultAudioLanguage&#39;][i]).find(&#39;en&#39;) &gt;= 0: df_all[&#39;new_lang&#39;][i]= &#39;en&#39; elif str(df_all[&#39;defaultAudioLanguage&#39;][i]) in [&#39;&#39;,&#39;nan&#39;,&#39;nan &#39;,&#39; &#39;]: df_all[&#39;new_lang&#39;][i]= df_all[&#39;predict_lang&#39;][i] else: df_all[&#39;new_lang&#39;][i]= df_all[&#39;predict_lang&#39;][i] #Show the side by side charts of the video language before and after imputation, and consolidation of the variable levels def display_side_by_side(*args,titles=cycle([&#39;&#39;])): html_str=&#39;&#39; for df,title in zip(args, chain(titles,cycle([&#39;&lt;/br&gt;&#39;])) ): html_str+=&#39;&lt;th style=&quot;text-align:center&quot;&gt;&lt;td style=&quot;vertical-align:top&quot;&gt;&#39; html_str+=f&#39;&lt;h2&gt;{title}&lt;/h2&gt;&#39; html_str+=df.to_html().replace(&#39;table&#39;,&#39;table style=&quot;display:inline&quot;&#39;) html_str+=&#39;&lt;/td&gt;&lt;/th&gt;&#39; display_html(html_str,raw=True) table1 = pd.DataFrame(df_all[[&#39;new_lang&#39;]].value_counts(),columns=[&#39;Count&#39;]) table2 = pd.DataFrame(df_all[[&#39;defaultAudioLanguage&#39;]].value_counts(),columns=[&#39;Count&#39;]) display_side_by_side(table2,table1, titles=[&#39;Before&#39;,&#39;After&#39;]) #Now we can drop defaultAudioLanguage since we have a replacement field for that df_all = df_all.drop(&#39;defaultAudioLanguage&#39;,axis=1) #We can now remove all rows that have missiong values since the rows with the most missing values are gone df_all = df_all.dropna() df_all = df_all.reset_index() all_labels = list(df_all[&quot;labels&quot;]) all_labels = [ x.split(&#39;,&#39;) for x in df_all[&quot;labels&quot;]] all_labels = [[re.sub(r&#39;[^a-zA-Z u00C0- u00FF s]&#39;, &quot; &quot;,i).strip(&#39; t n r&#39;).upper() for i in ii] for ii in all_labels] all_labels_flat = list(set([item for elem in all_labels for item in elem])) df_all[&#39;labels_words&#39;] = all_labels translation =[GoogleTranslator(source=&#39;en&#39;, target=&#39;es&#39;).translate(i) for i in all_labels_flat[0:500]] . video_id 0.0000 title 0.0000 title_len 0.0000 words 0.0000 upper_pct 0.0000 upload_date 0.0000 upload_time 0.0000 upload_day 0.0000 upload_time_of_day 0.0000 viewCount 0.0107 likeCount 0.0107 dislikeCount 0.0107 favoriteCount 0.0107 commentCount 0.0117 duration 0.0000 definition 0.0112 caption 0.0112 licensedContent 0.0112 thumbnail_url 0.0000 thumbnail_w 0.0000 thumbnail_h 0.0000 tags 0.0000 num_tags 0.0000 categoryId 0.0000 liveBroadcastContent 0.0000 defaultAudioLanguage 0.2026 topicCategories 0.2077 channel 0.0000 channel_subs 0.0000 channel_views 0.0000 channel_videos 0.0000 desc 0.0010 labels 0.0000 faces 0.0000 texts 0.0000 adult 0.0000 medical 0.0000 racy 0.0000 spoof 0.0000 violence 0.0000 all_text 0.0000 dtype: float64 es 0.397689 es-419 0.226377 es-MX 0.225697 en 0.134602 en-US 0.010877 es-US 0.002719 zxx 0.002039 Name: defaultAudioLanguage, dtype: float64 [&#39;https://en.wikipedia.org/wiki/Lifestyle_(sociology)&#39;] 0.309850 [&#39;https://en.wikipedia.org/wiki/Entertainment&#39;] 0.206566 [&#39;https://en.wikipedia.org/wiki/Food&#39;, &#39;https://en.wikipedia.org/wiki/Lifestyle_(sociology)&#39;] 0.076607 [&#39;https://en.wikipedia.org/wiki/Entertainment&#39;, &#39;https://en.wikipedia.org/wiki/Film&#39;] 0.071135 [&#39;https://en.wikipedia.org/wiki/Hobby&#39;, &#39;https://en.wikipedia.org/wiki/Lifestyle_(sociology)&#39;] 0.045144 ... [&#39;https://en.wikipedia.org/wiki/Entertainment&#39;, &#39;https://en.wikipedia.org/wiki/Tourism&#39;] 0.000684 [&#39;https://en.wikipedia.org/wiki/Hip_hop_music&#39;, &#39;https://en.wikipedia.org/wiki/Music&#39;, &#39;https://en.wikipedia.org/wiki/Music_of_Latin_America&#39;] 0.000684 [&#39;https://en.wikipedia.org/wiki/Hip_hop_music&#39;, &#39;https://en.wikipedia.org/wiki/Music_of_Latin_America&#39;] 0.000684 [&#39;https://en.wikipedia.org/wiki/Entertainment&#39;, &#39;https://en.wikipedia.org/wiki/Military&#39;] 0.000684 [&#39;https://en.wikipedia.org/wiki/Reggae&#39;] 0.000684 Name: topicCategories, Length: 68, dtype: float64 . Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar. C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:43: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:41: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:52: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:56: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:54: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:58: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . Before . Count . defaultAudioLanguage . es 627 | . es-419 378 | . es-MX 332 | . en 199 | . en-US 20 | . es-US 4 | . zxx 3 | . &lt;/table style=&quot;display:inline&quot;&gt;&lt;/td&gt;&lt;/th&gt;After . Count . new_lang . es 1620 | . en 340 | . &lt;/table style=&quot;display:inline&quot;&gt;&lt;/td&gt;&lt;/th&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; print(len(translation)) translation2 = translation + [GoogleTranslator(source=&#39;en&#39;, target=&#39;es&#39;).translate(i) for i in all_labels_flat[500:1000]] print(len(translation2)) . 500 1000 . print(len(translation2)) translation3 = translation2 + [GoogleTranslator(source=&#39;en&#39;, target=&#39;es&#39;).translate(i) for i in all_labels_flat[1000:1500]] print(len(translation3)) . 1000 1500 . print(len(translation3)) translation4 = translation3 + [GoogleTranslator(source=&#39;en&#39;, target=&#39;es&#39;).translate(i) for i in all_labels_flat[1500:]] print(len(translation4)) . 1500 1574 . #Scans the title for any emojis in general and also of country flags df_all[&quot;flag_emoji_count&quot;]=[(num_flag_emoji(x)&gt;0)*1 for x in df_all[&quot;title&quot;]] df_all[&quot;emoji_count&quot;]= [((emoji.emoji_count(x) - num_flag_emoji(x)/2)&gt;0)*1 for x in df_all[&quot;title&quot;]] ###Create variables to see how well title reflects the description, thumbnail and tags of the video #The text read in by the ggogle vision text detection is messy. #Need to clean up and create new variable &quot;thumb_words&quot;, thumb_word_count&quot; df_all[&#39;labels_words_spanish&#39;] = df_all[&#39;labels_words&#39;] df_all[&#39;labels_words_spanish&#39;] = [[translation4[x].upper() for x in range(len(all_labels_flat)) for y in range(len(z)) if z[y]==all_labels_flat[x]] for z in df_all[&#39;labels_words_spanish&#39;]] df_all[&#39;labels_word_count&#39;]=[len(df_all[&#39;labels_words&#39;][i]) for i in range(len(df_all))] discard_list = [&quot;&quot;,&quot;B&quot;,&#39;C&#39;,&#39;D&#39;,&#39;&#39;,&#39;F&#39;,&#39;G&#39;,&#39;H&#39;,&#39;&#39;,&#39;J&#39;,&#39;K&#39;,&#39;L&#39;,&#39;M&#39;,&#39;N&#39;,&#39;&#39;,&#39;P&#39;,&#39;Q&#39;,&#39;R&#39;,&#39;S&#39;,&#39;T&#39;,&#39;V&#39;,&#39;W&#39;,&#39;X&#39;,&#39;Z&#39;] df_all[&#39;thumb_words&#39;]=[list(set(re.sub(r&#39;[^a-zA-Z u00C0- u00FF]&#39;, &quot; &quot;,str(re.findall(r&#39;[^ n][ s ? ! &quot; &#39;]+[a-zA-Z u00C0- u00FF]+[ s . ? ! &quot; &#39;]?&#39;,str(df_all[&#39;texts&#39;][i])))).upper().split(&quot; &quot;))) for i in range(len(df_all))] for i in range(len(df_all)): test_list = df_all[&#39;thumb_words&#39;][i] remove_list = discard_list df_all[&#39;thumb_words&#39;][i] = [i for i in test_list if i not in remove_list] df_all[&#39;thumb_word_count&#39;]=[len(df_all[&#39;thumb_words&#39;][i]) for i in range(len(df_all))] df_all[&#39;title_in_desc&#39;]=[((df_all[&#39;desc&#39;][i].upper().find(df_all[&#39;title&#39;][i].upper()))&gt;-1)*1 for i in range(len(df_all))] df_all[&#39;thumb_words_in_title&#39;]=[ sum([(str(df_all[&#39;title&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;thumb_words&#39;][z]]) for z in range(len(df_all[&#39;title&#39;]))] df_all[&#39;thumb_words_in_tags&#39;]=[ sum([(str(df_all[&#39;tags&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;thumb_words&#39;][z]]) for z in range(len(df_all[&#39;tags&#39;]))] df_all[&#39;label_words_in_title&#39;]=&#39;&#39; df_all[&#39;label_words_in_tags&#39;]=&#39;&#39; for z in range(len(df_all[&#39;title&#39;])): if df_all[&#39;new_lang&#39;][z]==&#39;es&#39;: df_all[&#39;label_words_in_title&#39;][z]=sum([(str(df_all[&#39;title&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;labels_words_spanish&#39;][z]]) df_all[&#39;label_words_in_tags&#39;][z]=sum([(str(df_all[&#39;tags&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;labels_words_spanish&#39;][z]]) else: df_all[&#39;label_words_in_title&#39;][z]=sum([(str(df_all[&#39;title&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;labels_words&#39;][z]]) df_all[&#39;label_words_in_tags&#39;][z]=sum([(str(df_all[&#39;tags&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;labels_words&#39;][z]]) ##See how many faces are in the thumbnails df_all[&#39;faces_surprised&#39;]=[sum([(str(y)==&#39;suprised&#39;)*1 for y in re.findall(r&#39;[a-zA-Z u00C0- u00FF]+&#39;,df_all[&#39;faces&#39;][z])]) for z in range(len(df_all[&#39;faces&#39;]))] df_all[&#39;faces_angry&#39;]=[sum([(str(y)==&#39;angry&#39;)*1 for y in re.findall(r&#39;[a-zA-Z u00C0- u00FF]+&#39;,df_all[&#39;faces&#39;][z])]) for z in range(len(df_all[&#39;faces&#39;]))] df_all[&#39;faces_happy&#39;]=[sum([(str(y)==&#39;happy&#39;)*1 for y in re.findall(r&#39;[a-zA-Z u00C0- u00FF]+&#39;,df_all[&#39;faces&#39;][z])]) for z in range(len(df_all[&#39;faces&#39;]))] df_all[&#39;faces_other&#39;]=[sum([(str(y)==&#39;other&#39;)*1 for y in re.findall(r&#39;[a-zA-Z u00C0- u00FF]+&#39;,df_all[&#39;faces&#39;][z])]) for z in range(len(df_all[&#39;faces&#39;]))] ###Create varibales to see how much people like the videos df_all[&#39;likes_views_ratio&#39;]=df_all[&#39;likeCount&#39;]/df_all[&#39;viewCount&#39;] df_all[&#39;likes_subs_ratio&#39;]=df_all[&#39;likeCount&#39;]/df_all[&#39;channel_subs&#39;] df_all[&#39;comment_views_ratio&#39;]=df_all[&#39;commentCount&#39;]/df_all[&#39;viewCount&#39;] try: df_all[&#39;comment_likes_ratio&#39;]=df_all[&#39;commentCount&#39;]/df_all[&#39;likeCount&#39;] except: df_all[&#39;comment_likes_ratio&#39;]=df_all[&#39;commentCount&#39;].mean()/df_all[&#39;likeCount&#39;].mean() df_all[&#39;comment_subs_ratio&#39;]=df_all[&#39;commentCount&#39;]/df_all[&#39;channel_subs&#39;] df_all[&#39;views_favorite_ratio&#39;]=df_all[&#39;favoriteCount&#39;]/df_all[&#39;viewCount&#39;] df_all[&#39;like_percent&#39;]=df_all[&#39;likeCount&#39;]/(df_all[&#39;likeCount&#39;]+df_all[&#39;dislikeCount&#39;]) max_date=max(df_all[&#39;upload_date&#39;]) df_all[&#39;days_since_upload&#39;]= (pd.to_datetime(max_date) - pd.to_datetime(df_all[&#39;upload_date&#39;])).dt.days +1 df_all[&#39;views_per_day&#39;] = df_all[&#39;viewCount&#39;]/df_all[&#39;days_since_upload&#39;] df_all[&#39;views_per_day_per_sub&#39;] = df_all[&#39;viewCount&#39;]/df_all[&#39;days_since_upload&#39;]/df_all[&#39;channel_subs&#39;] df_all[&#39;views_per_sub&#39;] = df_all[&#39;viewCount&#39;]/df_all[&#39;channel_subs&#39;] #Combine level of some categorical fields for i in range(len(df_all)): if df_all[&quot;adult&quot;][i] in [&#39;VERY_UNLIKELY&#39;,&#39;UNLIKELY&#39;]: df_all[&quot;adult&quot;][i] = &#39;UNLIKELY&#39; else: df_all[&quot;adult&quot;][i] = &#39;LIKLEY&#39; if df_all[&quot;medical&quot;][i] in [&#39;VERY_UNLIKELY&#39;,&#39;UNLIKELY&#39;]: df_all[&quot;medical&quot;][i] = &#39;UNLIKELY&#39; else: df_all[&quot;medical&quot;][i] = &#39;LIKLEY&#39; if df_all[&quot;racy&quot;][i] in [&#39;VERY_UNLIKELY&#39;,&#39;UNLIKELY&#39;]: df_all[&quot;racy&quot;][i] = &#39;UNLIKELY&#39; else: df_all[&quot;racy&quot;][i] = &#39;LIKLEY&#39; if df_all[&quot;spoof&quot;][i] in [&#39;VERY_UNLIKELY&#39;,&#39;UNLIKELY&#39;]: df_all[&quot;spoof&quot;][i] = &#39;UNLIKELY&#39; else: df_all[&quot;spoof&quot;][i] = &#39;LIKLEY&#39; if df_all[&quot;violence&quot;][i] in [&#39;VERY_UNLIKELY&#39;,&#39;UNLIKELY&#39;]: df_all[&quot;violence&quot;][i] = &#39;UNLIKELY&#39; else: df_all[&quot;violence&quot;][i] = &#39;LIKLEY&#39; if df_all[&quot;categoryId&quot;][i] not in [24,22,19,27,26,23]: df_all[&quot;categoryId&quot;][i] = 99 . A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . print(df_all.columns) # plt.plot( df_all[&#39;days_since_upload&#39;],df_all[&#39;viewCount&#39;]) # plt.xlabel(&#39;Time (hr)&#39;) # plt.ylabel(&#39;Position (km)&#39;) df_model = df_all[[&#39;title_len&#39;, &#39;words&#39;, &#39;upper_pct&#39;, &#39;upload_day&#39;, &#39;upload_time_of_day&#39;, &#39;viewCount&#39;, &#39;likeCount&#39;, &#39;dislikeCount&#39;, &#39;favoriteCount&#39;, &#39;commentCount&#39;, &#39;duration&#39;, &#39;definition&#39;, &#39;caption&#39;, &#39;licensedContent&#39;, &#39;thumbnail_h&#39;, &#39;num_tags&#39;, &#39;categoryId&#39;, &#39;liveBroadcastContent&#39;, &#39;channel_subs&#39;, &#39;channel_views&#39;, &#39;channel_videos&#39;, &#39;adult&#39;, &#39;medical&#39;, &#39;racy&#39;, &#39;spoof&#39;, &#39;violence&#39;,&#39;new_lang&#39;, &#39;flag_emoji_count&#39;, &#39;emoji_count&#39;, &#39;thumb_word_count&#39;, &#39;title_in_desc&#39;, &#39;thumb_words_in_title&#39;, &#39;thumb_words_in_tags&#39;, &#39;label_words_in_title&#39;, &#39;label_words_in_tags&#39;, &#39;days_since_upload&#39;, &#39;views_per_day&#39;, &#39;views_per_day_per_sub&#39;, &#39;views_per_sub&#39;, &#39;likes_views_ratio&#39;,&#39;comment_views_ratio&#39;, &#39;likes_subs_ratio&#39;, &#39;comment_likes_ratio&#39;,&#39;comment_subs_ratio&#39;, &#39;views_favorite_ratio&#39;, &#39;like_percent&#39;,&#39;faces_surprised&#39;,&#39;faces_happy&#39;,&#39;faces_angry&#39;,&#39;faces_other&#39;]] print(shape(df_model)) df_model.describe() # plt.scatter(df_all[&#39;views_like_ratio&#39;],df_all[&#39;views_per_sub&#39;]) # plt.show() # plt.subplot(2,3,1) # if use subplot #placed a log transformation on number of views to show a more normal distribution #There are some extreme outliers in views, so we will drop these few outliers to not inflate the RMSE of the predicted views np.power(df_model[&#39;views_per_sub&#39;].astype(float)[(stats.zscore(np.power(df_all[&#39;views_per_sub&#39;].astype(float),1/6)) &lt; 2.5)],1/6).hist(bins=20) plt.show() # print(sum(df_model[&#39;viewCount&#39;].astype(float)[(stats.zscore(np.power(df_all[&#39;viewCount&#39;].astype(float),1/5.75)) &lt; 3)])) df_model2 = df_model[np.abs(stats.zscore(np.power(df_model[&#39;views_per_sub&#39;].astype(float),1/10))) &lt; 2.5] print(shape(df_model2)) # df_all[&#39;duration&#39;][(np.abs(stats.zscore(df_all[&#39;duration&#39;])) &lt; 3)].hist(bins=20) # plt.title(&#39;duration&#39;) # df_all[&#39;channel_subs&#39;][(np.abs(stats.zscore(df_all[&#39;channel_subs&#39;].astype(float))) &lt; 3)].hist(bins=20) # plt.title(&#39;channel_subs&#39;) # plt.show() # df_all.dtypes ##For the CategoryIDs field these are what the values correspond to # ID Category name # 1 Film &amp; Animation # 2 Autos &amp; Vehicles # 10 Music # 15 Pets &amp; Animals # 17 Sports # 19 Travel &amp; Events # 20 Gaming # 22 People &amp; Blogs # 23 Comedy # 24 Entertainment # 25 News &amp; Politics # 26 Howto &amp; Style # 27 Education # 28 Science &amp; Technology # 29 Nonprofits &amp; Activism ##For the upload_day field 0=Monday and 6=Sunday . Index([&#39;index&#39;, &#39;video_id&#39;, &#39;title&#39;, &#39;title_len&#39;, &#39;words&#39;, &#39;upper_pct&#39;, &#39;upload_date&#39;, &#39;upload_time&#39;, &#39;upload_day&#39;, &#39;upload_time_of_day&#39;, &#39;viewCount&#39;, &#39;likeCount&#39;, &#39;dislikeCount&#39;, &#39;favoriteCount&#39;, &#39;commentCount&#39;, &#39;duration&#39;, &#39;definition&#39;, &#39;caption&#39;, &#39;licensedContent&#39;, &#39;thumbnail_url&#39;, &#39;thumbnail_w&#39;, &#39;thumbnail_h&#39;, &#39;tags&#39;, &#39;num_tags&#39;, &#39;categoryId&#39;, &#39;liveBroadcastContent&#39;, &#39;channel&#39;, &#39;channel_subs&#39;, &#39;channel_views&#39;, &#39;channel_videos&#39;, &#39;desc&#39;, &#39;labels&#39;, &#39;faces&#39;, &#39;texts&#39;, &#39;adult&#39;, &#39;medical&#39;, &#39;racy&#39;, &#39;spoof&#39;, &#39;violence&#39;, &#39;all_text&#39;, &#39;predict_lang&#39;, &#39;new_lang&#39;, &#39;labels_words&#39;, &#39;flag_emoji_count&#39;, &#39;emoji_count&#39;, &#39;labels_words_spanish&#39;, &#39;labels_word_count&#39;, &#39;thumb_words&#39;, &#39;thumb_word_count&#39;, &#39;title_in_desc&#39;, &#39;thumb_words_in_title&#39;, &#39;thumb_words_in_tags&#39;, &#39;label_words_in_title&#39;, &#39;label_words_in_tags&#39;, &#39;faces_surprised&#39;, &#39;faces_angry&#39;, &#39;faces_happy&#39;, &#39;faces_other&#39;, &#39;likes_views_ratio&#39;, &#39;comment_views_ratio&#39;, &#39;views_favorite_ratio&#39;, &#39;like_percent&#39;, &#39;days_since_upload&#39;, &#39;views_per_day&#39;, &#39;views_per_day_per_sub&#39;, &#39;views_per_sub&#39;, &#39;likes_subs_ratio&#39;, &#39;comment_likes_ratio&#39;, &#39;comment_subs_ratio&#39;], dtype=&#39;object&#39;) (1932, 50) . (1897, 50) . import seaborn as sns ######Prepare the data for random Forest Model #List of categorical explanatory variables # cat_x_vars=[&#39;upload_day&#39;, &#39;upload_time_of_day&#39;, # &#39;caption&#39;,&#39;categoryId&#39;, &#39;title_in_desc&#39;, # &#39;adult&#39;, &#39;medical&#39;, &#39;racy&#39;, &#39;spoof&#39;, &#39;violence&#39;,&#39;new_lang&#39;] cat_x_vars=[&#39;upload_day&#39;, &#39;upload_time_of_day&#39;, &#39;caption&#39;,&#39;categoryId&#39;, &#39;title_in_desc&#39;, &#39;adult&#39;, &#39;racy&#39;, &#39;spoof&#39;, &#39;new_lang&#39;] for i in df_all.columns: temp_df = pd.DataFrame(df_all[i].value_counts(normalize=True).sort_values(ascending=False)) if shape(temp_df)[0] &lt;= 50 and i in cat_x_vars: print(temp_df) cat_features = pd.get_dummies(df_model2[cat_x_vars]) # , &#39;licensedContent&#39;, &#39;liveBroadcastContent&#39; #List of numeric explanatory variables # num_x_vars=[&#39;title_len&#39;, &#39;upper_pct&#39;, &#39;duration&#39;, &#39;num_tags&#39;, # &#39;flag_emoji_count&#39;,&#39;emoji_count&#39;, &#39;thumb_word_count&#39;, # &#39;thumb_words_in_title&#39;, &#39;thumb_words_in_tags&#39;, &#39;label_words_in_title&#39;, # &#39;label_words_in_tags&#39;,&#39;faces_surprised&#39;,&#39;faces_happy&#39;,&#39;faces_angry&#39;,&#39;faces_other&#39;,&#39;days_since_upload&#39;, # &#39;comment_views_ratio&#39;, &#39;like_percent&#39;,&#39;likes_views_ratio&#39;,&#39;channel_subs&#39;] #List of numeric explanatory variables num_x_vars=[&#39;title_len&#39;, &#39;upper_pct&#39;, &#39;duration&#39;, &#39;num_tags&#39;, &#39;emoji_count&#39;, &#39;thumb_word_count&#39;, &#39;like_percent&#39;, &#39;thumb_words_in_title&#39;, &#39;thumb_words_in_tags&#39;, &#39;label_words_in_title&#39;, &#39;label_words_in_tags&#39;,&#39;faces_surprised&#39;,&#39;faces_happy&#39;,&#39;faces_angry&#39;,&#39;faces_other&#39;, &#39;days_since_upload&#39;, &#39;comment_likes_ratio&#39;,&#39;likes_views_ratio&#39;,&#39;comment_views_ratio&#39;,&#39;channel_subs&#39;] #, &#39;days_since_upload&#39;,&#39;channel_subs&#39;,&#39;channel_views&#39;, &#39;channel_videos&#39;,&#39;comment_views_ratio&#39;, &#39;views_favorite_ratio&#39;, &#39;like_percent&#39;,&#39;views_like_ratio&#39;, num_features=df_model2[num_x_vars] for var in num_x_vars: num_features[var] = num_features[var].astype(float) print(num_features.describe()) plt.figure(figsize=(20,20)) sns.heatmap(num_features.corr(),annot=True,cmap=&quot;RdYlGn&quot;,annot_kws={&quot;size&quot;:15}) . upload_day 0 0.173913 2 0.166149 4 0.143375 3 0.139752 5 0.139234 6 0.130435 1 0.107143 upload_time_of_day night 0.560041 afternoon 0.325569 morning 0.062629 late_night 0.051760 caption False 0.912526 True 0.087474 categoryId 24 0.444617 22 0.296584 19 0.092650 27 0.065217 26 0.047619 23 0.030538 99 0.022774 adult UNLIKELY 0.971532 LIKLEY 0.028468 racy UNLIKELY 0.824017 LIKLEY 0.175983 spoof LIKLEY 0.561077 UNLIKELY 0.438923 new_lang es 0.825052 en 0.174948 title_in_desc 0 0.95911 1 0.04089 . A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . title_len upper_pct duration num_tags emoji_count count 1897.000000 1897.000000 1897.000000 1897.000000 1897.000000 mean 61.863469 0.403162 12.177561 24.033737 0.050606 std 23.064847 0.340798 10.039595 8.738520 0.219250 min 6.000000 0.000000 0.133333 1.000000 0.000000 25% 43.000000 0.000000 6.566667 19.000000 0.000000 50% 58.000000 0.375000 11.466667 25.000000 0.000000 75% 84.000000 0.692308 15.866667 31.000000 0.000000 max 100.000000 1.000000 244.650000 45.000000 1.000000 thumb_word_count like_percent thumb_words_in_title count 1897.00000 1897.000000 1897.000000 mean 4.73010 0.984336 2.198208 std 6.30492 0.019377 2.384182 min 0.00000 0.645161 0.000000 25% 1.00000 0.982449 0.000000 50% 4.00000 0.988439 1.000000 75% 6.00000 0.991952 4.000000 max 116.00000 1.000000 14.000000 thumb_words_in_tags label_words_in_title label_words_in_tags count 1897.000000 1897.000000 1897.000000 mean 2.535055 0.094360 0.296257 std 2.667384 0.331304 0.695640 min 0.000000 0.000000 0.000000 25% 0.000000 0.000000 0.000000 50% 2.000000 0.000000 0.000000 75% 4.000000 0.000000 0.000000 max 22.000000 3.000000 6.000000 faces_surprised faces_happy faces_angry faces_other count 1897.000000 1897.000000 1897.000000 1897.000000 mean 0.326832 0.989457 0.042172 0.778071 std 0.658186 1.127848 0.250133 1.410398 min 0.000000 0.000000 0.000000 0.000000 25% 0.000000 0.000000 0.000000 0.000000 50% 0.000000 1.000000 0.000000 0.000000 75% 0.000000 2.000000 0.000000 1.000000 max 5.000000 12.000000 3.000000 38.000000 days_since_upload comment_likes_ratio likes_views_ratio count 1897.000000 1897.000000 1897.000000 mean 796.163416 0.137330 0.061702 std 744.308051 0.143132 0.031095 min 1.000000 0.000000 0.003928 25% 241.000000 0.076097 0.042318 50% 575.000000 0.104082 0.057127 75% 1118.000000 0.145656 0.076294 max 3297.000000 2.076923 0.277778 comment_views_ratio channel_subs count 1897.000000 1.897000e+03 mean 0.009661 1.073822e+06 std 0.018212 1.276597e+06 min 0.000000 2.260000e+02 25% 0.003504 2.960000e+05 50% 0.005906 6.460000e+05 75% 0.009629 1.030000e+06 max 0.296703 4.280000e+06 . &lt;AxesSubplot:&gt; . df_model2[&#39;high_views&#39;] = pd.qcut(df_model2[&#39;views_per_sub&#39;], [0,0.5, 1], labels=[0,1]) from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.metrics import confusion_matrix, accuracy_score import shap import xgboost as xgb import matplotlib.pylab as pl # pred_features = np.power(df_model2[&#39;viewCount&#39;].astype(float),1/6) pred_features = df_model2[&#39;high_views&#39;].astype(int) # data = pd.concat([cat_features,num_features],axis=1) X_data=pd.concat([cat_features,num_features],axis=1) y_data=pred_features def split_data_train_model(labels, data): # 20% examples in test data train, test, train_labels, test_labels = train_test_split(X_data, y_data, test_size=0.2) # training data fit return test, test_labels, regressor # x_test, x_test_labels, regressor = split_data_train_model(y_data, X_data) # regressor = RandomForestRegressor(n_estimators=250) # regressor.fit(X_train, y_train) # predictions = regressor.predict(X_train) # reg_xgb = xgb.XGBRegressor() # objective =&#39;reg:linear&#39; # reg_xgb.fit(X_train,y_train,verbose=True,early_stopping_rounds=10,eval_set=[(X_test,y_test)]) ###optimize parameters for XGBoost X_train, X_test, y_train, y_test = train_test_split(X_data,y_data,test_size=0.2) # Index([&#39;title_in_desc&#39;, &#39;upload_day_0&#39;, &#39;upload_day_1&#39;, &#39;upload_day_2&#39;,3 # &#39;upload_day_3&#39;, &#39;upload_day_4&#39;, &#39;upload_day_5&#39;, &#39;upload_day_6&#39;,7 # &#39;upload_time_of_day_afternoon&#39;, &#39;upload_time_of_day_late_night&#39;,9 # &#39;upload_time_of_day_morning&#39;, &#39;upload_time_of_day_night&#39;,11 # &#39;caption_False&#39;, &#39;caption_True&#39;, &#39;categoryId_19&#39;, &#39;categoryId_22&#39;,15 # &#39;categoryId_23&#39;, &#39;categoryId_24&#39;, &#39;categoryId_26&#39;, &#39;categoryId_27&#39;,19 # &#39;categoryId_99&#39;, &#39;adult_LIKLEY&#39;, &#39;adult_UNLIKELY&#39;, &#39;racy_LIKLEY&#39;,23 # &#39;racy_UNLIKELY&#39;, &#39;spoof_LIKLEY&#39;, &#39;spoof_UNLIKELY&#39;, &#39;new_lang_en&#39;,27 # &#39;new_lang_es&#39;, &#39;title_len&#39;, &#39;upper_pct&#39;, &#39;duration&#39;, &#39;num_tags&#39;,32 # &#39;emoji_count&#39;, &#39;thumb_word_count&#39;, &#39;like_percent&#39;,35 # &#39;thumb_words_in_title&#39;, &#39;thumb_words_in_tags&#39;, &#39;label_words_in_title&#39;,38 # &#39;label_words_in_tags&#39;, &#39;faces_surprised&#39;, &#39;faces_happy&#39;, &#39;faces_angry&#39;,42 # &#39;faces_other&#39;, &#39;days_since_upload&#39;, &#39;comment_likes_ratio&#39;,45 # &#39;likes_views_ratio&#39;, &#39;comment_views_ratio&#39;, &#39;channel_subs&#39;],48 # dtype=&#39;object&#39;) # [&#39;neg_mean_squared_error&#39;,&#39;max_error&#39;,&#39;neg_mean_squared_log_error&#39;,&#39;r2&#39;,&#39;neg_mean_gamma_deviance&#39;] # #round1 # param_grid={ # &#39;interaction_constraints&#39;:[[],[[45,46,47,48]],[[40,41,42,43]],[[2,3,4,5,6,7,8,9,10,11]]], # &#39;n_estimators&#39;:[250,500,750], # &#39;max_depth&#39;:[4,6,8], # &#39;learning_rate&#39;: [.1,.3,.5], # &quot;min_child_weight&quot;: [ .5,1, 1.5], # &#39;gamma&#39;: [0,.5,1], # &#39;reg_lambda&#39;: [0,5,10], # } # #round2 # param_grid={ # &#39;interaction_constraints&#39;:[[[45,46,47,48],[40,41,42,43]],[45,46,47,48]], # &#39;n_estimators&#39;:[50,100,175], # &#39;max_depth&#39;:[5,6], # &#39;learning_rate&#39;: [.2,.3,.4], # &quot;min_child_weight&quot;: [ .8,1, 1.2], # &#39;gamma&#39;: [.3,.5], # &#39;reg_lambda&#39;: [12,15,20], # } # #round3 # param_grid={ # &#39;interaction_constraints&#39;:[[[45,46,47,48],[40,41,42,43]]], # &#39;n_estimators&#39;:[80,100,120], # &#39;max_depth&#39;:[6], # &#39;learning_rate&#39;: [.4], # &quot;min_child_weight&quot;: [1.2], # &#39;gamma&#39;: [.5], # &#39;reg_lambda&#39;: [15], # } # #best params # {&#39;gamma&#39;: 0.5, &#39;interaction_constraints&#39;: [[45, 46, 47, 48], [40, 41, 42, 43]], # &#39;learning_rate&#39;: 0.4, &#39;max_depth&#39;: 6, &#39;min_child_weight&#39;: 1.2, &#39;n_estimators&#39;: 80, &#39;reg_lambda&#39;: 15} # optimal_params = GridSearchCV( # estimator=xgb.XGBClassifier(objective=&quot;binary:logistic&quot;), # param_grid=param_grid, # verbose=1, # n_jobs=-1, # cv=3 # ) # optimal_params.fit(X_train,y_train,verbose=True,early_stopping_rounds=10,eval_metric=&quot;auc&quot;,eval_set=[(X_test,y_test)]) # print(optimal_params.best_params_) def run_xgboost(): X_train, X_test, y_train, y_test = train_test_split(X_data,y_data,test_size=0.2) reg_xgb = xgb.XGBClassifier(objective=&quot;binary:logistic&quot;,n_estimators=80, max_depth=6,learning_rate=.4,min_child_weight=1.2 ,gamma=.5,reg_lambda=15,interaction_constraints=[[45, 46, 47, 48], [40, 41, 42, 43]],check_additivity=False) reg_xgb.fit(X_train,y_train,verbose=False,early_stopping_rounds=10,eval_set=[(X_test,y_test)]) explainer = shap.TreeExplainer(reg_xgb) shap_values = explainer.shap_values(X_train.reset_index().drop(&#39;index&#39;,axis=1)) predictions = reg_xgb.predict(X_test, ntree_limit = 0) df_preds = pd.concat([X_test.reset_index(),pd.DataFrame(y_test,columns=[&quot;high_views&quot;]).reset_index(),pd.DataFrame(predictions,columns=[&quot;preds&quot;])],axis=1).drop(&#39;index&#39;,axis=1) return X_train,shap_values,df_preds for i in range(100): if i ==0: X_train,shap_values,df_preds = run_xgboost() df_all_X_train = X_train.reset_index().drop(&#39;index&#39;,axis=1) df_all_preds = df_preds all_shap_values = shap_values else: X_train,shap_values,df_preds = run_xgboost() df_all_X_train = pd.concat([df_all_X_train,X_train.reset_index().drop(&#39;index&#39;,axis=1)], ignore_index=True) df_all_preds = pd.concat([df_all_preds,df_preds], ignore_index=True) all_shap_values=np.append(all_shap_values,shap_values,axis=0) . A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . [00:22:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. . ntree_limit is deprecated, use `iteration_range` or model slicing instead. . [00:22:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:22:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:22:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:23:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:23:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:23:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:23:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:23:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:23:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:23:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:23:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:23:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:23:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:23:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:23:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:23:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:23:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:23:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:23:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. [00:23:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: Parameters: { &#34;check_additivity&#34; } might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [00:23:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. . from sklearn.metrics import confusion_matrix, accuracy_score accuracy = accuracy_score(df_all_preds[&#39;high_views&#39;], df_all_preds[&#39;preds&#39;]) confusionMatrix = confusion_matrix(df_all_preds[&#39;high_views&#39;], df_all_preds[&#39;preds&#39;]) print(df_all_preds.head()) print(f&#39;Accuracy Score: {accuracy}&#39;) print(f&#39;Confusion Matrix: n {confusionMatrix}&#39;) shap.summary_plot(all_shap_values,df_all_X_train,max_display=30) vals= np.abs(all_shap_values).mean(0) feature_importance = pd.DataFrame(list(zip(df_all_X_train.columns,vals)),columns=[&#39;col_name&#39;,&#39;feature_importance_vals&#39;]) feature_importance.sort_values(by=[&#39;feature_importance_vals&#39;],ascending=False,inplace=True) most_imortant = feature_importance[&quot;col_name&quot;][:30] for i in most_imortant: shap.dependence_plot(i,all_shap_values,df_all_X_train,show=False) x=df_all_X_train[i] y=[item[df_all_X_train.columns.get_loc(i)] for item in all_shap_values] if i in num_x_vars: mymodel=np.poly1d(np.polyfit(x,y,7)) myline = np.linspace(0,10000,1000000) pl.plot(myline,mymodel(myline), &#39;-k&#39; , linewidth=2) pl.legend(loc=&#39;upper left&#39;) pl.xlim(x.quantile(q=.005)*.75-.01,x.quantile(q=.995)*1.2) pl.ylim(np.quantile(y,.005)*.75-.01,np.quantile(y,.995)*1.2) pl.show() else: m, b = pl.polyfit( x, y , 1 ) pl.plot(x, m * x+b , &#39;-k&#39; , linewidth=2,label=&#39;y=&#39;+str(round(m,3))+&#39;x+&#39;+str(round(b,3))) pl.legend(loc=&#39;upper left&#39;) pl.xlim(min(x.quantile(q=.01)*.75,-.2),max(x.quantile(q=.99)*1.2,1)) pl.ylim(min(np.quantile(y,.005)*.75,-.01),max(np.quantile(y,.995)*1.2,.01)) pl.show() . title_in_desc upload_day_0 upload_day_1 upload_day_2 upload_day_3 0 0 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 3 0 0 0 1 0 4 0 0 0 0 1 upload_day_4 upload_day_5 upload_day_6 upload_time_of_day_afternoon 0 0 1 0 1 1 0 1 0 1 2 0 1 0 0 3 0 0 0 0 4 0 0 0 1 upload_time_of_day_late_night ... faces_happy faces_angry faces_other 0 0 ... 1.0 0.0 1.0 1 0 ... 0.0 0.0 0.0 2 0 ... 0.0 0.0 2.0 3 0 ... 1.0 0.0 2.0 4 0 ... 1.0 0.0 0.0 days_since_upload comment_likes_ratio likes_views_ratio 0 194.0 0.482759 0.044410 1 719.0 0.063877 0.050826 2 1965.0 0.100249 0.032208 3 281.0 0.044733 0.049889 4 28.0 0.153285 0.077686 comment_views_ratio channel_subs high_views preds 0 0.021440 2200.0 1 1 1 0.003247 1030000.0 1 1 2 0.003229 646000.0 1 0 3 0.002232 1030000.0 1 1 4 0.011908 51600.0 0 0 [5 rows x 51 columns] Accuracy Score: 0.7685 Confusion Matrix: [[14266 4609] [ 4188 14937]] . No handles with labels found to put in legend. . No handles with labels found to put in legend. . No handles with labels found to put in legend. . No handles with labels found to put in legend. . No handles with labels found to put in legend. . No handles with labels found to put in legend. . No handles with labels found to put in legend. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. No handles with labels found to put in legend. . Polyfit may be poorly conditioned No handles with labels found to put in legend. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. No handles with labels found to put in legend. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. No handles with labels found to put in legend. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. No handles with labels found to put in legend. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . No handles with labels found to put in legend. . No handles with labels found to put in legend. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. No handles with labels found to put in legend. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. No handles with labels found to put in legend. . &lt;/div&gt; | |",
            "url": "https://jmmerrell.github.io/ws/2021/08/11/youtube_guerito_project.html",
            "relUrl": "/2021/08/11/youtube_guerito_project.html",
            "date": " • Aug 11, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "YouTube Views with XGBoost",
            "content": ". I wanted to find the features that help me get more views on YouTube. Successfully identified which YouTube videos are successful with 85% accuracy. I used the YouTube API to gather data from my competitors on YouTube. I also used Google Vision&#39;s deep learning models to analyzye thumbnail images, and add that data to the YouTube API data. Data from nearly 2,000 videos was used to train and test an XGBoost model. . 1. Objective . We want to classify videos as succesful(high views per subscriber) or unsuccessful by scraping data from YouTube and using the data to train an XGBoost model. The main sources of data for this project are the APIs for YouTube and GoogleVision. From these two sources we gather information about each of my competitor&#39;s YouTube channels: views, subscribers, number of videos, titles of the videos, duration of each video, etc. From the YouTube API I also extract the thubnail URL for each video. Later I explain how the Google Vision API analyzes the image of the thumbnail for facial and text recognition. I combine the youtube data and the thumbnail data together for the analysis. . . 2. Import the necessary libraries and data set . 2.1. Libraries . The libraries used in this project are the following. . import os os.chdir(&quot;C: Users merre Desktop envs yt_api_env Lib site-packages&quot;) from numpy.core.fromnumeric import shape from numpy.lib.function_base import diff import pandas as pd import re from datetime import datetime, timedelta import numpy as np import matplotlib.pyplot as plt from pandas.core.indexes.base import Index import emoji from scipy import stats import fasttext from collections import Counter from pprint import pprint from IPython.display import display_html from itertools import chain,cycle import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.metrics import confusion_matrix, accuracy_score import shap import xgboost as xgb import matplotlib.pylab as pl from sklearn.metrics import confusion_matrix, accuracy_score . 2.2. Data Sets . I merge together all the datasets taken from YouTube&#39;s API with the Google Vision API thumbnail image data. You can see the full code below for retreiving this data. . YouTube API Data Pull . import json import requests from numpy import random from time import sleep import os class YTstats: def __init__(self, api_key, channel_id): self.api_key = api_key self.channel_id = channel_id self.channel_statistics = None self.video_data = None def extract_all(self): self.get_channel_statistics() self.get_channel_video_data() def get_channel_statistics(self): &quot;&quot;&quot;Extract the channel statistics&quot;&quot;&quot; print(&#39;get channel statistics...&#39;) url = f&#39;https://www.googleapis.com/youtube/v3/channels?part=statistics&amp;id={self.channel_id}&amp;key={self.api_key}&#39; json_url = requests.get(url) data = json.loads(json_url.text) try: data = data[&#39;items&#39;][0][&#39;statistics&#39;] except KeyError: print(&#39;Could not get channel statistics&#39;) data = {} self.channel_statistics = data return data def get_channel_video_data(self): global s s = requests.Session() &quot;Extract all video information of the channel&quot; print(&#39;get video data...&#39;) channel_videos, channel_playlists = self._get_channel_content(limit=50) parts=[&quot;snippet&quot;, &quot;statistics&quot;,&quot;contentDetails&quot;, &quot;topicDetails&quot;] print(len(channel_videos)) ii = 0 for video_id in channel_videos: ii += 1 print(ii/len(channel_videos)) for part in parts: data = self._get_single_video_data(video_id, part) channel_videos[video_id].update(data) self.video_data = channel_videos return channel_videos def _get_single_video_data(self, video_id, part): &quot;&quot;&quot; Extract further information for a single video parts can be: &#39;snippet&#39;, &#39;statistics&#39;, &#39;contentDetails&#39;, &#39;topicDetails&#39; &quot;&quot;&quot; sleep(random.uniform(1, 3)/2) url = f&quot;https://www.googleapis.com/youtube/v3/videos?part={part}&amp;id={video_id}&amp;key={self.api_key}&quot; json_url = s.get(url) data = json.loads(json_url.text) print(url) try: data = data[&#39;items&#39;][0][part] except KeyError as e: print(f&#39;Error! Could not get {part} part of data: n{data}&#39;) data = dict() return data def _get_channel_content(self, limit=None, check_all_pages=True): &quot;&quot;&quot; Extract all videos and playlists, can check all available search pages channel_videos = videoId: title, publishedAt channel_playlists = playlistId: title, publishedAt return channel_videos, channel_playlists &quot;&quot;&quot; url = f&quot;https://www.googleapis.com/youtube/v3/search?key={self.api_key}&amp;channelId={self.channel_id}&amp;part=snippet,id&amp;order=date&quot; if limit is not None and isinstance(limit, int): url += &quot;&amp;maxResults=&quot; + str(limit) vid, pl, npt = self._get_channel_content_per_page(url) idx = 0 while(check_all_pages and npt is not None and idx &lt; 50 and num_pages &lt; 2): nexturl = url + &quot;&amp;pageToken=&quot; + npt next_vid, next_pl, npt = self._get_channel_content_per_page(nexturl) vid.update(next_vid) pl.update(next_pl) idx += 1 print(check_all_pages, idx, npt) return vid, pl def _get_channel_content_per_page(self, url): &quot;&quot;&quot; Extract all videos and playlists per page return channel_videos, channel_playlists, nextPageToken &quot;&quot;&quot; sleep(random.uniform(1, 3)) json_url = requests.get(url) data = json.loads(json_url.text) channel_videos = dict() channel_playlists = dict() if &#39;items&#39; not in data: print(&#39;Error! Could not get correct channel data! n&#39;, data) return channel_videos, channel_playlists, None global num_pages num_pages+=1 nextPageToken = data.get(&quot;nextPageToken&quot;, None) item_data = data[&#39;items&#39;] for item in item_data: try: kind = item[&#39;id&#39;][&#39;kind&#39;] published_at = item[&#39;snippet&#39;][&#39;publishedAt&#39;] title = item[&#39;snippet&#39;][&#39;title&#39;] if kind == &#39;youtube#video&#39;: video_id = item[&#39;id&#39;][&#39;videoId&#39;] channel_videos[video_id] = {&#39;publishedAt&#39;: published_at, &#39;title&#39;: title} elif kind == &#39;youtube#playlist&#39;: playlist_id = item[&#39;id&#39;][&#39;playlistId&#39;] channel_playlists[playlist_id] = {&#39;publishedAt&#39;: published_at, &#39;title&#39;: title} except KeyError as e: print(&#39;Error! Could not extract data from item: n&#39;, item) return channel_videos, channel_playlists, nextPageToken def dump(self): &quot;&quot;&quot;Dumps channel statistics and video data in a single json file&quot;&quot;&quot; if self.channel_statistics is None or self.video_data is None: print(&#39;data is missing! nCall get_channel_statistics() and get_channel_video_data() first!&#39;) return fused_data = {self.channel_id: {&quot;channel_statistics&quot;: self.channel_statistics, &quot;video_data&quot;: self.video_data}} channel_title = self.video_data.popitem()[1].get(&#39;channelTitle&#39;, self.channel_id) channel_title = channel_title.replace(&quot; &quot;, &quot;_&quot;).lower() filename = channel_title + &#39;.json&#39; with open(filename, &#39;w&#39;) as f: json.dump(fused_data, f, indent=4) print(&#39;file dumped to&#39;, filename) #brian hull UCiNeUoUWfBLC8mJuMzI6hvw #Black Gryphon UCvzWGXYFDiiJ338KIJPhbhQ #Brock Baker UCLzdMXE3R2xXIklfIO9HCcQ # Ori UCra3g9Qvmgux0NyY2Pdj4Lw # Scheiffer Bates UCcBacTJIf67LSU_-yeJwDvg #Azerrz UCiwIAU4SNlrcv47504JrJeQ #Danny padilla &amp; mason sperling UCfhK8MfxO-9RCypkLDyW1rw # Brizzy UC7lObFRyZgoZcMYHHqxi9lg # Redfireball UC88CnZTYFz5ugp-JtDEQ3-g # Sounds like pizza UCh6OfzCefcCGFfihPbe_Y4g #joshiiwuh UCxRGk49YNiW3Cq8s7MGknqw # simau UCkXvCWJjAqNcFwxF7hW_ZRQ #Knep UCy7gv-FM-dMvw6dMtj8Qfgg # charlie hopkinson UCewLMcro9tNP97XQ1rxtLXQ #Uss JA doin UCqPYUMNbVeEhyTBIZCDO_VQ # Shanieology UCR93YdwZ4UKEUwf1gA-ZusA # BigShade UC7Wt6Nukmt83Bph3us5s5Aw # Best in Class UClQhFMEVUxJAwMW-KdZ0SvQ # Daniel Ferguson UCXFzOJmXVaP1tMLiww4aQzg # Mikey Boltz UC0gXT2T6KtmV0IHNNNvruAQ # Maxamili UC-0WjH-efG2qvNlZUBlX70Q api_key= os.environ.get(&#39;YT_API&#39;) # channel_ids= [&#39;UCiNeUoUWfBLC8mJuMzI6hvw&#39;,&#39;UCvzWGXYFDiiJ338KIJPhbhQ&#39;,&#39;UCLzdMXE3R2xXIklfIO9HCcQ&#39;,&#39;UCra3g9Qvmgux0NyY2Pdj4Lw&#39;,&#39;UCcBacTJIf67LSU_-yeJwDvg&#39;, # &#39;UCiwIAU4SNlrcv47504JrJeQ&#39;,&#39;UCfhK8MfxO-9RCypkLDyW1rw&#39;,&#39;UC7lObFRyZgoZcMYHHqxi9lg&#39;,&#39;UC88CnZTYFz5ugp-JtDEQ3-g&#39;,&#39;UCh6OfzCefcCGFfihPbe_Y4g&#39;, # &#39;UCxRGk49YNiW3Cq8s7MGknqw&#39;,&#39;UCkXvCWJjAqNcFwxF7hW_ZRQ&#39;,&#39;UCy7gv-FM-dMvw6dMtj8Qfgg&#39;,&#39;UCewLMcro9tNP97XQ1rxtLXQ&#39;,&#39;UCqPYUMNbVeEhyTBIZCDO_VQ&#39;, # &#39;UCR93YdwZ4UKEUwf1gA-ZusA&#39;,&#39;UC7Wt6Nukmt83Bph3us5s5Aw&#39;,&#39;UClQhFMEVUxJAwMW-KdZ0SvQ&#39;,&#39;UCXFzOJmXVaP1tMLiww4aQzg&#39;,&#39;UC0gXT2T6KtmV0IHNNNvruAQ&#39;, # &#39;UC-0WjH-efG2qvNlZUBlX70Q&#39;] channel_ids= [&#39;UC-0WjH-efG2qvNlZUBlX70Q&#39;,&#39;UClQhFMEVUxJAwMW-KdZ0SvQ&#39;] for channel_id in channel_ids: global num_pages num_pages = 0 yt = YTstats(api_key,channel_id) yt.get_channel_statistics() yt.get_channel_video_data() yt.dump() . . File &#34;&lt;ipython-input-2-7279167fc01f&gt;&#34;, line 2 __[YouTube API Data Pull](https://github.com/jmmerrell/ws/blob/master/data/youtube_jadoinstuff/query_youtube_api.py)__ ^ SyntaxError: invalid syntax . Convert JSON to Pandas . import json from os import replace import pandas as pd import re from datetime import datetime, timedelta import cv2 import urllib import numpy as np from skimage import io import matplotlib.pyplot as plt #C:/Users/merre/Desktop/data projects/ files= [&quot;shanieology.json&quot;,&quot;simau.json&quot;,&quot;soundslikepizza.json&quot;,&quot;azerrz.json&quot;,&quot;BigShade.json&quot;,&quot;black_gryph0n.json&quot; ,&quot;brian_hull.json&quot;,&quot;brizzy_voices.json&quot;,&quot;brock_baker.json&quot;,&quot;charlie_hopkinson.json&quot;,&quot;danny_padilla_&amp;_mason_sperling.json&quot; ,&quot;ja_doin_stuff.json&quot;,&quot;joshiiwuh.json&quot;,&quot;knep.json&quot;,&quot;ori.json&quot;,&quot;redfireball555.json&quot;,&quot;scheiffer_bates.json&quot;,&quot;daniel_ferguson.json&quot;, &quot;BigShade.json&quot;,&quot;best_in_class.json&quot;,&quot;maxamili.json&quot;,&quot;mikey_bolts.json&quot;] data=None df_channel_new=None df_channel = None for file in files: with open(file,&#39;r&#39;) as f: data = json.load(f) channel_id, stats = data.popitem() channel_stats=stats[&quot;channel_statistics&quot;] video_stats = stats[&quot;video_data&quot;] channel_views= channel_stats[&quot;viewCount&quot;] channel_subs= channel_stats[&quot;subscriberCount&quot;] channel_videos= channel_stats[&quot;videoCount&quot;] try: sorted_vids = sorted(video_stats.items(), key=lambda item: int(item[1][&quot;viewCount&quot;]), reverse=True) except: sorted_vids = video_stats.items() stats = [] for vid in sorted_vids: video_id = vid[0] title = vid[1][&quot;title&quot;] title_len = len(title) title_words = re.findall(r&#39; w+&#39;,title) words=0 upper_words=0 for word in title_words: words += 1 if word.isupper(): upper_words += 1 upper_pct = upper_words/words emoji_count = len(re.findall(u&#39;[ U0001f600- U0001f650]&#39;, title)) #Convert time to Mexico City Time upload_date_time = datetime.strptime(vid[1][&quot;publishedAt&quot;],&#39;%Y-%m-%dT%H:%M:%SZ&#39;)-timedelta(hours=5) upload_date = upload_date_time.date() upload_time = upload_date_time.time() #0 is Monday, 6 is Sunday upload_day = upload_date.weekday() if datetime.strptime(&#39;04:00:00&#39;, &#39;%H:%M:%S&#39;).time() &lt;= upload_time &lt;= datetime.strptime(&#39;10:30:00&#39;, &#39;%H:%M:%S&#39;).time(): upload_time_of_day = &#39;morning&#39; elif datetime.strptime(&#39;10:30:01&#39;, &#39;%H:%M:%S&#39;).time() &lt;= upload_time &lt;= datetime.strptime(&#39;18:00:00&#39;, &#39;%H:%M:%S&#39;).time(): upload_time_of_day = &#39;afternoon&#39; elif datetime.strptime(&#39;18:00:01&#39;, &#39;%H:%M:%S&#39;).time() &lt;= upload_time &lt;= datetime.strptime(&#39;23:00:00&#39;, &#39;%H:%M:%S&#39;).time(): upload_time_of_day = &#39;night&#39; else: upload_time_of_day = &quot;late_night&quot; try: thumbnail_url = vid[1][&quot;thumbnails&quot;][&quot;maxres&quot;][&quot;url&quot;] thumbnail_h = vid[1][&quot;thumbnails&quot;][&quot;maxres&quot;][&quot;height&quot;] thumbnail_w = vid[1][&quot;thumbnails&quot;][&quot;maxres&quot;][&quot;width&quot;] except: try: thumbnail_url = vid[1][&quot;thumbnails&quot;][&quot;high&quot;][&quot;url&quot;] thumbnail_h = vid[1][&quot;thumbnails&quot;][&quot;high&quot;][&quot;height&quot;] thumbnail_w = vid[1][&quot;thumbnails&quot;][&quot;high&quot;][&quot;width&quot;] except: try: thumbnail_url = vid[1][&quot;thumbnails&quot;][&quot;default&quot;][&quot;url&quot;] thumbnail_h = vid[1][&quot;thumbnails&quot;][&quot;default&quot;][&quot;height&quot;] thumbnail_w = vid[1][&quot;thumbnails&quot;][&quot;default&quot;][&quot;width&quot;] except: thumbnail_url=None thumbnail_h=None thumbnail_w=None try: channel = vid[1][&quot;channelTitle&quot;] except: channel=None try: tags = vid[1][&quot;tags&quot;] except: tag = None num_tags = len(tags) try: categoryId = vid[1][&quot;categoryId&quot;] except: categoryId=None try: liveBroadcastContent = vid[1][&quot;liveBroadcastContent&quot;] except: liveBroadcastContent = None try: defaultAudioLanguage = vid[1][&quot;defaultAudioLanguage&quot;] except: defaultAudioLanguage = None try: viewCount = vid[1][&quot;viewCount&quot;] except: viewCount = None try: likeCount = vid[1][&quot;likeCount&quot;] except: likeCount =None try: dislikeCount = vid[1][&quot;dislikeCount&quot;] except: dislikeCount=None try: favoriteCount = vid[1][&quot;favoriteCount&quot;] except: favoriteCount = None try: commentCount = vid[1][&quot;commentCount&quot;] except: commentCount=None try: duration0 = vid[1][&quot;duration&quot;] except: duration0=None try: hours = int(re.findall(r&#39; d+H&#39;,duration0)[0].replace(&#39;H&#39;,&#39;&#39;)) except: hours = None try: mins = int(re.findall(r&#39; d+M&#39;,duration0)[0].replace(&#39;M&#39;,&#39;&#39;)) except: mins=None try: secs = int(re.findall(r&#39; d+S&#39;,duration0)[0].replace(&#39;S&#39;,&#39;&#39;)) except: secs=0 if hours is not None and mins is not None and secs is not None: duration = hours*60 + mins + secs/60 elif mins is not None and secs is not None: duration = mins + secs/60 elif secs is not None: duration = secs/60 try: definition = vid[1][&quot;definition&quot;] except: definition =None try: captions = vid[1][&quot;caption&quot;] except: captions = None try: licensedContent = vid[1][&quot;licensedContent&quot;] except: licensedContent=None try: projection = vid[1][&quot;projection&quot;] except: projection = None try: topicCategories = vid[1][&quot;topicCategories&quot;] except: topicCategories = None try: desc = vid[1][&quot;description&quot;] except: desc = None video_id = vid[0] stats.append([video_id,title,title_len,words,upper_pct,emoji_count,upload_date,upload_time,upload_day,upload_time_of_day,viewCount,likeCount,dislikeCount,favoriteCount, commentCount,duration,definition,captions,licensedContent,thumbnail_url, thumbnail_w, thumbnail_h, tags,num_tags,categoryId,liveBroadcastContent, defaultAudioLanguage,topicCategories, channel, channel_subs, channel_views, channel_videos,desc]) df = pd.DataFrame(stats) df.columns = [&#39;video_id&#39;,&#39;title&#39;,&#39;title_len&#39;,&#39;words&#39;,&#39;upper_pct&#39;,&#39;emoji_count&#39;,&#39;upload_date&#39;,&#39;upload_time&#39;,&#39;upload_day&#39;,&#39;upload_time_of_day&#39;,&#39;viewCount&#39;,&#39;likeCount&#39;,&#39;dislikeCount&#39;, &#39;favoriteCount&#39;,&#39;commentCount&#39;,&#39;duration&#39;,&#39;definition&#39;,&#39;caption&#39;,&#39;licensedContent&#39;,&#39;thumbnail_url&#39;, &#39;thumbnail_w&#39;, &#39;thumbnail_h&#39;, &#39;tags&#39;,&#39;num_tags&#39;, &#39;categoryId&#39;,&#39;liveBroadcastContent&#39;,&#39;defaultAudioLanguage&#39;,&#39;topicCategories&#39;, &#39;channel&#39;, &#39;channel_subs&#39;, &#39;channel_views&#39;, &#39;channel_videos&#39;,&#39;desc&#39;] df.to_csv(file.replace(&#39;json&#39;,&#39;txt&#39;)) . . Functions to Query Google Vision API . import httplib2 import sys from googleapiclient import discovery from oauth2client import tools, file, client import json import os import cv2 from base64 import b64encode import numpy as np # limited preview only (sorry!) API_DISCOVERY_FILE = os.environ.get(&#39;GOOGLE_VISION_API&#39;) &quot;&quot;&quot; Google Authentication Utilities &quot;&quot;&quot; def get_vision_api(): credentials = get_api_credentials(&#39;https://www.googleapis.com/auth/cloud-platform&#39;) with open(API_DISCOVERY_FILE, &#39;r&#39;) as f: doc = f.read() return discovery.build_from_document(doc, credentials=credentials, http=httplib2.Http()) def get_api_credentials(scope, service_account=True): &quot;&quot;&quot; Build API client based on oAuth2 authentication &quot;&quot;&quot; # STORAGE = file.Storage(os.environ.get(&#39;GOOGLE_VISION_API&#39;)) #local storage of oAuth tokens STORAGE = file.Storage(API_DISCOVERY_FILE) #local storage of oAuth tokens credentials = STORAGE.get() if credentials is None or credentials.invalid: #check if new oAuth flow is needed if service_account: #server 2 server flow # with open(os.environ.get(&#39;GOOGLE_VISION_API&#39;)) as f: with open(API_DISCOVERY_FILE) as f: account = json.loads(f.read()) email = account[&#39;client_email&#39;] key = account[&#39;private_key&#39;] credentials = client.SignedJwtAssertionCredentials(email, key, scope=scope) STORAGE.put(credentials) else: #normal oAuth2 flow CLIENT_SECRETS = os.path.join(os.path.dirname(__file__), &#39;client_secrets.json&#39;) FLOW = client.flow_from_clientsecrets(CLIENT_SECRETS, scope=scope) PARSER = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter, parents=[tools.argparser]) FLAGS = PARSER.parse_args(sys.argv[1:]) credentials = tools.run_flow(FLOW, STORAGE, FLAGS) return credentials &quot;&quot;&quot; read/write utilities &quot;&quot;&quot; def read_image(filename): return cv2.imread(filename) def save_image(filename, im): cv2.imwrite(filename, cv2.cvtColor(im, cv2.COLOR_RGB2BGR)) def read_image_base64(filename): with open(filename, &#39;rb&#39;) as f: return b64encode(f.read()) &quot;&quot;&quot; OpenCV drawing utilities &quot;&quot;&quot; def draw_face(im, annotations): faces = [] for a in annotations: if a[&#39;detectionConfidence&#39;] &gt; .4: try: tl_,br_ = draw_box(im, a[&#39;fdBoundingPoly&#39;][&#39;vertices&#39;]) except: tl_,br_=None try: joy = a[&#39;joyLikelihood&#39;] except: joy=&#39;&#39; try: sad = a[&#39;sorrowLikelihood&#39;] except: sad=&#39;&#39; try: angry = a[&#39;angerLikelihood&#39;] except: angry = &#39;&#39; try: suprise=a[&#39;surpriseLikelihood&#39;] except: suprise=&#39;&#39; emotions=[joy,sad,angry,suprise] if &#39;VERY_LIKELY&#39; in emotions: emotion = emotions.index(&#39;VERY_LIKELY&#39;) elif &#39;LIKELY&#39; in emotions: emotion = emotions.index(&#39;LIKELY&#39;) elif &#39;POSSIBLE&#39; in emotions: emotion = emotions.index(&#39;POSSIBLE&#39;) else: emotion=None if emotion==0: text= &quot;happy&quot; elif emotion==1: text=&quot;sad&quot; elif emotion==2: text=&quot;angry&quot; elif emotion==3: text=&quot;suprised&quot; else: text=&quot;other&quot; faces.append(text) if im is not None and tl_ is not None: draw_text(im, text ,tl_) try: for landmark in a[&#39;landmarks&#39;]: if im is not None: try: draw_point(im, landmark[&#39;position&#39;]) except: pass except: pass return faces def extract_vertices(vertices): &quot;&quot;&quot; Extract two opposite vertices from a list of 4 (assumption: rectangle) &quot;&quot;&quot; min_x,max_x,min_y,max_y = float(&quot;inf&quot;),float(&quot;-inf&quot;),float(&quot;inf&quot;),float(&quot;-inf&quot;) for v in vertices: if v.get(&#39;x&#39;,min_y) &lt; min_x: min_x = v.get(&#39;x&#39;) if v.get(&#39;x&#39;,max_y) &gt; max_x: max_x = v.get(&#39;x&#39;) if v.get(&#39;y&#39;,min_y) &lt; min_y: min_y = v.get(&#39;y&#39;) if v.get(&#39;y&#39;,max_y) &gt; max_y: max_y = v.get(&#39;y&#39;) try: v1 = next(v for v in vertices if v.get(&#39;x&#39;) == min_x and v.get(&#39;y&#39;) == min_y) v2 = next(v for v in vertices if v.get(&#39;x&#39;) == max_x and v.get(&#39;y&#39;) == max_y) except: v1=None v2=None return v1,v2 def draw_box(im, vertices): v1,v2 = extract_vertices(vertices) try: pt1 = (v1.get(&#39;x&#39;,0), v1.get(&#39;y&#39;,0)) pt2 = (v2.get(&#39;x&#39;,0), v2.get(&#39;y&#39;,0)) cv2.rectangle(im, pt1, pt2, (0,0,255),thickness=4) except: pt1=None pt2=None return pt1, pt2 def draw_point(im, position): pt = (int(position.get(&#39;x&#39;,0)), int(position.get(&#39;y&#39;,0))) cv2.circle(im, pt, 3, (0,0,255)) return pt def draw_text(im, text,loc): font_face = cv2.FONT_HERSHEY_SIMPLEX #thickness = 1 thickness=round(0.002 * (im.shape[0] + im.shape[1]) / 2) + 10 # for scale in np.arange(20,0,-0.2): # (w,h),baseline = cv2.getTextSize(text, font_face, scale, thickness) # if w &lt;= im.shape[1]: # new_img = cv2.copyMakeBorder(im, 0, baseline*4, 0, 0, cv2.BORDER_CONSTANT, value=0) # cv2.putText(new_img, text, (baseline*2 +20 ,new_img.shape[0]-baseline +20 ), font_face, 2, (255,255,255), thickness) # return new_img new_img = im cv2.putText(new_img, text, loc, font_face, 2.5, (102,255,0), thickness) return new_img . . Google Vision API Data Pull . from datetime import date import datetime import json from webbrowser import get from google.cloud.vision_v1.types.image_annotator import AnnotateImageRequest, AnnotateImageResponse from numpy.core.fromnumeric import shape from numpy.core.numeric import NaN from numpy.lib.arraysetops import unique from skimage.util import dtype from functions_for_google_vision_api import (get_vision_api, read_image, read_image_base64, save_image, draw_face, draw_box, draw_text) from skimage import io import os from google.cloud import vision_v1 from google.cloud import vision from google.cloud.vision_v1 import types import cv2 import pandas as pd import numpy as np import itertools import time import random ##################################################################### import httplib2 from googleapiclient import discovery from oauth2client.client import GoogleCredentials os.environ[&#39;GOOGLE_APPLICATION_CREDENTIALS&#39;] = os.environ.get(&#39;GOOGLE_VISION_API&#39;) DISCOVERY_URL=&#39;https://{api}.googleapis.com/$discovery/rest?version={apiVersion}&#39; def get_vision_service(): credentials = GoogleCredentials.get_application_default() return discovery.build(&#39;vision&#39;, &#39;v1&#39;, credentials=credentials, discoveryServiceUrl = DISCOVERY_URL) def main(video_id, inputfile): service = get_vision_service() outputfile= &quot;C:/Users/merre/Desktop/ws/data/youtube_jadoinstuff/output_images/thumbnail_&quot; +inputfile[inputfile.rfind(&#39;/&#39;, 0, inputfile.rfind(&#39;/&#39;))+1:inputfile.rfind(&#39;/&#39;)] + &quot;.jpg&quot; batch_request=[ { &quot;features&quot;: [ { &quot;maxResults&quot;: 50, &quot;type&quot;: &quot;FACE_DETECTION&quot; }, { &quot;maxResults&quot;: 50, &quot;type&quot;: &quot;LABEL_DETECTION&quot; }, { &quot;maxResults&quot;: 20, &quot;type&quot;: &quot;SAFE_SEARCH_DETECTION&quot; }, { &quot;maxResults&quot;: 50, &quot;type&quot;: &quot;TEXT_DETECTION&quot; } ], &quot;image&quot;: { &quot;source&quot;: { &quot;imageUri&quot;: inputfile } } } ] request = service.images().annotate(body={ &#39;requests&#39;: batch_request, }) time.sleep(random.random()*4) response = request.execute() inputfile,labels,faces,texts,adult,medical,racy,spoof,violence = show_results(inputfile, response, outputfile) vars_list = [video_id,inputfile,labels,faces,texts,adult,medical,racy,spoof,violence] i=0 for v in vars_list: if type(v) == np.ndarray: v = v.tolist() vars_list[i]=v i += 1 return vars_list def show_results(inputfile, data, outputfile): #read original file im = io.imread(inputfile) #draw face, boxes and text for each response faces=[] labels=[] texts=[] #dict_keys = data.keys() for r in data[&#39;responses&#39;]: if &#39;faceAnnotations&#39; in r: faces = draw_face(im, r[&#39;faceAnnotations&#39;]) if &#39;labelAnnotations&#39; in r: for label in r[&#39;labelAnnotations&#39;]: if label[&#39;score&#39;] &gt; .6: try: labels.append(label[&#39;description&#39;]) except: labels=labels if &#39;textAnnotations&#39; in r: for a in r[&#39;textAnnotations&#39;]: if a[&#39;description&#39;] != &#39;&#39;: try: texts.append(a[&#39;description&#39;]) except: texts=texts if &#39;safeSearchAnnotation&#39; in r: try: adult = r[&#39;safeSearchAnnotation&#39;][&quot;adult&quot;] except: adult=&#39;&#39; try: medical = r[&#39;safeSearchAnnotation&#39;][&quot;medical&quot;] except: medical=&#39;&#39; try: racy = r[&#39;safeSearchAnnotation&#39;][&quot;racy&quot;] except: racy=&#39;&#39; try: spoof = r[&#39;safeSearchAnnotation&#39;][&quot;spoof&quot;] except: spoof=&#39;&#39; try: violence = r[&#39;safeSearchAnnotation&#39;][&quot;violence&quot;] except: violence=&#39;&#39; labels=unique(labels) texts=unique(texts) #save to output file save_image(outputfile, im) return inputfile,labels,faces,texts,adult,medical,racy,spoof,violence # files= [&quot;shanieology.txt&quot;,&quot;simau.txt&quot;,&quot;soundslikepizza.txt&quot;,&quot;azerrz.txt&quot;,&quot;BigShade.txt&quot;,&quot;black_gryph0n.txt&quot; # ,&quot;brian_hull.txt&quot;,&quot;brizzy_voices.txt&quot;,&quot;brock_baker.txt&quot;,&quot;charlie_hopkinson.txt&quot;,&quot;danny_padilla_&amp;_mason_sperling.txt&quot; # ,&quot;ja_doin_stuff.txt&quot;,&quot;joshiiwuh.txt&quot;,&quot;knep.txt&quot;,&quot;ori.txt&quot;,&quot;redfireball555.txt&quot;,&quot;scheiffer_bates.txt&quot;,&quot;daniel_ferguson.txt&quot;, # &quot;BigShade.txt&quot;,&quot;best_in_class.txt&quot;,&quot;maxamili.txt&quot;,&quot;mikey_bolts.txt&quot;] files= [&quot;daniel_ferguson.txt&quot;,&quot;BigShade.txt&quot;,&quot;best_in_class.txt&quot;,&quot;maxamili.txt&quot;,&quot;mikey_bolts.txt&quot;] vid_ids=[] vid_thumb_urls=[] for file in files: videos_loop= pd.read_csv(file) vid_ids.append(list(videos_loop[pd.to_datetime(videos_loop[&quot;upload_date&quot;])&gt;datetime.datetime(2012,7,1,0,0,0,0)][&quot;video_id&quot;])) vid_thumb_urls.append(list(videos_loop[pd.to_datetime(videos_loop[&quot;upload_date&quot;])&gt;datetime.datetime(2012,7,1,0,0,0,0)][&quot;thumbnail_url&quot;])) vid_ids=list(itertools.chain(*vid_ids)) vid_thumb_urls=list(itertools.chain(*vid_thumb_urls)) df = pd.DataFrame(columns=[&#39;video_id&#39;,&#39;thumbnail_url&#39;, &#39;labels&#39;,&#39;faces&#39;,&#39;texts&#39;,&#39;adult&#39;,&#39;medical&#39;,&#39;racy&#39;,&#39;spoof&#39;,&#39;violence&#39;]) ii = 0 for i in range(len(vid_ids)): if vid_thumb_urls[i] is not NaN: time.sleep(5) try: df.loc[len(df)] = main(video_id=vid_ids[i],inputfile=vid_thumb_urls[i]) ii += 1 except: pass if ii % 30 == 0 or i==len(vid_ids)-1 or i==len(vid_ids): df.to_csv(&#39;thumbnail_data_&#39;+str(datetime.datetime.now()).replace(&#39;-&#39;,&#39;&#39;).replace(&#39; &#39;,&#39;_&#39;).replace(&#39;:&#39;,&#39;-&#39;)+&#39;.txt&#39;, header=True, index=None, mode=&#39;w&#39;) print(&quot;Num videos&quot;,i,&quot;- Percent complete:&quot;,(round(i/len(vid_ids),3))*100) . . The Google API uses deep learning to identify number of faces, facial expressions and the text contained in the thumbnail image of each youtube video. An example is shown below of the facial recognition and the text that was recognized within the image. . . os.chdir(&quot;C: Users merre Desktop ws data youtube_jadoinstuff&quot;) files= [&quot;shanieology.txt&quot;,&quot;simau.txt&quot;,&quot;soundslikepizza.txt&quot;,&quot;azerrz.txt&quot;,&quot;BigShade.txt&quot;,&quot;black_gryph0n.txt&quot; ,&quot;brian_hull.txt&quot;,&quot;brizzy_voices.txt&quot;,&quot;brock_baker.txt&quot;,&quot;charlie_hopkinson.txt&quot;,&quot;danny_padilla_&amp;_mason_sperling.txt&quot; ,&quot;ja_doin_stuff.txt&quot;,&quot;joshiiwuh.txt&quot;,&quot;knep.txt&quot;,&quot;ori.txt&quot;,&quot;redfireball555.txt&quot;,&quot;scheiffer_bates.txt&quot;,&quot;daniel_ferguson.txt&quot;, &quot;BigShade.txt&quot;,&quot;best_in_class.txt&quot;,&quot;maxamili.txt&quot;,&quot;mikey_bolts.txt&quot;] df = pd.DataFrame(columns=[&#39;video_id&#39;,&#39;title&#39;,&#39;title_len&#39;,&#39;words&#39;,&#39;upper_pct&#39;,&#39;emoji_count&#39;,&#39;upload_date&#39;,&#39;upload_time&#39;, &#39;upload_day&#39;,&#39;upload_time_of_day&#39;,&#39;viewCount&#39;,&#39;likeCount&#39;,&#39;dislikeCount&#39;,&#39;favoriteCount&#39;, &#39;commentCount&#39;,&#39;duration&#39;,&#39;definition&#39;,&#39;caption&#39;,&#39;licensedContent&#39;,&#39;thumbnail_url&#39;, &#39;thumbnail_w&#39;, &#39;thumbnail_h&#39;, &#39;tags&#39;,&#39;num_tags&#39;,&#39;categoryId&#39;,&#39;liveBroadcastContent&#39;, &#39;topicCategories&#39;, &#39;channel&#39;, &#39;channel_subs&#39;, &#39;channel_views&#39;, &#39;channel_videos&#39;,&#39;desc&#39;]) #Loop through all the youtuber&#39;s data files and combine into on data frame for file in files: df_add= pd.read_csv(file) df = df.append(df_add.drop([&#39;Unnamed: 0&#39;],axis=1)) #Read in the files that have the thumbnail data, and combine with the youtuber data df_thumb = pd.read_csv(&quot;thumbnail_data_20210801_23-03-21.638854.txt&quot;).append(pd.read_csv(&quot;thumbnail_data_20210802_21-39-16.451172.txt&quot;)) df_all = pd.merge(df.drop(&#39;defaultAudioLanguage&#39;,axis=1),df_thumb.drop([&#39;thumbnail_url&#39;],axis=1),on=&quot;video_id&quot;,how=&quot;inner&quot;).drop([&#39;emoji_count&#39;],axis=1) df_all[&#39;all_text&#39;] = df_all[&#39;title&#39;].astype(str) + df_all[&#39;tags&#39;].astype(str) + df_all[&#39;desc&#39;].astype(str) . To get an overview of the data, let&#39;s check the first rows and the size of the data set. . df_all.head() . video_id title title_len words upper_pct upload_date upload_time upload_day upload_time_of_day viewCount ... desc labels faces texts adult medical racy spoof violence all_text . 0 eyyTBJUKI3c | Skeletor reacts to Episode 1 of Teela and the ... | 80 | 14 | 0.000000 | 2021-07-25 | 11:22:54 | 6 | afternoon | 15005 | ... | In this video Skeletor will react and give his... | [&#39;Action figure&#39;, &#39;Animated cartoon&#39;, &#39;Animati... | [] | [&#39;A&#39;, &#39;AND&#39;, &#39;BAIT&#39;, &#39;R.I.P.&#39;, &#39;SWITCH!&#39;, &#39;THI... | UNLIKELY | UNLIKELY | UNLIKELY | LIKELY | UNLIKELY | Skeletor reacts to Episode 1 of Teela and the ... | . 1 S1Bmx8Dti6Y | Top 5 Worst Reboot Offences Part 2 || Skeletor... | 53 | 9 | 0.000000 | 2021-07-27 | 18:40:21 | 1 | night | 9497 | ... | In this video Skeletor finishes his top 5 wors... | [&#39;Animated cartoon&#39;, &#39;Animation&#39;, &#39;Art&#39;, &#39;Ente... | [&#39;other&#39;, &#39;other&#39;, &#39;other&#39;, &#39;other&#39;, &#39;other&#39;] | [&#39;Stop&#39;, &#39;Stop exploiting nour nostalgia! n&#39;, ... | VERY_UNLIKELY | UNLIKELY | VERY_UNLIKELY | LIKELY | UNLIKELY | Top 5 Worst Reboot Offences Part 2 || Skeletor... | . 2 DStvv0peyYQ | Top 5 Worst Reboot Offences Part 1 :Skeletor R... | 51 | 9 | 0.000000 | 2021-07-21 | 11:03:11 | 2 | afternoon | 5312 | ... | In this video Skeletor does a top 5 list of re... | [&#39;Advertising&#39;, &#39;Animated cartoon&#39;, &#39;Animation... | [&#39;other&#39;, &#39;other&#39;, &#39;other&#39;, &#39;other&#39;] | [&#39;1&#39;, &#39;5&#39;, &#39;Hollywood&#39;, &#39;I&#39;, &quot;I wonder why we ... | UNLIKELY | UNLIKELY | VERY_UNLIKELY | LIKELY | UNLIKELY | Top 5 Worst Reboot Offences Part 1 :Skeletor R... | . 3 PCwtNUQ3t30 | Skeletor Reacts to the new MOTU toy line Pt2 W... | 71 | 15 | 0.066667 | 2021-07-17 | 13:34:25 | 5 | afternoon | 5079 | ... | This video is part 2 of Skeletor&#39;s reaction to... | [&#39;Animated cartoon&#39;, &#39;Animation&#39;, &#39;Art&#39;, &#39;Cart... | [] | [&#39;ETERNIA&#39;, &#39;No&#39;, &#39;SHORE&#39;, &#39;So...&#39;, &quot;So... nWh... | VERY_UNLIKELY | VERY_UNLIKELY | VERY_UNLIKELY | LIKELY | UNLIKELY | Skeletor Reacts to the new MOTU toy line Pt2 W... | . 4 yiQto5PfuII | Skeletor Reacts to Episode 2 of Teela and the ... | 87 | 16 | 0.000000 | 2021-07-31 | 12:13:54 | 5 | afternoon | 4891 | ... | This video is part one of Skeletor watches and... | [&#39;Animated cartoon&#39;, &#39;Animation&#39;, &#39;Art&#39;, &#39;Elec... | [&#39;other&#39;] | [&#39;SOMETHING&#39;, &#39;SOMETHING nSTINKS! n&#39;, &#39;STINKS!&#39;] | VERY_UNLIKELY | VERY_UNLIKELY | VERY_UNLIKELY | POSSIBLE | UNLIKELY | Skeletor Reacts to Episode 2 of Teela and the ... | . 5 rows × 40 columns . X.shape . (1460, 79) . y.shape . (1460,) . We have 1,460 rows and 79 columns. Later on, we&#39;ll check these columns to verify which of them will be meaningful to the model. . In the next step, we&#39;ll split the data into training and validation sets. . 3. Training and validation data . It is crucial to break our data into a set for training the model and another one to validate the results. It&#39;s worth mentioning that we should never use the test data here. Our test set stays untouched until we are satisfied with our model&#39;s performance. . What we&#39;re going to do is taking the predictors X and target vector y and breaking them into training and validation sets. For that, we&#39;ll use scikit-learn&#39;s train_test_split. . X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0) . Checking the shape of our training and validation sets, we get the following. . print(f&#39;Shape of X_train_full: {X_train_full.shape}&#39;) print(f&#39;Shape of X_valid_full: {X_valid_full.shape}&#39;) print(f&#39;Shape of y_train: {y_train.shape}&#39;) print(f&#39;Shape of y_valid: {y_valid.shape}&#39;) . Shape of X_train_full: (1168, 79) Shape of X_valid_full: (292, 79) Shape of y_train: (1168,) Shape of y_valid: (292,) . 4. Analyze and prepare the data . Now, we start analyzing the data by checking some information about the features. . X.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1460 entries, 1 to 1460 Data columns (total 79 columns): # Column Non-Null Count Dtype -- -- 0 MSSubClass 1460 non-null int64 1 MSZoning 1460 non-null object 2 LotFrontage 1201 non-null float64 3 LotArea 1460 non-null int64 4 Street 1460 non-null object 5 Alley 91 non-null object 6 LotShape 1460 non-null object 7 LandContour 1460 non-null object 8 Utilities 1460 non-null object 9 LotConfig 1460 non-null object 10 LandSlope 1460 non-null object 11 Neighborhood 1460 non-null object 12 Condition1 1460 non-null object 13 Condition2 1460 non-null object 14 BldgType 1460 non-null object 15 HouseStyle 1460 non-null object 16 OverallQual 1460 non-null int64 17 OverallCond 1460 non-null int64 18 YearBuilt 1460 non-null int64 19 YearRemodAdd 1460 non-null int64 20 RoofStyle 1460 non-null object 21 RoofMatl 1460 non-null object 22 Exterior1st 1460 non-null object 23 Exterior2nd 1460 non-null object 24 MasVnrType 1452 non-null object 25 MasVnrArea 1452 non-null float64 26 ExterQual 1460 non-null object 27 ExterCond 1460 non-null object 28 Foundation 1460 non-null object 29 BsmtQual 1423 non-null object 30 BsmtCond 1423 non-null object 31 BsmtExposure 1422 non-null object 32 BsmtFinType1 1423 non-null object 33 BsmtFinSF1 1460 non-null int64 34 BsmtFinType2 1422 non-null object 35 BsmtFinSF2 1460 non-null int64 36 BsmtUnfSF 1460 non-null int64 37 TotalBsmtSF 1460 non-null int64 38 Heating 1460 non-null object 39 HeatingQC 1460 non-null object 40 CentralAir 1460 non-null object 41 Electrical 1459 non-null object 42 1stFlrSF 1460 non-null int64 43 2ndFlrSF 1460 non-null int64 44 LowQualFinSF 1460 non-null int64 45 GrLivArea 1460 non-null int64 46 BsmtFullBath 1460 non-null int64 47 BsmtHalfBath 1460 non-null int64 48 FullBath 1460 non-null int64 49 HalfBath 1460 non-null int64 50 BedroomAbvGr 1460 non-null int64 51 KitchenAbvGr 1460 non-null int64 52 KitchenQual 1460 non-null object 53 TotRmsAbvGrd 1460 non-null int64 54 Functional 1460 non-null object 55 Fireplaces 1460 non-null int64 56 FireplaceQu 770 non-null object 57 GarageType 1379 non-null object 58 GarageYrBlt 1379 non-null float64 59 GarageFinish 1379 non-null object 60 GarageCars 1460 non-null int64 61 GarageArea 1460 non-null int64 62 GarageQual 1379 non-null object 63 GarageCond 1379 non-null object 64 PavedDrive 1460 non-null object 65 WoodDeckSF 1460 non-null int64 66 OpenPorchSF 1460 non-null int64 67 EnclosedPorch 1460 non-null int64 68 3SsnPorch 1460 non-null int64 69 ScreenPorch 1460 non-null int64 70 PoolArea 1460 non-null int64 71 PoolQC 7 non-null object 72 Fence 281 non-null object 73 MiscFeature 54 non-null object 74 MiscVal 1460 non-null int64 75 MoSold 1460 non-null int64 76 YrSold 1460 non-null int64 77 SaleType 1460 non-null object 78 SaleCondition 1460 non-null object dtypes: float64(3), int64(33), object(43) memory usage: 912.5+ KB . From the summary above, we can observe that some columns have missing values. Let&#39;s take a closer look. . 4.1. Missing Values . missing_values = X.isnull().sum() missing_values = missing_values[missing_values &gt; 0].sort_values(ascending=False) print(missing_values) . PoolQC 1453 MiscFeature 1406 Alley 1369 Fence 1179 FireplaceQu 690 LotFrontage 259 GarageYrBlt 81 GarageType 81 GarageFinish 81 GarageQual 81 GarageCond 81 BsmtFinType2 38 BsmtExposure 38 BsmtFinType1 37 BsmtCond 37 BsmtQual 37 MasVnrArea 8 MasVnrType 8 Electrical 1 dtype: int64 . Some features have missing values counting for the majority of their entries. Checking the competition page, we find more details about the values for each feature, which will help us handle missing data. . For instance, in the columns PoolQC, MiscFeature, Alley, Fence, and FireplaceQu, the missing values mean that the house doesn&#39;t count with that specific feature, so, we&#39;ll fill the missing values with &quot;NA&quot;. All the null values in columns starting with Garage and Bsmt are related to houses that don&#39;t have a garage or basement, respectively. We&#39;ll fill those and the remaining null values with &quot;NA&quot; or the mean value, considering if the features are categorical or numerical. . 4.2. Preprocessing the categorical variables . Most machine learning models only work with numerical variables. Therefore, if we feed the model with categorical variables without preprocessing them first, we&#39;ll get an error. . There are several ways to deal with categorical values. Here, we&#39;ll use One-Hot Encoding, which will create new columns indicating the presence or absence of each value in the original data. . One issue of One-Hot Encoding is dealing with variables with numerous unique categories since it will create a new column for each unique category. Thus, this project will only include categorical variables with no more than 15 unique values. . categorical_cols = [col for col in X_train_full.columns if X_train_full[col].nunique() &lt;= 15 and X_train_full[col].dtype == &#39;object&#39;] # Select numeric values numeric_cols = [col for col in X_train_full.columns if X_train_full[col].dtype in [&#39;int64&#39;, &#39;float64&#39;]] # Keep selected columns my_columns = categorical_cols + numeric_cols X_train = X_train_full[my_columns].copy() X_valid = X_valid_full[my_columns].copy() X_test = X_test_full[my_columns].copy() . 4.3. Create a pipeline . Pipelines are a great way to keep the data modeling and preprocessing more organized and easier to understand. Creating a pipeline, we&#39;ll handle the missing values and the preprocessing covered in the previous two steps. . As defined above, numerical missing entries will be filled with the mean value while missing categorical variables will be filled with &quot;NA&quot;. Furthermore, categorical columns will also be preprocessed with One-Hot Encoding. . We are using SimpleImputer to fill in missing values and ColumnTransformer will help us to apply the numerical and categorical preprocessors in a single transformer. . numerical_transformer = SimpleImputer(strategy=&#39;mean&#39;) # Preprocessing categorical values categorical_transformer = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;NA&#39;)), (&#39;onehot&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;)) ]) # Pack the preprocessors together preprocessor = ColumnTransformer(transformers=[ (&#39;num&#39;, numerical_transformer, numeric_cols), (&#39;cat&#39;, categorical_transformer, categorical_cols) ]) . 5. Define a model . Now that we have bundled our preprocessors in a pipeline, we can define a model. In this article, we are working with XGBoost, one of the most effective machine learning algorithms, that presents great results in many Kaggle competitions. As a metric of evaluation, we are using the Mean Absolute Error. . model = XGBRegressor(verbosity=0, random_state=0) # Pack preprocessing and modeling together in a pipeline my_pipeline = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;model&#39;, model) ]) # Preprocessing of training data, fit model my_pipeline.fit(X_train, y_train) # Preprocessing of validation data, get predictions preds = my_pipeline.predict(X_valid) print(&#39;MAE:&#39;, mean_absolute_error(y_valid, preds)) . MAE: 16706.181988441782 . 6. Cross-validation . Using Cross-Validation can yield better results. Instead of simply using the training and test sets, cross-validation will run our model on different subsets of the data to get multiple measures of model quality. . We&#39;ll use the cross-validator KFold in its default setup to split the training data into 5 folds. Then, each fold will be used once as validation while the remaining folds will form the training set. After that, cross-validate will evaluate the metrics. In this case, we&#39;re using the Mean Absolute Error. . kfold = KFold(shuffle=True, random_state=0) # Evaluating the Mean Absolute Error scores = cross_validate(my_pipeline, X_train, y_train, scoring=&#39;neg_mean_absolute_error&#39;, cv=kfold) # Multiply by -1 since sklearn calculates negative MAE print(&#39;Average MAE score:&#39;, (scores[&#39;test_score&#39;] * -1).mean()) . Average MAE score: 16168.894833206665 . With cross-validation we could improve our score, reducing the error. In the next step, we&#39;ll try to further improve the model, optimizing some hyperparameters. . 7. Hyperparameter tuning . XGBoost in its default setup usually yields great results, but it also has plenty of hyperparameters that can be optimized to improve the model. Here, we&#39;ll use a method called GridSearchCV which will search over specified parameter values and return the best ones. Once again, we&#39;ll utilize the pipeline and the cross-validator KFold defined above. . GridSearchCV will perform an exhaustive search over parameters, which can demand a lot of computational power and take a lot of time to be finished. We can speed up the process a little bit by setting the parameter n_jobs to -1, which means that the machine will use all processors on the task. . &quot;&quot;&quot; To pass parameter in a pipeline, we should add the names of the steps and the parameter name separated by a ‘__’. Ex: Instead of &#39;n_estimators&#39;, we should set &#39;model__n_estimators&#39;. https://github.com/scikit-learn/scikit-learn/issues/18472 &quot;&quot;&quot; # parameters to be searched over param_grid = {&#39;model__n_estimators&#39;: [10, 50, 100, 200, 400, 600], &#39;model__max_depth&#39;: [2, 3, 5, 7, 10], &#39;model__min_child_weight&#39;: [0.0001, 0.001, 0.01], &#39;model__learning_rate&#39;: [0.01, 0.1, 0.5, 1]} # find the best parameter kfold = KFold(shuffle=True, random_state=0) grid_search = GridSearchCV(my_pipeline, param_grid, scoring=&#39;neg_mean_absolute_error&#39;, cv=kfold, n_jobs=-1) grid_result = grid_search.fit(X_train, y_train) . print(&#39;Best result:&#39;, round((grid_result.best_score_ * -1), 2), &#39;for&#39;, grid_result.best_params_) . Best result: 15750.17 for {&#39;model__learning_rate&#39;: 0.1, &#39;model__max_depth&#39;: 3, &#39;model__min_child_weight&#39;: 0.0001, &#39;model__n_estimators&#39;: 400} . 8. Generate test predictions . After tuning some hyperparameters, it&#39;s time to go over the modeling process again to make predictions on the test set. We&#39;ll define our final model based on the optimized values provided by GridSearchCV. . final_model = XGBRegressor(n_estimators=400, max_depth=3, min_child_weight=0.0001, learning_rate=0.1, verbosity=0, random_state=0 ) # Create a pipeline final_pipeline = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;final_model&#39;, final_model) ]) # Fit the model final_pipeline.fit(X_train, y_train) # Get predictions on the test set final_prediction = final_pipeline.predict(X_test) . 9. Submit your results . We&#39;re almost there! The machine learning modeling is done, but we still need to submit our results to have our score recorded. . This step is quite simple. We need to create a .csv file containing the predictions. This file consists of a DataFrame with two columns. In this case, one column for &quot;Id&quot; and the other one for the test predictions on the target feature. . output = pd.DataFrame({&#39;Id&#39;: X_test.index, &#39;SalePrice&#39;: final_prediction}) output.to_csv(&#39;submission.csv&#39;, index=False) . 10. Join the competition . Finally, we just need to join the competition. Please follow the steps below, according to Kaggle&#39;s instructions. . Start by accessing the competition page and clicking on Join Competition. | In your Kaggle notebook, click on the blue Save Version button in the top right corner of the window. | A pop-up window will show up. Select the option Save and Run All and then click on the blue Save button. | A new pop-up shows up in the bottom left corner while your notebook is running. When it stops running, click on the number to the right of the Save Version button. You should click on the ellipsis (...) to the right of the most recent notebook version, and select Open in Viewer. This brings you into view mode of the same page. | Now, click on the Output tab on the right of the screen. Then, click on the blue Submit button to submit your results to the leaderboard. | . After submitting, you can check your score and position on the leaderboard. . . Conclusion . This article was intended to be instructive, helping data science beginners to structure their first projects on Kaggle in simple steps. With this straightforward approach, I&#39;ve got a score of 14,778.87, which ranked this project in the Top 7%. . After further studying, you can go back on past projects and try to enhance their performance, using new skills you&#39;ve learned. To improve this project, we could investigate and treat the outliers more closely, apply a different approach to missing values, or do some feature engineering, for instance. . My advice to beginners is to keep it simple when starting out. Instead of aiming at the &quot;perfect&quot; model, focus on completing the project, applying your skills correctly, and learning from your mistakes, understanding where and why you messed things up. The data science community is on constant expansion and there&#39;s plenty of more experienced folks willing to help on websites like Kaggle or Stack Overflow. Try to learn from their past mistakes as well! With practice and discipline, it&#39;s just a matter of time to start building more elaborate projects and climb up the ranking of Kaggle&#39;s competitions. .",
            "url": "https://jmmerrell.github.io/ws/xgboost/youtube/api/google%20vision/2021/07/30/youtube_views_xgboost.html",
            "relUrl": "/xgboost/youtube/api/google%20vision/2021/07/30/youtube_views_xgboost.html",
            "date": " • Jul 30, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jmmerrell.github.io/ws/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jmmerrell.github.io/ws/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jmmerrell.github.io/ws/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jmmerrell.github.io/ws/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}