{
  
    
        "post0": {
            "title": "How to get started on Kaggle Competitions",
            "content": "1. Understand the Data . The first step when you face a new data set is to take some time to know the data. In Kaggle competitions, you&#39;ll come across something like the sample below. . . On the competition&#39;s page, you can check the project description on Overview and you&#39;ll find useful information about the data set on the tab Data. In Kaggle competitions, it&#39;s common to have the training and test sets provided in separate files. On the same tab, there&#39;s usually a summary of the features you&#39;ll be working with and some basic statistics. It&#39;s crucial to understand which problem needs to be addressed and the data set we have at hand. . You can use the Kaggle notebooks to execute your projects, as they are similar to Jupyter Notebooks. . 2. Import the necessary libraries and data set . 2.1. Libraries . The libraries used in this project are the following. . import pandas as pd # Data analysis tool import numpy as np # Package for scientific computing from sklearn.model_selection import train_test_split # Splits arrays or matrices into random train and test subsets from sklearn.model_selection import KFold # Cross-validator from sklearn.model_selection import cross_validate # Evaluate metrics by cross-validation from sklearn.model_selection import GridSearchCV # Search over specified parameter values for an estimator from sklearn.compose import ColumnTransformer # Applies transformers to columns of DataFrames from sklearn.pipeline import Pipeline # Helps building a chain of transforms and estimators from sklearn.impute import SimpleImputer # Imputation transformer for completing missing values from sklearn.preprocessing import OneHotEncoder # Encode categorical features from sklearn.metrics import mean_absolute_error # One of many statistical measures of error from xgboost import XGBRegressor # Our model estimator . 2.2. Data set . The next step is to read the data set into a pandas DataFrame and obtain target vector y, which will be the column SalePrice, and predictors X, which, for now, will be the remaining columns. . X_full = pd.read_csv(&#39;https://raw.githubusercontent.com/rmpbastos/data_sets/main/housing_price_train.csv&#39;, index_col=&#39;Id&#39;) X_test_full = pd.read_csv(&#39;https://raw.githubusercontent.com/rmpbastos/data_sets/main/housing_price_test.csv&#39;, index_col=&#39;Id&#39;) # Obtain target vectors and predictors X = X_full.copy() y = X.SalePrice X.drop([&#39;SalePrice&#39;], axis=1, inplace=True) . To get an overview of the data, let&#39;s check the first rows and the size of the data set. . X.head() . MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating HeatingQC CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition . Id . 1 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2003 | 2003 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 196.0 | Gd | TA | PConc | Gd | TA | No | GLQ | 706 | Unf | 0 | 150 | 856 | GasA | Ex | Y | SBrkr | 856 | 854 | 0 | 1710 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 8 | Typ | 0 | NaN | Attchd | 2003.0 | RFn | 2 | 548 | TA | TA | Y | 0 | 61 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | . 2 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | FR2 | Gtl | Veenker | Feedr | Norm | 1Fam | 1Story | 6 | 8 | 1976 | 1976 | Gable | CompShg | MetalSd | MetalSd | None | 0.0 | TA | TA | CBlock | Gd | TA | Gd | ALQ | 978 | Unf | 0 | 284 | 1262 | GasA | Ex | Y | SBrkr | 1262 | 0 | 0 | 1262 | 0 | 1 | 2 | 0 | 3 | 1 | TA | 6 | Typ | 1 | TA | Attchd | 1976.0 | RFn | 2 | 460 | TA | TA | Y | 298 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | . 3 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2001 | 2002 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 162.0 | Gd | TA | PConc | Gd | TA | Mn | GLQ | 486 | Unf | 0 | 434 | 920 | GasA | Ex | Y | SBrkr | 920 | 866 | 0 | 1786 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 6 | Typ | 1 | TA | Attchd | 2001.0 | RFn | 2 | 608 | TA | TA | Y | 0 | 42 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | . 4 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | Crawfor | Norm | Norm | 1Fam | 2Story | 7 | 5 | 1915 | 1970 | Gable | CompShg | Wd Sdng | Wd Shng | None | 0.0 | TA | TA | BrkTil | TA | Gd | No | ALQ | 216 | Unf | 0 | 540 | 756 | GasA | Gd | Y | SBrkr | 961 | 756 | 0 | 1717 | 1 | 0 | 1 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Detchd | 1998.0 | Unf | 3 | 642 | TA | TA | Y | 0 | 35 | 272 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | . 5 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | FR2 | Gtl | NoRidge | Norm | Norm | 1Fam | 2Story | 8 | 5 | 2000 | 2000 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 350.0 | Gd | TA | PConc | Gd | TA | Av | GLQ | 655 | Unf | 0 | 490 | 1145 | GasA | Ex | Y | SBrkr | 1145 | 1053 | 0 | 2198 | 1 | 0 | 2 | 1 | 4 | 1 | Gd | 9 | Typ | 1 | TA | Attchd | 2000.0 | RFn | 3 | 836 | TA | TA | Y | 192 | 84 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | . X.shape . (1460, 79) . y.shape . (1460,) . We have 1,460 rows and 79 columns. Later on, we&#39;ll check these columns to verify which of them will be meaningful to the model. . In the next step, we&#39;ll split the data into training and validation sets. . 3. Training and validation data . It is crucial to break our data into a set for training the model and another one to validate the results. It&#39;s worth mentioning that we should never use the test data here. Our test set stays untouched until we are satisfied with our model&#39;s performance. . What we&#39;re going to do is taking the predictors X and target vector y and breaking them into training and validation sets. For that, we&#39;ll use scikit-learn&#39;s train_test_split. . X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0) . Checking the shape of our training and validation sets, we get the following. . print(f&#39;Shape of X_train_full: {X_train_full.shape}&#39;) print(f&#39;Shape of X_valid_full: {X_valid_full.shape}&#39;) print(f&#39;Shape of y_train: {y_train.shape}&#39;) print(f&#39;Shape of y_valid: {y_valid.shape}&#39;) . Shape of X_train_full: (1168, 79) Shape of X_valid_full: (292, 79) Shape of y_train: (1168,) Shape of y_valid: (292,) . 4. Analyze and prepare the data . Now, we start analyzing the data by checking some information about the features. . X.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1460 entries, 1 to 1460 Data columns (total 79 columns): # Column Non-Null Count Dtype -- -- 0 MSSubClass 1460 non-null int64 1 MSZoning 1460 non-null object 2 LotFrontage 1201 non-null float64 3 LotArea 1460 non-null int64 4 Street 1460 non-null object 5 Alley 91 non-null object 6 LotShape 1460 non-null object 7 LandContour 1460 non-null object 8 Utilities 1460 non-null object 9 LotConfig 1460 non-null object 10 LandSlope 1460 non-null object 11 Neighborhood 1460 non-null object 12 Condition1 1460 non-null object 13 Condition2 1460 non-null object 14 BldgType 1460 non-null object 15 HouseStyle 1460 non-null object 16 OverallQual 1460 non-null int64 17 OverallCond 1460 non-null int64 18 YearBuilt 1460 non-null int64 19 YearRemodAdd 1460 non-null int64 20 RoofStyle 1460 non-null object 21 RoofMatl 1460 non-null object 22 Exterior1st 1460 non-null object 23 Exterior2nd 1460 non-null object 24 MasVnrType 1452 non-null object 25 MasVnrArea 1452 non-null float64 26 ExterQual 1460 non-null object 27 ExterCond 1460 non-null object 28 Foundation 1460 non-null object 29 BsmtQual 1423 non-null object 30 BsmtCond 1423 non-null object 31 BsmtExposure 1422 non-null object 32 BsmtFinType1 1423 non-null object 33 BsmtFinSF1 1460 non-null int64 34 BsmtFinType2 1422 non-null object 35 BsmtFinSF2 1460 non-null int64 36 BsmtUnfSF 1460 non-null int64 37 TotalBsmtSF 1460 non-null int64 38 Heating 1460 non-null object 39 HeatingQC 1460 non-null object 40 CentralAir 1460 non-null object 41 Electrical 1459 non-null object 42 1stFlrSF 1460 non-null int64 43 2ndFlrSF 1460 non-null int64 44 LowQualFinSF 1460 non-null int64 45 GrLivArea 1460 non-null int64 46 BsmtFullBath 1460 non-null int64 47 BsmtHalfBath 1460 non-null int64 48 FullBath 1460 non-null int64 49 HalfBath 1460 non-null int64 50 BedroomAbvGr 1460 non-null int64 51 KitchenAbvGr 1460 non-null int64 52 KitchenQual 1460 non-null object 53 TotRmsAbvGrd 1460 non-null int64 54 Functional 1460 non-null object 55 Fireplaces 1460 non-null int64 56 FireplaceQu 770 non-null object 57 GarageType 1379 non-null object 58 GarageYrBlt 1379 non-null float64 59 GarageFinish 1379 non-null object 60 GarageCars 1460 non-null int64 61 GarageArea 1460 non-null int64 62 GarageQual 1379 non-null object 63 GarageCond 1379 non-null object 64 PavedDrive 1460 non-null object 65 WoodDeckSF 1460 non-null int64 66 OpenPorchSF 1460 non-null int64 67 EnclosedPorch 1460 non-null int64 68 3SsnPorch 1460 non-null int64 69 ScreenPorch 1460 non-null int64 70 PoolArea 1460 non-null int64 71 PoolQC 7 non-null object 72 Fence 281 non-null object 73 MiscFeature 54 non-null object 74 MiscVal 1460 non-null int64 75 MoSold 1460 non-null int64 76 YrSold 1460 non-null int64 77 SaleType 1460 non-null object 78 SaleCondition 1460 non-null object dtypes: float64(3), int64(33), object(43) memory usage: 912.5+ KB . From the summary above, we can observe that some columns have missing values. Let&#39;s take a closer look. . 4.1. Missing Values . missing_values = X.isnull().sum() missing_values = missing_values[missing_values &gt; 0].sort_values(ascending=False) print(missing_values) . PoolQC 1453 MiscFeature 1406 Alley 1369 Fence 1179 FireplaceQu 690 LotFrontage 259 GarageYrBlt 81 GarageType 81 GarageFinish 81 GarageQual 81 GarageCond 81 BsmtFinType2 38 BsmtExposure 38 BsmtFinType1 37 BsmtCond 37 BsmtQual 37 MasVnrArea 8 MasVnrType 8 Electrical 1 dtype: int64 . Some features have missing values counting for the majority of their entries. Checking the competition page, we find more details about the values for each feature, which will help us handle missing data. . For instance, in the columns PoolQC, MiscFeature, Alley, Fence, and FireplaceQu, the missing values mean that the house doesn&#39;t count with that specific feature, so, we&#39;ll fill the missing values with &quot;NA&quot;. All the null values in columns starting with Garage and Bsmt are related to houses that don&#39;t have a garage or basement, respectively. We&#39;ll fill those and the remaining null values with &quot;NA&quot; or the mean value, considering if the features are categorical or numerical. . 4.2. Preprocessing the categorical variables . Most machine learning models only work with numerical variables. Therefore, if we feed the model with categorical variables without preprocessing them first, we&#39;ll get an error. . There are several ways to deal with categorical values. Here, we&#39;ll use One-Hot Encoding, which will create new columns indicating the presence or absence of each value in the original data. . One issue of One-Hot Encoding is dealing with variables with numerous unique categories since it will create a new column for each unique category. Thus, this project will only include categorical variables with no more than 15 unique values. . categorical_cols = [col for col in X_train_full.columns if X_train_full[col].nunique() &lt;= 15 and X_train_full[col].dtype == &#39;object&#39;] # Select numeric values numeric_cols = [col for col in X_train_full.columns if X_train_full[col].dtype in [&#39;int64&#39;, &#39;float64&#39;]] # Keep selected columns my_columns = categorical_cols + numeric_cols X_train = X_train_full[my_columns].copy() X_valid = X_valid_full[my_columns].copy() X_test = X_test_full[my_columns].copy() . 4.3. Create a pipeline . Pipelines are a great way to keep the data modeling and preprocessing more organized and easier to understand. Creating a pipeline, we&#39;ll handle the missing values and the preprocessing covered in the previous two steps. . As defined above, numerical missing entries will be filled with the mean value while missing categorical variables will be filled with &quot;NA&quot;. Furthermore, categorical columns will also be preprocessed with One-Hot Encoding. . We are using SimpleImputer to fill in missing values and ColumnTransformer will help us to apply the numerical and categorical preprocessors in a single transformer. . numerical_transformer = SimpleImputer(strategy=&#39;mean&#39;) # Preprocessing categorical values categorical_transformer = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;NA&#39;)), (&#39;onehot&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;)) ]) # Pack the preprocessors together preprocessor = ColumnTransformer(transformers=[ (&#39;num&#39;, numerical_transformer, numeric_cols), (&#39;cat&#39;, categorical_transformer, categorical_cols) ]) . 5. Define a model . Now that we have bundled our preprocessors in a pipeline, we can define a model. In this article, we are working with XGBoost, one of the most effective machine learning algorithms, that presents great results in many Kaggle competitions. As a metric of evaluation, we are using the Mean Absolute Error. . model = XGBRegressor(verbosity=0, random_state=0) # Pack preprocessing and modeling together in a pipeline my_pipeline = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;model&#39;, model) ]) # Preprocessing of training data, fit model my_pipeline.fit(X_train, y_train) # Preprocessing of validation data, get predictions preds = my_pipeline.predict(X_valid) print(&#39;MAE:&#39;, mean_absolute_error(y_valid, preds)) . MAE: 16706.181988441782 . 6. Cross-validation . Using Cross-Validation can yield better results. Instead of simply using the training and test sets, cross-validation will run our model on different subsets of the data to get multiple measures of model quality. . We&#39;ll use the cross-validator KFold in its default setup to split the training data into 5 folds. Then, each fold will be used once as validation while the remaining folds will form the training set. After that, cross-validate will evaluate the metrics. In this case, we&#39;re using the Mean Absolute Error. . kfold = KFold(shuffle=True, random_state=0) # Evaluating the Mean Absolute Error scores = cross_validate(my_pipeline, X_train, y_train, scoring=&#39;neg_mean_absolute_error&#39;, cv=kfold) # Multiply by -1 since sklearn calculates negative MAE print(&#39;Average MAE score:&#39;, (scores[&#39;test_score&#39;] * -1).mean()) . Average MAE score: 16168.894833206665 . With cross-validation we could improve our score, reducing the error. In the next step, we&#39;ll try to further improve the model, optimizing some hyperparameters. . 7. Hyperparameter tuning . XGBoost in its default setup usually yields great results, but it also has plenty of hyperparameters that can be optimized to improve the model. Here, we&#39;ll use a method called GridSearchCV which will search over specified parameter values and return the best ones. Once again, we&#39;ll utilize the pipeline and the cross-validator KFold defined above. . GridSearchCV will perform an exhaustive search over parameters, which can demand a lot of computational power and take a lot of time to be finished. We can speed up the process a little bit by setting the parameter n_jobs to -1, which means that the machine will use all processors on the task. . &quot;&quot;&quot; To pass parameter in a pipeline, we should add the names of the steps and the parameter name separated by a ‘__’. Ex: Instead of &#39;n_estimators&#39;, we should set &#39;model__n_estimators&#39;. https://github.com/scikit-learn/scikit-learn/issues/18472 &quot;&quot;&quot; # parameters to be searched over param_grid = {&#39;model__n_estimators&#39;: [10, 50, 100, 200, 400, 600], &#39;model__max_depth&#39;: [2, 3, 5, 7, 10], &#39;model__min_child_weight&#39;: [0.0001, 0.001, 0.01], &#39;model__learning_rate&#39;: [0.01, 0.1, 0.5, 1]} # find the best parameter kfold = KFold(shuffle=True, random_state=0) grid_search = GridSearchCV(my_pipeline, param_grid, scoring=&#39;neg_mean_absolute_error&#39;, cv=kfold, n_jobs=-1) grid_result = grid_search.fit(X_train, y_train) . print(&#39;Best result:&#39;, round((grid_result.best_score_ * -1), 2), &#39;for&#39;, grid_result.best_params_) . Best result: 15750.17 for {&#39;model__learning_rate&#39;: 0.1, &#39;model__max_depth&#39;: 3, &#39;model__min_child_weight&#39;: 0.0001, &#39;model__n_estimators&#39;: 400} . 8. Generate test predictions . After tuning some hyperparameters, it&#39;s time to go over the modeling process again to make predictions on the test set. We&#39;ll define our final model based on the optimized values provided by GridSearchCV. . final_model = XGBRegressor(n_estimators=400, max_depth=3, min_child_weight=0.0001, learning_rate=0.1, verbosity=0, random_state=0 ) # Create a pipeline final_pipeline = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;final_model&#39;, final_model) ]) # Fit the model final_pipeline.fit(X_train, y_train) # Get predictions on the test set final_prediction = final_pipeline.predict(X_test) . 9. Submit your results . We&#39;re almost there! The machine learning modeling is done, but we still need to submit our results to have our score recorded. . This step is quite simple. We need to create a .csv file containing the predictions. This file consists of a DataFrame with two columns. In this case, one column for &quot;Id&quot; and the other one for the test predictions on the target feature. . output = pd.DataFrame({&#39;Id&#39;: X_test.index, &#39;SalePrice&#39;: final_prediction}) output.to_csv(&#39;submission.csv&#39;, index=False) . 10. Join the competition . Finally, we just need to join the competition. Please follow the steps below, according to Kaggle&#39;s instructions. . Start by accessing the competition page and clicking on Join Competition. | In your Kaggle notebook, click on the blue Save Version button in the top right corner of the window. | A pop-up window will show up. Select the option Save and Run All and then click on the blue Save button. | A new pop-up shows up in the bottom left corner while your notebook is running. When it stops running, click on the number to the right of the Save Version button. You should click on the ellipsis (...) to the right of the most recent notebook version, and select Open in Viewer. This brings you into view mode of the same page. | Now, click on the Output tab on the right of the screen. Then, click on the blue Submit button to submit your results to the leaderboard. | . After submitting, you can check your score and position on the leaderboard. . . Conclusion . This article was intended to be instructive, helping data science beginners to structure their first projects on Kaggle in simple steps. With this straightforward approach, I&#39;ve got a score of 14,778.87, which ranked this project in the Top 7%. . After further studying, you can go back on past projects and try to enhance their performance, using new skills you&#39;ve learned. To improve this project, we could investigate and treat the outliers more closely, apply a different approach to missing values, or do some feature engineering, for instance. . My advice to beginners is to keep it simple when starting out. Instead of aiming at the &quot;perfect&quot; model, focus on completing the project, applying your skills correctly, and learning from your mistakes, understanding where and why you messed things up. The data science community is on constant expansion and there&#39;s plenty of more experienced folks willing to help on websites like Kaggle or Stack Overflow. Try to learn from their past mistakes as well! With practice and discipline, it&#39;s just a matter of time to start building more elaborate projects and climb up the ranking of Kaggle&#39;s competitions. .",
            "url": "https://jmmerrell.github.io/ws/poop/stuff/2021/07/30/youtube_views_xgboost.html",
            "relUrl": "/poop/stuff/2021/07/30/youtube_views_xgboost.html",
            "date": " • Jul 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jmmerrell.github.io/ws/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jmmerrell.github.io/ws/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jmmerrell.github.io/ws/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jmmerrell.github.io/ws/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}