{
  
    
        "post0": {
            "title": "How to get started on Kaggle Competitions",
            "content": "1. Understand the Data . The first step when you face a new data set is to take some time to know the data. In Kaggle competitions, you&#39;ll come across something like the sample below. . . On the competition&#39;s page, you can check the project description on Overview and you&#39;ll find useful information about the data set on the tab Data. In Kaggle competitions, it&#39;s common to have the training and test sets provided in separate files. On the same tab, there&#39;s usually a summary of the features you&#39;ll be working with and some basic statistics. It&#39;s crucial to understand which problem needs to be addressed and the data set we have at hand. . You can use the Kaggle notebooks to execute your projects, as they are similar to Jupyter Notebooks. . 2. Import the necessary libraries and data set . 2.1. Libraries . The libraries used in this project are the following. . import pandas as pd # Data analysis tool import numpy as np # Package for scientific computing from sklearn.model_selection import train_test_split # Splits arrays or matrices into random train and test subsets from sklearn.model_selection import KFold # Cross-validator from sklearn.model_selection import cross_validate # Evaluate metrics by cross-validation from sklearn.model_selection import GridSearchCV # Search over specified parameter values for an estimator from sklearn.compose import ColumnTransformer # Applies transformers to columns of DataFrames from sklearn.pipeline import Pipeline # Helps building a chain of transforms and estimators from sklearn.impute import SimpleImputer # Imputation transformer for completing missing values from sklearn.preprocessing import OneHotEncoder # Encode categorical features from sklearn.metrics import mean_absolute_error # One of many statistical measures of error from xgboost import XGBRegressor # Our model estimator . 2.2. Data set . The next step is to read the data set into a pandas DataFrame and obtain target vector y, which will be the column SalePrice, and predictors X, which, for now, will be the remaining columns. . X_full = pd.read_csv(&#39;https://raw.githubusercontent.com/rmpbastos/data_sets/main/housing_price_train.csv&#39;, index_col=&#39;Id&#39;) X_test_full = pd.read_csv(&#39;https://raw.githubusercontent.com/rmpbastos/data_sets/main/housing_price_test.csv&#39;, index_col=&#39;Id&#39;) # Obtain target vectors and predictors X = X_full.copy() y = X.SalePrice X.drop([&#39;SalePrice&#39;], axis=1, inplace=True) . To get an overview of the data, let&#39;s check the first rows and the size of the data set. . X.head() . MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating HeatingQC CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition . Id . 1 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2003 | 2003 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 196.0 | Gd | TA | PConc | Gd | TA | No | GLQ | 706 | Unf | 0 | 150 | 856 | GasA | Ex | Y | SBrkr | 856 | 854 | 0 | 1710 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 8 | Typ | 0 | NaN | Attchd | 2003.0 | RFn | 2 | 548 | TA | TA | Y | 0 | 61 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | . 2 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | FR2 | Gtl | Veenker | Feedr | Norm | 1Fam | 1Story | 6 | 8 | 1976 | 1976 | Gable | CompShg | MetalSd | MetalSd | None | 0.0 | TA | TA | CBlock | Gd | TA | Gd | ALQ | 978 | Unf | 0 | 284 | 1262 | GasA | Ex | Y | SBrkr | 1262 | 0 | 0 | 1262 | 0 | 1 | 2 | 0 | 3 | 1 | TA | 6 | Typ | 1 | TA | Attchd | 1976.0 | RFn | 2 | 460 | TA | TA | Y | 298 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | . 3 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2001 | 2002 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 162.0 | Gd | TA | PConc | Gd | TA | Mn | GLQ | 486 | Unf | 0 | 434 | 920 | GasA | Ex | Y | SBrkr | 920 | 866 | 0 | 1786 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 6 | Typ | 1 | TA | Attchd | 2001.0 | RFn | 2 | 608 | TA | TA | Y | 0 | 42 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | . 4 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | Crawfor | Norm | Norm | 1Fam | 2Story | 7 | 5 | 1915 | 1970 | Gable | CompShg | Wd Sdng | Wd Shng | None | 0.0 | TA | TA | BrkTil | TA | Gd | No | ALQ | 216 | Unf | 0 | 540 | 756 | GasA | Gd | Y | SBrkr | 961 | 756 | 0 | 1717 | 1 | 0 | 1 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Detchd | 1998.0 | Unf | 3 | 642 | TA | TA | Y | 0 | 35 | 272 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | . 5 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | FR2 | Gtl | NoRidge | Norm | Norm | 1Fam | 2Story | 8 | 5 | 2000 | 2000 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 350.0 | Gd | TA | PConc | Gd | TA | Av | GLQ | 655 | Unf | 0 | 490 | 1145 | GasA | Ex | Y | SBrkr | 1145 | 1053 | 0 | 2198 | 1 | 0 | 2 | 1 | 4 | 1 | Gd | 9 | Typ | 1 | TA | Attchd | 2000.0 | RFn | 3 | 836 | TA | TA | Y | 192 | 84 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | . X.shape . (1460, 79) . y.shape . (1460,) . We have 1,460 rows and 79 columns. Later on, we&#39;ll check these columns to verify which of them will be meaningful to the model. . In the next step, we&#39;ll split the data into training and validation sets. . 3. Training and validation data . It is crucial to break our data into a set for training the model and another one to validate the results. It&#39;s worth mentioning that we should never use the test data here. Our test set stays untouched until we are satisfied with our model&#39;s performance. . What we&#39;re going to do is taking the predictors X and target vector y and breaking them into training and validation sets. For that, we&#39;ll use scikit-learn&#39;s train_test_split. . X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0) . Checking the shape of our training and validation sets, we get the following. . print(f&#39;Shape of X_train_full: {X_train_full.shape}&#39;) print(f&#39;Shape of X_valid_full: {X_valid_full.shape}&#39;) print(f&#39;Shape of y_train: {y_train.shape}&#39;) print(f&#39;Shape of y_valid: {y_valid.shape}&#39;) . Shape of X_train_full: (1168, 79) Shape of X_valid_full: (292, 79) Shape of y_train: (1168,) Shape of y_valid: (292,) . 4. Analyze and prepare the data . Now, we start analyzing the data by checking some information about the features. . X.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1460 entries, 1 to 1460 Data columns (total 79 columns): # Column Non-Null Count Dtype -- -- 0 MSSubClass 1460 non-null int64 1 MSZoning 1460 non-null object 2 LotFrontage 1201 non-null float64 3 LotArea 1460 non-null int64 4 Street 1460 non-null object 5 Alley 91 non-null object 6 LotShape 1460 non-null object 7 LandContour 1460 non-null object 8 Utilities 1460 non-null object 9 LotConfig 1460 non-null object 10 LandSlope 1460 non-null object 11 Neighborhood 1460 non-null object 12 Condition1 1460 non-null object 13 Condition2 1460 non-null object 14 BldgType 1460 non-null object 15 HouseStyle 1460 non-null object 16 OverallQual 1460 non-null int64 17 OverallCond 1460 non-null int64 18 YearBuilt 1460 non-null int64 19 YearRemodAdd 1460 non-null int64 20 RoofStyle 1460 non-null object 21 RoofMatl 1460 non-null object 22 Exterior1st 1460 non-null object 23 Exterior2nd 1460 non-null object 24 MasVnrType 1452 non-null object 25 MasVnrArea 1452 non-null float64 26 ExterQual 1460 non-null object 27 ExterCond 1460 non-null object 28 Foundation 1460 non-null object 29 BsmtQual 1423 non-null object 30 BsmtCond 1423 non-null object 31 BsmtExposure 1422 non-null object 32 BsmtFinType1 1423 non-null object 33 BsmtFinSF1 1460 non-null int64 34 BsmtFinType2 1422 non-null object 35 BsmtFinSF2 1460 non-null int64 36 BsmtUnfSF 1460 non-null int64 37 TotalBsmtSF 1460 non-null int64 38 Heating 1460 non-null object 39 HeatingQC 1460 non-null object 40 CentralAir 1460 non-null object 41 Electrical 1459 non-null object 42 1stFlrSF 1460 non-null int64 43 2ndFlrSF 1460 non-null int64 44 LowQualFinSF 1460 non-null int64 45 GrLivArea 1460 non-null int64 46 BsmtFullBath 1460 non-null int64 47 BsmtHalfBath 1460 non-null int64 48 FullBath 1460 non-null int64 49 HalfBath 1460 non-null int64 50 BedroomAbvGr 1460 non-null int64 51 KitchenAbvGr 1460 non-null int64 52 KitchenQual 1460 non-null object 53 TotRmsAbvGrd 1460 non-null int64 54 Functional 1460 non-null object 55 Fireplaces 1460 non-null int64 56 FireplaceQu 770 non-null object 57 GarageType 1379 non-null object 58 GarageYrBlt 1379 non-null float64 59 GarageFinish 1379 non-null object 60 GarageCars 1460 non-null int64 61 GarageArea 1460 non-null int64 62 GarageQual 1379 non-null object 63 GarageCond 1379 non-null object 64 PavedDrive 1460 non-null object 65 WoodDeckSF 1460 non-null int64 66 OpenPorchSF 1460 non-null int64 67 EnclosedPorch 1460 non-null int64 68 3SsnPorch 1460 non-null int64 69 ScreenPorch 1460 non-null int64 70 PoolArea 1460 non-null int64 71 PoolQC 7 non-null object 72 Fence 281 non-null object 73 MiscFeature 54 non-null object 74 MiscVal 1460 non-null int64 75 MoSold 1460 non-null int64 76 YrSold 1460 non-null int64 77 SaleType 1460 non-null object 78 SaleCondition 1460 non-null object dtypes: float64(3), int64(33), object(43) memory usage: 912.5+ KB . From the summary above, we can observe that some columns have missing values. Let&#39;s take a closer look. . 4.1. Missing Values . missing_values = X.isnull().sum() missing_values = missing_values[missing_values &gt; 0].sort_values(ascending=False) print(missing_values) . PoolQC 1453 MiscFeature 1406 Alley 1369 Fence 1179 FireplaceQu 690 LotFrontage 259 GarageYrBlt 81 GarageType 81 GarageFinish 81 GarageQual 81 GarageCond 81 BsmtFinType2 38 BsmtExposure 38 BsmtFinType1 37 BsmtCond 37 BsmtQual 37 MasVnrArea 8 MasVnrType 8 Electrical 1 dtype: int64 . Some features have missing values counting for the majority of their entries. Checking the competition page, we find more details about the values for each feature, which will help us handle missing data. . For instance, in the columns PoolQC, MiscFeature, Alley, Fence, and FireplaceQu, the missing values mean that the house doesn&#39;t count with that specific feature, so, we&#39;ll fill the missing values with &quot;NA&quot;. All the null values in columns starting with Garage and Bsmt are related to houses that don&#39;t have a garage or basement, respectively. We&#39;ll fill those and the remaining null values with &quot;NA&quot; or the mean value, considering if the features are categorical or numerical. . 4.2. Preprocessing the categorical variables . Most machine learning models only work with numerical variables. Therefore, if we feed the model with categorical variables without preprocessing them first, we&#39;ll get an error. . There are several ways to deal with categorical values. Here, we&#39;ll use One-Hot Encoding, which will create new columns indicating the presence or absence of each value in the original data. . One issue of One-Hot Encoding is dealing with variables with numerous unique categories since it will create a new column for each unique category. Thus, this project will only include categorical variables with no more than 15 unique values. . categorical_cols = [col for col in X_train_full.columns if X_train_full[col].nunique() &lt;= 15 and X_train_full[col].dtype == &#39;object&#39;] # Select numeric values numeric_cols = [col for col in X_train_full.columns if X_train_full[col].dtype in [&#39;int64&#39;, &#39;float64&#39;]] # Keep selected columns my_columns = categorical_cols + numeric_cols X_train = X_train_full[my_columns].copy() X_valid = X_valid_full[my_columns].copy() X_test = X_test_full[my_columns].copy() . 4.3. Create a pipeline . Pipelines are a great way to keep the data modeling and preprocessing more organized and easier to understand. Creating a pipeline, we&#39;ll handle the missing values and the preprocessing covered in the previous two steps. . As defined above, numerical missing entries will be filled with the mean value while missing categorical variables will be filled with &quot;NA&quot;. Furthermore, categorical columns will also be preprocessed with One-Hot Encoding. . We are using SimpleImputer to fill in missing values and ColumnTransformer will help us to apply the numerical and categorical preprocessors in a single transformer. . numerical_transformer = SimpleImputer(strategy=&#39;mean&#39;) # Preprocessing categorical values categorical_transformer = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;NA&#39;)), (&#39;onehot&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;)) ]) # Pack the preprocessors together preprocessor = ColumnTransformer(transformers=[ (&#39;num&#39;, numerical_transformer, numeric_cols), (&#39;cat&#39;, categorical_transformer, categorical_cols) ]) . 5. Define a model . Now that we have bundled our preprocessors in a pipeline, we can define a model. In this article, we are working with XGBoost, one of the most effective machine learning algorithms, that presents great results in many Kaggle competitions. As a metric of evaluation, we are using the Mean Absolute Error. . model = XGBRegressor(verbosity=0, random_state=0) # Pack preprocessing and modeling together in a pipeline my_pipeline = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;model&#39;, model) ]) # Preprocessing of training data, fit model my_pipeline.fit(X_train, y_train) # Preprocessing of validation data, get predictions preds = my_pipeline.predict(X_valid) print(&#39;MAE:&#39;, mean_absolute_error(y_valid, preds)) . MAE: 16706.181988441782 . 6. Cross-validation . Using Cross-Validation can yield better results. Instead of simply using the training and test sets, cross-validation will run our model on different subsets of the data to get multiple measures of model quality. . We&#39;ll use the cross-validator KFold in its default setup to split the training data into 5 folds. Then, each fold will be used once as validation while the remaining folds will form the training set. After that, cross-validate will evaluate the metrics. In this case, we&#39;re using the Mean Absolute Error. . kfold = KFold(shuffle=True, random_state=0) # Evaluating the Mean Absolute Error scores = cross_validate(my_pipeline, X_train, y_train, scoring=&#39;neg_mean_absolute_error&#39;, cv=kfold) # Multiply by -1 since sklearn calculates negative MAE print(&#39;Average MAE score:&#39;, (scores[&#39;test_score&#39;] * -1).mean()) . Average MAE score: 16168.894833206665 . With cross-validation we could improve our score, reducing the error. In the next step, we&#39;ll try to further improve the model, optimizing some hyperparameters. . 7. Hyperparameter tuning . XGBoost in its default setup usually yields great results, but it also has plenty of hyperparameters that can be optimized to improve the model. Here, we&#39;ll use a method called GridSearchCV which will search over specified parameter values and return the best ones. Once again, we&#39;ll utilize the pipeline and the cross-validator KFold defined above. . GridSearchCV will perform an exhaustive search over parameters, which can demand a lot of computational power and take a lot of time to be finished. We can speed up the process a little bit by setting the parameter n_jobs to -1, which means that the machine will use all processors on the task. . &quot;&quot;&quot; To pass parameter in a pipeline, we should add the names of the steps and the parameter name separated by a ‘__’. Ex: Instead of &#39;n_estimators&#39;, we should set &#39;model__n_estimators&#39;. https://github.com/scikit-learn/scikit-learn/issues/18472 &quot;&quot;&quot; # parameters to be searched over param_grid = {&#39;model__n_estimators&#39;: [10, 50, 100, 200, 400, 600], &#39;model__max_depth&#39;: [2, 3, 5, 7, 10], &#39;model__min_child_weight&#39;: [0.0001, 0.001, 0.01], &#39;model__learning_rate&#39;: [0.01, 0.1, 0.5, 1]} # find the best parameter kfold = KFold(shuffle=True, random_state=0) grid_search = GridSearchCV(my_pipeline, param_grid, scoring=&#39;neg_mean_absolute_error&#39;, cv=kfold, n_jobs=-1) grid_result = grid_search.fit(X_train, y_train) . print(&#39;Best result:&#39;, round((grid_result.best_score_ * -1), 2), &#39;for&#39;, grid_result.best_params_) . Best result: 15750.17 for {&#39;model__learning_rate&#39;: 0.1, &#39;model__max_depth&#39;: 3, &#39;model__min_child_weight&#39;: 0.0001, &#39;model__n_estimators&#39;: 400} . 8. Generate test predictions . After tuning some hyperparameters, it&#39;s time to go over the modeling process again to make predictions on the test set. We&#39;ll define our final model based on the optimized values provided by GridSearchCV. . final_model = XGBRegressor(n_estimators=400, max_depth=3, min_child_weight=0.0001, learning_rate=0.1, verbosity=0, random_state=0 ) # Create a pipeline final_pipeline = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;final_model&#39;, final_model) ]) # Fit the model final_pipeline.fit(X_train, y_train) # Get predictions on the test set final_prediction = final_pipeline.predict(X_test) . 9. Submit your results . We&#39;re almost there! The machine learning modeling is done, but we still need to submit our results to have our score recorded. . This step is quite simple. We need to create a .csv file containing the predictions. This file consists of a DataFrame with two columns. In this case, one column for &quot;Id&quot; and the other one for the test predictions on the target feature. . output = pd.DataFrame({&#39;Id&#39;: X_test.index, &#39;SalePrice&#39;: final_prediction}) output.to_csv(&#39;submission.csv&#39;, index=False) . 10. Join the competition . Finally, we just need to join the competition. Please follow the steps below, according to Kaggle&#39;s instructions. . Start by accessing the competition page and clicking on Join Competition. | In your Kaggle notebook, click on the blue Save Version button in the top right corner of the window. | A pop-up window will show up. Select the option Save and Run All and then click on the blue Save button. | A new pop-up shows up in the bottom left corner while your notebook is running. When it stops running, click on the number to the right of the Save Version button. You should click on the ellipsis (...) to the right of the most recent notebook version, and select Open in Viewer. This brings you into view mode of the same page. | Now, click on the Output tab on the right of the screen. Then, click on the blue Submit button to submit your results to the leaderboard. | . After submitting, you can check your score and position on the leaderboard. . . Conclusion . This article was intended to be instructive, helping data science beginners to structure their first projects on Kaggle in simple steps. With this straightforward approach, I&#39;ve got a score of 14,778.87, which ranked this project in the Top 7%. . After further studying, you can go back on past projects and try to enhance their performance, using new skills you&#39;ve learned. To improve this project, we could investigate and treat the outliers more closely, apply a different approach to missing values, or do some feature engineering, for instance. . My advice to beginners is to keep it simple when starting out. Instead of aiming at the &quot;perfect&quot; model, focus on completing the project, applying your skills correctly, and learning from your mistakes, understanding where and why you messed things up. The data science community is on constant expansion and there&#39;s plenty of more experienced folks willing to help on websites like Kaggle or Stack Overflow. Try to learn from their past mistakes as well! With practice and discipline, it&#39;s just a matter of time to start building more elaborate projects and climb up the ranking of Kaggle&#39;s competitions. .",
            "url": "https://jmmerrell.github.io/ws/poop/stuff/2021/07/30/youtube_views_xgboost.html",
            "relUrl": "/poop/stuff/2021/07/30/youtube_views_xgboost.html",
            "date": " • Jul 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "from os import truncate from numpy.core.fromnumeric import shape from numpy.lib.function_base import diff import pandas as pd import re from datetime import datetime, timedelta import numpy as np import matplotlib.pyplot as plt from pandas.core.indexes.base import Index import emoji from scipy import stats import fasttext from collections import Counter from pprint import pprint from IPython.display import display_html from itertools import chain,cycle from deep_translator import GoogleTranslator #create a function to identify which character are a country flag emoji def num_flag_emoji(text): num_country_emoji =0 for c in text: if &quot; U0001F1E6 U0001F1E8&quot; &lt;= c &lt;= &quot; U0001F1FF U0001F1FC&quot; or c in [&quot; U0001F3F4 U000e0067 U000e0062 U000e0065 U000e006e U000e0067 U000e007f&quot;, &quot; U0001F3F4 U000e0067 U000e0062 U000e0073 U000e0063 U000e0074 U000e007f&quot;, &quot; U0001F3F4 U000e0067 U000e0062 U000e0077 U000e006c U000e0073 U000e007f&quot;]: num_country_emoji += 1 return num_country_emoji files= [&quot;jan_el_wero.txt&quot;, &quot;nate&#39;s_adventures.txt&quot;, &quot;nathan_seastrand.txt&quot;, &quot;rusos_reaccionan.txt&quot;, &quot;superholly.txt&quot;, &quot;vlog_güero.txt&quot;, &quot;wero_wero_tv.txt&quot;, &quot;american_boy.txt&quot;, &quot;dustin_luke.txt&quot;, &quot;el_gringo.txt&quot;, &quot;el_güerito.txt&quot;, &quot;ford_quarterman.txt&quot;, &quot;gringa_reacciona.txt&quot;] df = pd.DataFrame(columns=[&#39;video_id&#39;,&#39;title&#39;,&#39;title_len&#39;,&#39;words&#39;,&#39;upper_pct&#39;,&#39;emoji_count&#39;,&#39;upload_date&#39;,&#39;upload_time&#39;, &#39;upload_day&#39;,&#39;upload_time_of_day&#39;,&#39;viewCount&#39;,&#39;likeCount&#39;,&#39;dislikeCount&#39;,&#39;favoriteCount&#39;, &#39;commentCount&#39;,&#39;duration&#39;,&#39;definition&#39;,&#39;caption&#39;,&#39;licensedContent&#39;,&#39;thumbnail_url&#39;, &#39;thumbnail_w&#39;, &#39;thumbnail_h&#39;, &#39;tags&#39;,&#39;num_tags&#39;,&#39;categoryId&#39;,&#39;liveBroadcastContent&#39;,&#39;defaultAudioLanguage&#39;, &#39;topicCategories&#39;, &#39;channel&#39;, &#39;channel_subs&#39;, &#39;channel_views&#39;, &#39;channel_videos&#39;,&#39;desc&#39;]) #Loop through all the youtuber&#39;s data files and combine into on data frame for file in files: df_add= pd.read_csv(file) df = df.append(df_add.drop([&#39;Unnamed: 0&#39;],axis=1)) #Read in the files that have the thumbnail data, and combine with the youtuber data df_thumb = pd.read_csv(&quot;thumbnail_data_20210716_13-23-46.486750.txt&quot;).append(pd.read_csv(&quot;thumbnail_data_20210716_02-31-27.088616.txt&quot;)) df_all = pd.merge(df,df_thumb.drop(&#39;thumbnail_url&#39;,axis=1),on=&quot;video_id&quot;,how=&quot;inner&quot;).drop([&#39;emoji_count&#39;],axis=1) df_all[&#39;all_text&#39;] = df_all[&#39;title&#39;].astype(str) + df_all[&#39;tags&#39;].astype(str) + df_all[&#39;desc&#39;].astype(str) . print(df_all.isna().mean().round(4)) #looks like &quot;defaultAudioLanguage&quot; and &quot;topicCategories&quot; may need imputation and/or further exploration print(df[&#39;defaultAudioLanguage&#39;].value_counts(normalize=True)) print(df[&#39;topicCategories&#39;].value_counts(normalize=True)) #There are too many levels for the &#39;topicCategories&#39; variable and not realated to other variables, so imputation will be difficult #Therefore I will toss &#39;topicCategories&#39; from the study df_all = df_all.drop(&#39;topicCategories&#39;,axis=1) #There are two main languages used in the &#39;defaultAudioLanguage&#39; Spanish and English, so the follwoing model will predict the #language for the missing data using the tags of the youtube videos, tags with more english words in them will mean an English video #This was the most accurate model for predicting video language. Combined the title, tags, and description into all text column #Also used the most common words in the text to feed the model and have 95% accuracy in predictions PRETRAINED_MODEL_PATH = &quot;C: Users merre Desktop lid.176.bin&quot; model = fasttext.load_model(PRETRAINED_MODEL_PATH) df_all[&#39;predict_lang&#39;]=&#39;other&#39; ii=0 #finds which language is most probable, English or Spanish for i in df_all[&quot;all_text&quot;].replace(&#39;[&#39;,&#39;&#39;).replace(&#39;,&#39;,&#39;&#39;).replace(&#39;]&#39;,&#39;&#39;): sup =[item.replace(&#39; n&#39;, &#39;&#39;).replace(&quot;&#39;&quot;,&#39;&#39;) for item in re.findall(r&#39;[ s ? ! &quot; &#39;]+[a-zA-Z u00C0- u00FF]+[ s . ? ! &quot; &#39;]+&#39;,i)] sup=[j[0] for j in Counter(sup).most_common(15)] model_preds= list(model.predict(str(sup),k=4)) if str(model_preds[0][0].replace(&#39;&#39;&#39;__label__&#39;&#39;&#39;,&#39;&#39;)) in [&#39;en&#39;,&#39;es&#39;]: which_pred = 0 elif str(model_preds[0][1].replace(&#39;&#39;&#39;__label__&#39;&#39;&#39;,&#39;&#39;)) in [&#39;en&#39;,&#39;es&#39;]: which_pred =1 elif str(model_preds[0][2].replace(&#39;&#39;&#39;__label__&#39;&#39;&#39;,&#39;&#39;)) in [&#39;en&#39;,&#39;es&#39;]: which_pred =2 elif str(model_preds[0][3].replace(&#39;&#39;&#39;__label__&#39;&#39;&#39;,&#39;&#39;)) in [&#39;en&#39;,&#39;es&#39;]: which_pred =3 else: which_pred=4 if which_pred==4: df_all[&#39;predict_lang&#39;][ii]=&#39;es&#39; else: df_all[&#39;predict_lang&#39;][ii]=model_preds[0][which_pred].replace(&#39;&#39;&#39;__label__&#39;&#39;&#39;,&#39;&#39;) ii +=1 df_all[&#39;new_lang&#39;]=&#39;es&#39; #Use the predictions to replace the missing data for i in range(len(df_all[&#39;predict_lang&#39;])): if str(df_all[&#39;defaultAudioLanguage&#39;][i]).find(&#39;es&#39;) &gt;= 0: df_all[&#39;new_lang&#39;][i]= &#39;es&#39; elif str(df_all[&#39;defaultAudioLanguage&#39;][i]).find(&#39;en&#39;) &gt;= 0: df_all[&#39;new_lang&#39;][i]= &#39;en&#39; elif str(df_all[&#39;defaultAudioLanguage&#39;][i]) in [&#39;&#39;,&#39;nan&#39;,&#39;nan &#39;,&#39; &#39;]: df_all[&#39;new_lang&#39;][i]= df_all[&#39;predict_lang&#39;][i] else: df_all[&#39;new_lang&#39;][i]= df_all[&#39;predict_lang&#39;][i] #Show the side by side charts of the video language before and after imputation, and consolidation of the variable levels def display_side_by_side(*args,titles=cycle([&#39;&#39;])): html_str=&#39;&#39; for df,title in zip(args, chain(titles,cycle([&#39;&lt;/br&gt;&#39;])) ): html_str+=&#39;&lt;th style=&quot;text-align:center&quot;&gt;&lt;td style=&quot;vertical-align:top&quot;&gt;&#39; html_str+=f&#39;&lt;h2&gt;{title}&lt;/h2&gt;&#39; html_str+=df.to_html().replace(&#39;table&#39;,&#39;table style=&quot;display:inline&quot;&#39;) html_str+=&#39;&lt;/td&gt;&lt;/th&gt;&#39; display_html(html_str,raw=True) table1 = pd.DataFrame(df_all[[&#39;new_lang&#39;]].value_counts(),columns=[&#39;Count&#39;]) table2 = pd.DataFrame(df_all[[&#39;defaultAudioLanguage&#39;]].value_counts(),columns=[&#39;Count&#39;]) display_side_by_side(table2,table1, titles=[&#39;Before&#39;,&#39;After&#39;]) #Now we can drop defaultAudioLanguage since we have a replacement field for that df_all = df_all.drop(&#39;defaultAudioLanguage&#39;,axis=1) #We can now remove all rows that have missiong values since the rows with the most missing values are gone df_all = df_all.dropna() df_all = df_all.reset_index() all_labels = list(df_all[&quot;labels&quot;]) all_labels = [ x.split(&#39;,&#39;) for x in df_all[&quot;labels&quot;]] all_labels = [[re.sub(r&#39;[^a-zA-Z u00C0- u00FF s]&#39;, &quot; &quot;,i).strip(&#39; t n r&#39;).upper() for i in ii] for ii in all_labels] all_labels_flat = list(set([item for elem in all_labels for item in elem])) df_all[&#39;labels_words&#39;] = all_labels translation =[GoogleTranslator(source=&#39;en&#39;, target=&#39;es&#39;).translate(i) for i in all_labels_flat[0:500]] . video_id 0.0000 title 0.0000 title_len 0.0000 words 0.0000 upper_pct 0.0000 upload_date 0.0000 upload_time 0.0000 upload_day 0.0000 upload_time_of_day 0.0000 viewCount 0.0107 likeCount 0.0107 dislikeCount 0.0107 favoriteCount 0.0107 commentCount 0.0117 duration 0.0000 definition 0.0112 caption 0.0112 licensedContent 0.0112 thumbnail_url 0.0000 thumbnail_w 0.0000 thumbnail_h 0.0000 tags 0.0000 num_tags 0.0000 categoryId 0.0000 liveBroadcastContent 0.0000 defaultAudioLanguage 0.2026 topicCategories 0.2077 channel 0.0000 channel_subs 0.0000 channel_views 0.0000 channel_videos 0.0000 desc 0.0010 labels 0.0000 faces 0.0000 texts 0.0000 adult 0.0000 medical 0.0000 racy 0.0000 spoof 0.0000 violence 0.0000 all_text 0.0000 dtype: float64 es 0.397689 es-419 0.226377 es-MX 0.225697 en 0.134602 en-US 0.010877 es-US 0.002719 zxx 0.002039 Name: defaultAudioLanguage, dtype: float64 [&#39;https://en.wikipedia.org/wiki/Lifestyle_(sociology)&#39;] 0.309850 [&#39;https://en.wikipedia.org/wiki/Entertainment&#39;] 0.206566 [&#39;https://en.wikipedia.org/wiki/Food&#39;, &#39;https://en.wikipedia.org/wiki/Lifestyle_(sociology)&#39;] 0.076607 [&#39;https://en.wikipedia.org/wiki/Entertainment&#39;, &#39;https://en.wikipedia.org/wiki/Film&#39;] 0.071135 [&#39;https://en.wikipedia.org/wiki/Hobby&#39;, &#39;https://en.wikipedia.org/wiki/Lifestyle_(sociology)&#39;] 0.045144 ... [&#39;https://en.wikipedia.org/wiki/Action-adventure_game&#39;] 0.000684 [&#39;https://en.wikipedia.org/wiki/Entertainment&#39;, &#39;https://en.wikipedia.org/wiki/Mixed_martial_arts&#39;] 0.000684 [&#39;https://en.wikipedia.org/wiki/Hip_hop_music&#39;, &#39;https://en.wikipedia.org/wiki/Music_of_Latin_America&#39;] 0.000684 [&#39;https://en.wikipedia.org/wiki/Entertainment&#39;, &#39;https://en.wikipedia.org/wiki/Military&#39;] 0.000684 [&#39;https://en.wikipedia.org/wiki/Basketball&#39;, &#39;https://en.wikipedia.org/wiki/Sport&#39;] 0.000684 Name: topicCategories, Length: 68, dtype: float64 . Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar. C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:43: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:41: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:52: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:56: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:54: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:58: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . Before . Count . defaultAudioLanguage . es 627 | . es-419 378 | . es-MX 332 | . en 199 | . en-US 20 | . es-US 4 | . zxx 3 | . &lt;/table style=&quot;display:inline&quot;&gt;&lt;/td&gt;&lt;/th&gt;After . Count . new_lang . es 1620 | . en 340 | . &lt;/table style=&quot;display:inline&quot;&gt;&lt;/td&gt;&lt;/th&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; print(len(translation)) translation2 = translation + [GoogleTranslator(source=&#39;en&#39;, target=&#39;es&#39;).translate(i) for i in all_labels_flat[500:1000]] print(len(translation2)) . 500 1000 . print(len(translation2)) translation3 = translation2 + [GoogleTranslator(source=&#39;en&#39;, target=&#39;es&#39;).translate(i) for i in all_labels_flat[1000:1500]] print(len(translation3)) . 1000 1500 . print(len(translation3)) translation4 = translation3 + [GoogleTranslator(source=&#39;en&#39;, target=&#39;es&#39;).translate(i) for i in all_labels_flat[1500:]] print(len(translation4)) . 1500 1574 . #Scans the title for any emojis in general and also of country flags df_all[&quot;flag_emoji_count&quot;]=[(num_flag_emoji(x)&gt;0)*1 for x in df_all[&quot;title&quot;]] df_all[&quot;emoji_count&quot;]= [((emoji.emoji_count(x) - num_flag_emoji(x)/2)&gt;0)*1 for x in df_all[&quot;title&quot;]] ###Create variables to see how well title reflects the description, thumbnail and tags of the video #The text read in by the ggogle vision text detection is messy. #Need to clean up and create new variable &quot;thumb_words&quot;, thumb_word_count&quot; df_all[&#39;labels_words_spanish&#39;] = df_all[&#39;labels_words&#39;] df_all[&#39;labels_words_spanish&#39;] = [[translation4[x].upper() for x in range(len(all_labels_flat)) for y in range(len(z)) if z[y]==all_labels_flat[x]] for z in df_all[&#39;labels_words_spanish&#39;]] df_all[&#39;labels_word_count&#39;]=[len(df_all[&#39;labels_words&#39;][i]) for i in range(len(df_all))] discard_list = [&quot;&quot;,&quot;B&quot;,&#39;C&#39;,&#39;D&#39;,&#39;&#39;,&#39;F&#39;,&#39;G&#39;,&#39;H&#39;,&#39;&#39;,&#39;J&#39;,&#39;K&#39;,&#39;L&#39;,&#39;M&#39;,&#39;N&#39;,&#39;&#39;,&#39;P&#39;,&#39;Q&#39;,&#39;R&#39;,&#39;S&#39;,&#39;T&#39;,&#39;V&#39;,&#39;W&#39;,&#39;X&#39;,&#39;Z&#39;] df_all[&#39;thumb_words&#39;]=[list(set(re.sub(r&#39;[^a-zA-Z u00C0- u00FF]&#39;, &quot; &quot;,str(re.findall(r&#39;[^ n][ s ? ! &quot; &#39;]+[a-zA-Z u00C0- u00FF]+[ s . ? ! &quot; &#39;]?&#39;,str(df_all[&#39;texts&#39;][i])))).upper().split(&quot; &quot;))) for i in range(len(df_all))] for i in range(len(df_all)): test_list = df_all[&#39;thumb_words&#39;][i] remove_list = discard_list df_all[&#39;thumb_words&#39;][i] = [i for i in test_list if i not in remove_list] df_all[&#39;thumb_word_count&#39;]=[len(df_all[&#39;thumb_words&#39;][i]) for i in range(len(df_all))] df_all[&#39;title_in_desc&#39;]=[((df_all[&#39;desc&#39;][i].upper().find(df_all[&#39;title&#39;][i].upper()))&gt;-1)*1 for i in range(len(df_all))] df_all[&#39;thumb_words_in_title&#39;]=[ sum([(str(df_all[&#39;title&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;thumb_words&#39;][z]]) for z in range(len(df_all[&#39;title&#39;]))] df_all[&#39;thumb_words_in_tags&#39;]=[ sum([(str(df_all[&#39;tags&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;thumb_words&#39;][z]]) for z in range(len(df_all[&#39;tags&#39;]))] df_all[&#39;label_words_in_title&#39;]=&#39;&#39; df_all[&#39;label_words_in_tags&#39;]=&#39;&#39; for z in range(len(df_all[&#39;title&#39;])): if df_all[&#39;new_lang&#39;][z]==&#39;es&#39;: df_all[&#39;label_words_in_title&#39;][z]=sum([(str(df_all[&#39;title&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;labels_words_spanish&#39;][z]]) df_all[&#39;label_words_in_tags&#39;][z]=sum([(str(df_all[&#39;tags&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;labels_words_spanish&#39;][z]]) else: df_all[&#39;label_words_in_title&#39;][z]=sum([(str(df_all[&#39;title&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;labels_words&#39;][z]]) df_all[&#39;label_words_in_tags&#39;][z]=sum([(str(df_all[&#39;tags&#39;][z]).upper().find(y)&gt;-1)*1 for y in df_all[&#39;labels_words&#39;][z]]) ##See how many faces are in the thumbnails df_all[&#39;faces_surprised&#39;]=[sum([(str(y)==&#39;suprised&#39;)*1 for y in re.findall(r&#39;[a-zA-Z u00C0- u00FF]+&#39;,df_all[&#39;faces&#39;][z])]) for z in range(len(df_all[&#39;faces&#39;]))] df_all[&#39;faces_angry&#39;]=[sum([(str(y)==&#39;angry&#39;)*1 for y in re.findall(r&#39;[a-zA-Z u00C0- u00FF]+&#39;,df_all[&#39;faces&#39;][z])]) for z in range(len(df_all[&#39;faces&#39;]))] df_all[&#39;faces_happy&#39;]=[sum([(str(y)==&#39;happy&#39;)*1 for y in re.findall(r&#39;[a-zA-Z u00C0- u00FF]+&#39;,df_all[&#39;faces&#39;][z])]) for z in range(len(df_all[&#39;faces&#39;]))] df_all[&#39;faces_other&#39;]=[sum([(str(y)==&#39;other&#39;)*1 for y in re.findall(r&#39;[a-zA-Z u00C0- u00FF]+&#39;,df_all[&#39;faces&#39;][z])]) for z in range(len(df_all[&#39;faces&#39;]))] ###Create varibales to see how much people like the videos df_all[&#39;views_like_ratio&#39;]=df_all[&#39;likeCount&#39;]/df_all[&#39;viewCount&#39;] df_all[&#39;comment_views_ratio&#39;]=df_all[&#39;commentCount&#39;]/df_all[&#39;viewCount&#39;] df_all[&#39;views_favorite_ratio&#39;]=df_all[&#39;favoriteCount&#39;]/df_all[&#39;viewCount&#39;] df_all[&#39;like_percent&#39;]=df_all[&#39;likeCount&#39;]/(df_all[&#39;likeCount&#39;]+df_all[&#39;dislikeCount&#39;]) max_date=max(df_all[&#39;upload_date&#39;]) df_all[&#39;days_since_upload&#39;]= (pd.to_datetime(max_date) - pd.to_datetime(df_all[&#39;upload_date&#39;])).dt.days +1 df_all[&#39;views_per_day&#39;] = df_all[&#39;viewCount&#39;]/df_all[&#39;days_since_upload&#39;] df_all[&#39;views_per_day_per_sub&#39;] = df_all[&#39;viewCount&#39;]/df_all[&#39;days_since_upload&#39;]/df_all[&#39;channel_subs&#39;] df_all[&#39;views_per_sub&#39;] = df_all[&#39;viewCount&#39;]/df_all[&#39;channel_subs&#39;] . C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:23: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:35: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:36: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:38: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy C: Users merre Desktop data projects env_yt_api lib site-packages ipykernel_launcher.py:39: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . print(df_all.columns) # plt.plot( df_all[&#39;days_since_upload&#39;],df_all[&#39;viewCount&#39;]) # plt.xlabel(&#39;Time (hr)&#39;) # plt.ylabel(&#39;Position (km)&#39;) df_model = df_all[[&#39;title_len&#39;, &#39;words&#39;, &#39;upper_pct&#39;, &#39;upload_day&#39;, &#39;upload_time_of_day&#39;, &#39;viewCount&#39;, &#39;likeCount&#39;, &#39;dislikeCount&#39;, &#39;favoriteCount&#39;, &#39;commentCount&#39;, &#39;duration&#39;, &#39;definition&#39;, &#39;caption&#39;, &#39;licensedContent&#39;, &#39;thumbnail_h&#39;, &#39;num_tags&#39;, &#39;categoryId&#39;, &#39;liveBroadcastContent&#39;, &#39;channel_subs&#39;, &#39;channel_views&#39;, &#39;channel_videos&#39;, &#39;adult&#39;, &#39;medical&#39;, &#39;racy&#39;, &#39;spoof&#39;, &#39;violence&#39;,&#39;new_lang&#39;, &#39;flag_emoji_count&#39;, &#39;emoji_count&#39;, &#39;thumb_word_count&#39;, &#39;title_in_desc&#39;, &#39;thumb_words_in_title&#39;, &#39;thumb_words_in_tags&#39;, &#39;label_words_in_title&#39;, &#39;label_words_in_tags&#39;, &#39;days_since_upload&#39;, &#39;views_per_day&#39;, &#39;views_per_day_per_sub&#39;, &#39;views_per_sub&#39;, &#39;views_like_ratio&#39;, &#39;comment_views_ratio&#39;, &#39;views_favorite_ratio&#39;, &#39;like_percent&#39;,&#39;faces_surprised&#39;,&#39;faces_happy&#39;,&#39;faces_angry&#39;,&#39;faces_other&#39;]] print(shape(df_model)) df_model.describe() # plt.scatter(df_all[&#39;views_like_ratio&#39;],df_all[&#39;views_per_sub&#39;]) # plt.show() # plt.subplot(2,3,1) # if use subplot #placed a log transformation on number of views to show a more normal distribution #There are some extreme outliers in views, so we will drop these few outliers to not inflate the RMSE of the predicted views np.power(df_model[&#39;viewCount&#39;].astype(float)[(stats.zscore(np.power(df_all[&#39;viewCount&#39;].astype(float),1/5.75)) &lt; 2.5)],1/5.75).hist(bins=20) plt.show() # print(sum(df_model[&#39;viewCount&#39;].astype(float)[(stats.zscore(np.power(df_all[&#39;viewCount&#39;].astype(float),1/5.75)) &lt; 3)])) df_model2 = df_model[np.abs(stats.zscore(np.power(df_model[&#39;viewCount&#39;].astype(float),1/5.75))) &lt; 2.5] print(shape(df_model2)) # df_all[&#39;duration&#39;][(np.abs(stats.zscore(df_all[&#39;duration&#39;])) &lt; 3)].hist(bins=20) # plt.title(&#39;duration&#39;) # df_all[&#39;channel_subs&#39;][(np.abs(stats.zscore(df_all[&#39;channel_subs&#39;].astype(float))) &lt; 3)].hist(bins=20) # plt.title(&#39;channel_subs&#39;) # plt.show() # df_all.dtypes ##For the CategoryIDs field these are what the values correspond to # ID Category name # 1 Film &amp; Animation # 2 Autos &amp; Vehicles # 10 Music # 15 Pets &amp; Animals # 17 Sports # 19 Travel &amp; Events # 20 Gaming # 22 People &amp; Blogs # 23 Comedy # 24 Entertainment # 25 News &amp; Politics # 26 Howto &amp; Style # 27 Education # 28 Science &amp; Technology # 29 Nonprofits &amp; Activism ##For the upload_day field 0=Monday and 6=Sunday . Index([&#39;index&#39;, &#39;video_id&#39;, &#39;title&#39;, &#39;title_len&#39;, &#39;words&#39;, &#39;upper_pct&#39;, &#39;upload_date&#39;, &#39;upload_time&#39;, &#39;upload_day&#39;, &#39;upload_time_of_day&#39;, &#39;viewCount&#39;, &#39;likeCount&#39;, &#39;dislikeCount&#39;, &#39;favoriteCount&#39;, &#39;commentCount&#39;, &#39;duration&#39;, &#39;definition&#39;, &#39;caption&#39;, &#39;licensedContent&#39;, &#39;thumbnail_url&#39;, &#39;thumbnail_w&#39;, &#39;thumbnail_h&#39;, &#39;tags&#39;, &#39;num_tags&#39;, &#39;categoryId&#39;, &#39;liveBroadcastContent&#39;, &#39;channel&#39;, &#39;channel_subs&#39;, &#39;channel_views&#39;, &#39;channel_videos&#39;, &#39;desc&#39;, &#39;labels&#39;, &#39;faces&#39;, &#39;texts&#39;, &#39;adult&#39;, &#39;medical&#39;, &#39;racy&#39;, &#39;spoof&#39;, &#39;violence&#39;, &#39;all_text&#39;, &#39;predict_lang&#39;, &#39;new_lang&#39;, &#39;labels_words&#39;, &#39;flag_emoji_count&#39;, &#39;emoji_count&#39;, &#39;labels_words_spanish&#39;, &#39;labels_word_count&#39;, &#39;thumb_words&#39;, &#39;thumb_word_count&#39;, &#39;title_in_desc&#39;, &#39;thumb_words_in_title&#39;, &#39;thumb_words_in_tags&#39;, &#39;label_words_in_title&#39;, &#39;label_words_in_tags&#39;, &#39;faces_surprised&#39;, &#39;faces_angry&#39;, &#39;faces_happy&#39;, &#39;faces_other&#39;, &#39;views_like_ratio&#39;, &#39;comment_views_ratio&#39;, &#39;views_favorite_ratio&#39;, &#39;like_percent&#39;, &#39;days_since_upload&#39;, &#39;views_per_day&#39;, &#39;views_per_day_per_sub&#39;, &#39;views_per_sub&#39;], dtype=&#39;object&#39;) (1932, 47) . (1916, 47) . import seaborn as sns ######Prepare the data for random Forest Model #List of categorical explanatory variables cat_x_vars=[&#39;upload_day&#39;, &#39;upload_time_of_day&#39;, &#39;definition&#39;, &#39;caption&#39;,&#39;thumbnail_h&#39;, &#39;categoryId&#39;, &#39;title_in_desc&#39;, &#39;adult&#39;, &#39;medical&#39;, &#39;racy&#39;, &#39;spoof&#39;, &#39;violence&#39;,&#39;new_lang&#39;] cat_features = pd.get_dummies(df_model2[cat_x_vars]) # , &#39;licensedContent&#39;, &#39;liveBroadcastContent&#39; #List of numeric explanatory variables num_x_vars=[&#39;title_len&#39;, &#39;words&#39;, &#39;upper_pct&#39;, &#39;duration&#39;, &#39;num_tags&#39;, &#39;flag_emoji_count&#39;,&#39;emoji_count&#39;, &#39;thumb_word_count&#39;, &#39;thumb_words_in_title&#39;, &#39;thumb_words_in_tags&#39;, &#39;label_words_in_title&#39;, &#39;label_words_in_tags&#39;,&#39;faces_surprised&#39;,&#39;faces_happy&#39;,&#39;faces_angry&#39;,&#39;faces_other&#39;,&#39;days_since_upload&#39;,&#39;channel_subs&#39;,] #, &#39;days_since_upload&#39;,&#39;channel_subs&#39;,&#39;channel_views&#39;, &#39;channel_videos&#39;,&#39;comment_views_ratio&#39;, &#39;views_favorite_ratio&#39;, &#39;like_percent&#39;,&#39;views_like_ratio&#39;, num_features=df_model2[num_x_vars] for var in num_x_vars: num_features[var] = num_features[var].astype(float) print(num_features.describe()) plt.figure(figsize=(20,20)) sns.heatmap(num_features.corr(),annot=True,cmap=&quot;RdYlGn&quot;,annot_kws={&quot;size&quot;:15}) #independent variable pred_features =df_model2[&#39;viewCount&#39;].astype(float) # &#39;views_per_day&#39;, # &#39;views_per_day_per_sub&#39;, &#39;views_per_sub&#39;, #print(df_all[df_all.columns[3]].value_counts(normalize=True).sort_index()) . A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . title_len words upper_pct duration num_tags count 1916.000000 1916.000000 1916.000000 1916.000000 1916.000000 mean 61.735386 9.791754 0.404112 12.052775 24.078810 std 23.022056 3.709413 0.341441 9.966107 8.674656 min 6.000000 1.000000 0.000000 0.133333 1.000000 25% 43.000000 7.000000 0.000000 6.345833 19.000000 50% 57.000000 9.000000 0.375000 11.358333 25.000000 75% 84.000000 13.000000 0.692308 15.783333 31.000000 max 100.000000 20.000000 1.000000 244.650000 45.000000 flag_emoji_count emoji_count thumb_word_count thumb_words_in_title count 1916.000000 1916.000000 1916.000000 1916.000000 mean 0.320981 0.049582 4.686848 2.184760 std 0.466975 0.217137 6.284305 2.381894 min 0.000000 0.000000 0.000000 0.000000 25% 0.000000 0.000000 1.000000 0.000000 50% 0.000000 0.000000 4.000000 1.000000 75% 1.000000 0.000000 6.000000 4.000000 max 1.000000 1.000000 116.000000 14.000000 thumb_words_in_tags label_words_in_title label_words_in_tags count 1916.000000 1916.000000 1916.000000 mean 2.513570 0.097077 0.308455 std 2.671252 0.338900 0.709761 min 0.000000 0.000000 0.000000 25% 0.000000 0.000000 0.000000 50% 2.000000 0.000000 0.000000 75% 4.000000 0.000000 0.000000 max 22.000000 3.000000 6.000000 faces_surprised faces_happy faces_angry faces_other count 1916.000000 1916.000000 1916.000000 1916.000000 mean 0.324113 0.982777 0.042276 0.775052 std 0.655854 1.121923 0.249883 1.406075 min 0.000000 0.000000 0.000000 0.000000 25% 0.000000 0.000000 0.000000 0.000000 50% 0.000000 1.000000 0.000000 0.000000 75% 0.000000 2.000000 0.000000 1.000000 max 5.000000 12.000000 3.000000 38.000000 days_since_upload channel_subs count 1916.000000 1.916000e+03 mean 799.972338 1.052149e+06 std 747.503011 1.261664e+06 min 1.000000 2.260000e+02 25% 242.750000 2.960000e+05 50% 573.500000 6.460000e+05 75% 1121.500000 1.030000e+06 max 3297.000000 4.280000e+06 . from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV import shap import xgboost as xgb import matplotlib.pylab as pl # data = pd.concat([cat_features,num_features],axis=1) X_data=pd.concat([cat_features,num_features],axis=1) y_data=pred_features def split_data_train_model(labels, data): # 20% examples in test data train, test, train_labels, test_labels = train_test_split(X_data, y_data, test_size=0.2) # training data fit return test, test_labels, regressor # x_test, x_test_labels, regressor = split_data_train_model(y_data, X_data) X_train, X_test, y_train, y_test = train_test_split(X_data,y_data,test_size=0.2) # regressor = RandomForestRegressor(n_estimators=250) # regressor.fit(X_train, y_train) # predictions = regressor.predict(X_train) # elements = np.power(y_train - predictions, 2) # print(float(np.sqrt(np.sum(elements) / len(predictions)))) # reg_xgb = xgb.XGBRegressor() # objective =&#39;reg:linear&#39; # reg_xgb.fit(X_train,y_train,verbose=True,early_stopping_rounds=10,eval_set=[(X_test,y_test)]) ###optimize parameters for XGBoost #round1 # param_grid={ # &#39;max_depth&#39;:[4,5,6,7,8], # &#39;learning_rate&#39;: [.2,.3,.4], # &quot;min_child_weight&quot;: [ .5,1, 1.5], # &#39;gamma&#39;: [0,.25,.75,1], # &#39;reg_lambda&#39;: [0,1,5,10], # } #round2 # param_grid={ # &#39;max_depth&#39;:[8,10,12], # &#39;learning_rate&#39;: [.1,.15,.2,.25], # &quot;min_child_weight&quot;: [ .1,.25,.5], # &#39;gamma&#39;: [.25], # &#39;reg_lambda&#39;: [0,.1], # } # #round3 # param_grid={ # &#39;max_depth&#39;:[12,15], # &#39;learning_rate&#39;: [.05,.1,.15], # &quot;min_child_weight&quot;: [ .05,.075,.1], # &#39;gamma&#39;: [.25], # &#39;reg_lambda&#39;: [.1], # } #round4 # param_grid={ # &#39;max_depth&#39;:[15,17,20], # &#39;learning_rate&#39;: [.1], # &quot;min_child_weight&quot;: [ .01,.025,.05], # &#39;gamma&#39;: [.25], # &#39;reg_lambda&#39;: [.1], # } #BEST! # param_grid={ # &#39;max_depth&#39;:[15], # &#39;learning_rate&#39;: [.1], # &quot;min_child_weight&quot;: [ 0], # &#39;gamma&#39;: [.25], # &#39;reg_lambda&#39;: [.1], # } # optimal_params = GridSearchCV( # estimator=xgb.XGBRegressor(), # param_grid=param_grid, # verbose=0, # n_jobs=10, # cv=5, # scoring=&#39;neg_mean_squared_error&#39; # ) # optimal_params.fit(X_train,y_train,verbose=True,early_stopping_rounds=10,eval_set=[(X_test,y_test)]) # print(optimal_params.best_params_) reg_xgb = xgb.XGBRegressor(n_estimators=100, max_depth=15,learning_rate=.1,min_child_weight=0 ,gamma=.25,reg_lambda=.1) reg_xgb.fit(X_train,y_train,verbose=False,early_stopping_rounds=10,eval_set=[(X_test,y_test)]) # predictions = reg_xgb.predict(X_test, ntree_limit = 0) # params = {&#39;objective&#39;:&#39;reg:squarederror&#39;} # dmatrix = xgb.DMatrix(data = X_train, label = y_train) # reg_xbg = xgb.train(dtrain=dmatrix, params=params) explainer = shap.TreeExplainer(reg_xgb) shap_values = explainer.shap_values(X_train) shap.summary_plot(shap_values,X_train) shap_values for i in X_train.columns: shap.dependence_plot(i,shap_values,X_train,show=False) x=X_train[i] y=[item[X_train.columns.get_loc(i)] for item in shap_values] m, b = pl.polyfit( x, y , 1 ) pl.plot(x, m * x+b , &#39;-k&#39; , linewidth=2,label=&#39;y=&#39;+str(int(round(m,-2)))+&#39;x+&#39;+str(int(round(b,-2)))) pl.legend(loc=&#39;upper left&#39;) pl.xlim(min(x.quantile(q=.01)*.75,-.2),max(x.quantile(q=.99)*1.2,1)) pl.ylim(min(np.quantile(y,.01)*.75,-.2),max(np.quantile(y,.99)*1.2,1)) pl.show() . ntree_limit is deprecated, use `iteration_range` or model slicing instead. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . Passing parameters norm and vmin/vmax simultaneously is deprecated since 3.3 and will become an error two minor releases later. Please pass vmin/vmax directly to the norm when creating it. . shap.initjs() shap.plots.bar(shap_values,X_train) tot_shap=[0]*76 tot_shap_abs =[0]*76 for i in shap_values.values: tot_shap = tot_shap + i tot_shap_abs = tot_shap_abs + np.abs(i) df2 = pd.DataFrame([x_data.columns,(tot_shap/384),(tot_shap_abs/384)]).transpose().sort_values([2],ascending=False) df2.columns = [&#39;Var_name&#39;,&#39;Mean SHAP&#39;,&#39;Mean SHAP Abs&#39;] print(df2) # print(shap.plots.bar) shap.plots.beeswarm(shap_values,color=plt.get_cmap(&quot;cool&quot;),max_display=20) shap.plots.scatter(shap_values[:,&quot;days_since_upload&quot;]) shap.plots.scatter(shap_values[:,&quot;duration&quot;]) shap.plots.scatter(shap_values[:,&quot;faces_other&quot;]) shap.plots.scatter(shap_values[:,&quot;spoof_LIKELY&quot;]) shap.plots.scatter(shap_values[:,&quot;title_len&quot;]) shap.plots.scatter(shap_values[:,&quot;upper_pct&quot;]) # shap.summary_plot(shap_values.values, x_data, plot_type=&#39;bar&#39;) . AssertionError Traceback (most recent call last) &lt;ipython-input-233-46ade666ae0e&gt; in &lt;module&gt; 1 shap.initjs() 2 -&gt; 3 shap.plots.bar(shap_values,X_train) 4 5 tot_shap=[0]*76 ~ Desktop data projects env_yt_api lib site-packages shap plots _bar.py in bar(shap_values, max_display, order, clustering, clustering_cutoff, merge_cohorts, show_data, show) 49 cohorts = shap_values.cohorts 50 else: &gt; 51 assert isinstance(shap_values, dict), &#34;You must pass an Explanation object, Cohorts object, or dictionary to bar plot!&#34; 52 53 # unpack our list of Explanation objects we need to plot AssertionError: You must pass an Explanation object, Cohorts object, or dictionary to bar plot! . from scipy.stats.stats import pearsonr from sklearn.inspection import partial_dependence from sklearn.inspection import plot_partial_dependence predictions = reg_xgb.predict(X_test, ntree_limit = 0) elements = np.power(y_test - predictions, 2) elements2 = np.power(y_test - y_test.mean(), 2) print(float(np.sqrt(np.sum(elements) / len(predictions)))) #r-Squared print(1-np.sum(elements)/np.sum(elements2)) print(pearsonr(predictions, y_test)) plt.scatter(predictions,y_test) plt.show() # # predictions = regressor.predict(x_test) # feature_names = [f&#39;feature {i}&#39; for i in range(x_data.shape[1])] # features_importance = regressor.feature_importances_ # # print(&quot;Feature ranking:&quot;) # importance= pd.DataFrame({&quot;Feature&quot;:x_data.columns,&quot;Importance&quot;:features_importance}) # # for i in range(len(features_importance)): # # importance[&#39;Feature&#39;][i] = x_data.columns[i] # # importance[&#39;Importance&#39;][i] = features_importance[i] # importance.sort_values(by=&#39;Importance&#39; ,ascending=False).head(60) # regressor.predict, x_data # print(&#39;Computing partial dependence plots...&#39;) # display1 = plot_partial_dependence( # regressor.fit(x_data, y_data), X=x_data, feature_names=x_data.columns,features=[58,59,60,61,62,63], kind=&quot;average&quot;, subsample=10, # n_jobs=3, grid_resolution=50, random_state=0 # ) # display2 = plot_partial_dependence( # regressor.fit(x_data, y_data), X=x_data, feature_names=x_data.columns,features=[64,65,66,67,68,69], kind=&quot;average&quot;, subsample=10, # n_jobs=3, grid_resolution=50, random_state=0 # ) # display3 = plot_partial_dependence( # regressor.fit(x_data, y_data), X=x_data, feature_names=x_data.columns,features=[70,71,72,73,74,75], kind=&quot;average&quot;, subsample=10, # n_jobs=3, grid_resolution=50, random_state=0 # ) # display1.figure_.subplots_adjust(hspace=0.8) # display2.figure_.subplots_adjust(hspace=0.8) # display3.figure_.subplots_adjust(hspace=0.8) # display.figure_.suptitle( # &#39;Partial dependence of views with Random Forest Regressor&#39; # ) #pd.concat([pd.DataFrame(predictions),x_test],axis=1,ignore_index=True) . 410837.6469265359 0.12252179630996374 (0.4381658337651223, 1.9099304713707704e-19) . dir(pl) . [&#39;ALLOW_THREADS&#39;, &#39;Annotation&#39;, &#39;Arrow&#39;, &#39;Artist&#39;, &#39;AutoLocator&#39;, &#39;Axes&#39;, &#39;AxisError&#39;, &#39;BUFSIZE&#39;, &#39;BitGenerator&#39;, &#39;Button&#39;, &#39;CLIP&#39;, &#39;Circle&#39;, &#39;ComplexWarning&#39;, &#39;DAILY&#39;, &#39;DataSource&#39;, &#39;DateFormatter&#39;, &#39;DateLocator&#39;, &#39;DayLocator&#39;, &#39;ERR_CALL&#39;, &#39;ERR_DEFAULT&#39;, &#39;ERR_IGNORE&#39;, &#39;ERR_LOG&#39;, &#39;ERR_PRINT&#39;, &#39;ERR_RAISE&#39;, &#39;ERR_WARN&#39;, &#39;FLOATING_POINT_SUPPORT&#39;, &#39;FPE_DIVIDEBYZERO&#39;, &#39;FPE_INVALID&#39;, &#39;FPE_OVERFLOW&#39;, &#39;FPE_UNDERFLOW&#39;, &#39;FR&#39;, &#39;False_&#39;, &#39;Figure&#39;, &#39;FigureCanvasBase&#39;, &#39;FixedFormatter&#39;, &#39;FixedLocator&#39;, &#39;FormatStrFormatter&#39;, &#39;Formatter&#39;, &#39;FuncFormatter&#39;, &#39;Generator&#39;, &#39;GridSpec&#39;, &#39;HOURLY&#39;, &#39;HourLocator&#39;, &#39;IndexDateFormatter&#39;, &#39;IndexLocator&#39;, &#39;Inf&#39;, &#39;Infinity&#39;, &#39;LinAlgError&#39;, &#39;Line2D&#39;, &#39;LinearLocator&#39;, &#39;Locator&#39;, &#39;LogFormatter&#39;, &#39;LogFormatterExponent&#39;, &#39;LogFormatterMathtext&#39;, &#39;LogLocator&#39;, &#39;MAXDIMS&#39;, &#39;MAY_SHARE_BOUNDS&#39;, &#39;MAY_SHARE_EXACT&#39;, &#39;MINUTELY&#39;, &#39;MO&#39;, &#39;MONTHLY&#39;, &#39;MT19937&#39;, &#39;MachAr&#39;, &#39;MaxNLocator&#39;, &#39;MinuteLocator&#39;, &#39;ModuleDeprecationWarning&#39;, &#39;MonthLocator&#39;, &#39;MouseButton&#39;, &#39;MultipleLocator&#39;, &#39;NAN&#39;, &#39;NINF&#39;, &#39;NZERO&#39;, &#39;NaN&#39;, &#39;Normalize&#39;, &#39;NullFormatter&#39;, &#39;NullLocator&#39;, &#39;Number&#39;, &#39;PCG64&#39;, &#39;PINF&#39;, &#39;PZERO&#39;, &#39;Philox&#39;, &#39;PolarAxes&#39;, &#39;Polygon&#39;, &#39;RAISE&#39;, &#39;RRuleLocator&#39;, &#39;RandomState&#39;, &#39;RankWarning&#39;, &#39;Rectangle&#39;, &#39;SA&#39;, &#39;SECONDLY&#39;, &#39;SFC64&#39;, &#39;SHIFT_DIVIDEBYZERO&#39;, &#39;SHIFT_INVALID&#39;, &#39;SHIFT_OVERFLOW&#39;, &#39;SHIFT_UNDERFLOW&#39;, &#39;SU&#39;, &#39;ScalarFormatter&#39;, &#39;ScalarType&#39;, &#39;SecondLocator&#39;, &#39;SeedSequence&#39;, &#39;Slider&#39;, &#39;Subplot&#39;, &#39;SubplotTool&#39;, &#39;TH&#39;, &#39;TU&#39;, &#39;Text&#39;, &#39;TickHelper&#39;, &#39;TooHardError&#39;, &#39;True_&#39;, &#39;UFUNC_BUFSIZE_DEFAULT&#39;, &#39;UFUNC_PYVALS_NAME&#39;, &#39;VisibleDeprecationWarning&#39;, &#39;WE&#39;, &#39;WEEKLY&#39;, &#39;WRAP&#39;, &#39;WeekdayLocator&#39;, &#39;Widget&#39;, &#39;YEARLY&#39;, &#39;YearLocator&#39;, &#39;_UFUNC_API&#39;, &#39;__builtins__&#39;, &#39;__cached__&#39;, &#39;__doc__&#39;, &#39;__file__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__spec__&#39;, &#39;__version__&#39;, &#39;_add_newdoc_ufunc&#39;, &#39;absolute&#39;, &#39;acorr&#39;, &#39;add&#39;, &#39;add_docstring&#39;, &#39;add_newdoc&#39;, &#39;add_newdoc_ufunc&#39;, &#39;alen&#39;, &#39;all&#39;, &#39;allclose&#39;, &#39;alltrue&#39;, &#39;amax&#39;, &#39;amin&#39;, &#39;angle&#39;, &#39;angle_spectrum&#39;, &#39;annotate&#39;, &#39;any&#39;, &#39;append&#39;, &#39;apply_along_axis&#39;, &#39;apply_over_axes&#39;, &#39;arange&#39;, &#39;arccos&#39;, &#39;arccosh&#39;, &#39;arcsin&#39;, &#39;arcsinh&#39;, &#39;arctan&#39;, &#39;arctan2&#39;, &#39;arctanh&#39;, &#39;argmax&#39;, &#39;argmin&#39;, &#39;argpartition&#39;, &#39;argsort&#39;, &#39;argwhere&#39;, &#39;around&#39;, &#39;array&#39;, &#39;array2string&#39;, &#39;array_equal&#39;, &#39;array_equiv&#39;, &#39;array_repr&#39;, &#39;array_split&#39;, &#39;array_str&#39;, &#39;arrow&#39;, &#39;asanyarray&#39;, &#39;asarray&#39;, &#39;asarray_chkfinite&#39;, &#39;ascontiguousarray&#39;, &#39;asfarray&#39;, &#39;asfortranarray&#39;, &#39;asmatrix&#39;, &#39;asscalar&#39;, &#39;atleast_1d&#39;, &#39;atleast_2d&#39;, &#39;atleast_3d&#39;, &#39;autoscale&#39;, &#39;autumn&#39;, &#39;average&#39;, &#39;axes&#39;, &#39;axhline&#39;, &#39;axhspan&#39;, &#39;axis&#39;, &#39;axline&#39;, &#39;axvline&#39;, &#39;axvspan&#39;, &#39;bar&#39;, &#39;barbs&#39;, &#39;barh&#39;, &#39;bartlett&#39;, &#39;base_repr&#39;, &#39;beta&#39;, &#39;binary_repr&#39;, &#39;bincount&#39;, &#39;binomial&#39;, &#39;bitwise_and&#39;, &#39;bitwise_not&#39;, &#39;bitwise_or&#39;, &#39;bitwise_xor&#39;, &#39;blackman&#39;, &#39;block&#39;, &#39;bmat&#39;, &#39;bone&#39;, &#39;bool8&#39;, &#39;bool_&#39;, &#39;box&#39;, &#39;boxplot&#39;, &#39;broadcast&#39;, &#39;broadcast_arrays&#39;, &#39;broadcast_to&#39;, &#39;broken_barh&#39;, &#39;busday_count&#39;, &#39;busday_offset&#39;, &#39;busdaycalendar&#39;, &#39;byte&#39;, &#39;byte_bounds&#39;, &#39;bytes&#39;, &#39;bytes0&#39;, &#39;bytes_&#39;, &#39;c_&#39;, &#39;can_cast&#39;, &#39;cast&#39;, &#39;cbook&#39;, &#39;cbrt&#39;, &#39;cdouble&#39;, &#39;ceil&#39;, &#39;cfloat&#39;, &#39;char&#39;, &#39;character&#39;, &#39;chararray&#39;, &#39;chisquare&#39;, &#39;choice&#39;, &#39;cholesky&#39;, &#39;choose&#39;, &#39;cla&#39;, &#39;clabel&#39;, &#39;clf&#39;, &#39;clim&#39;, &#39;clip&#39;, &#39;clongdouble&#39;, &#39;clongfloat&#39;, &#39;close&#39;, &#39;cm&#39;, &#39;cohere&#39;, &#39;colorbar&#39;, &#39;colormaps&#39;, &#39;column_stack&#39;, &#39;common_type&#39;, &#39;compare_chararrays&#39;, &#39;complex128&#39;, &#39;complex64&#39;, &#39;complex_&#39;, &#39;complexfloating&#39;, &#39;compress&#39;, &#39;concatenate&#39;, &#39;cond&#39;, &#39;conj&#39;, &#39;conjugate&#39;, &#39;connect&#39;, &#39;contour&#39;, &#39;contourf&#39;, &#39;convolve&#39;, &#39;cool&#39;, &#39;copper&#39;, &#39;copy&#39;, &#39;copysign&#39;, &#39;copyto&#39;, &#39;corrcoef&#39;, &#39;correlate&#39;, &#39;cos&#39;, &#39;cosh&#39;, &#39;count_nonzero&#39;, &#39;cov&#39;, &#39;cross&#39;, &#39;csd&#39;, &#39;csingle&#39;, &#39;ctypeslib&#39;, &#39;cumprod&#39;, &#39;cumproduct&#39;, &#39;cumsum&#39;, &#39;cycler&#39;, &#39;date2num&#39;, &#39;datestr2num&#39;, &#39;datetime&#39;, &#39;datetime64&#39;, &#39;datetime_as_string&#39;, &#39;datetime_data&#39;, &#39;default_rng&#39;, &#39;deg2rad&#39;, &#39;degrees&#39;, &#39;delaxes&#39;, &#39;delete&#39;, &#39;deprecate&#39;, &#39;deprecate_with_doc&#39;, &#39;det&#39;, &#39;detrend&#39;, &#39;detrend_linear&#39;, &#39;detrend_mean&#39;, &#39;detrend_none&#39;, &#39;diag&#39;, &#39;diag_indices&#39;, &#39;diag_indices_from&#39;, &#39;diagflat&#39;, &#39;diagonal&#39;, &#39;diff&#39;, &#39;digitize&#39;, &#39;dirichlet&#39;, &#39;disconnect&#39;, &#39;disp&#39;, &#39;divide&#39;, &#39;divmod&#39;, &#39;docstring&#39;, &#39;dot&#39;, &#39;double&#39;, &#39;drange&#39;, &#39;draw&#39;, &#39;draw_all&#39;, &#39;draw_if_interactive&#39;, &#39;dsplit&#39;, &#39;dstack&#39;, &#39;dtype&#39;, &#39;e&#39;, &#39;ediff1d&#39;, &#39;eig&#39;, &#39;eigh&#39;, &#39;eigvals&#39;, &#39;eigvalsh&#39;, &#39;einsum&#39;, &#39;einsum_path&#39;, &#39;emath&#39;, &#39;empty&#39;, &#39;empty_like&#39;, &#39;epoch2num&#39;, &#39;equal&#39;, &#39;errorbar&#39;, &#39;errstate&#39;, &#39;euler_gamma&#39;, &#39;eventplot&#39;, &#39;exp&#39;, &#39;exp2&#39;, &#39;expand_dims&#39;, &#39;expm1&#39;, &#39;exponential&#39;, &#39;extract&#39;, &#39;eye&#39;, &#39;f&#39;, &#39;fabs&#39;, &#39;fastCopyAndTranspose&#39;, &#39;fft&#39;, &#39;fft2&#39;, &#39;fftfreq&#39;, &#39;fftn&#39;, &#39;fftshift&#39;, &#39;figaspect&#39;, &#39;figimage&#39;, &#39;figlegend&#39;, &#39;fignum_exists&#39;, &#39;figtext&#39;, &#39;figure&#39;, &#39;fill&#39;, &#39;fill_between&#39;, &#39;fill_betweenx&#39;, &#39;fill_diagonal&#39;, &#39;find_common_type&#39;, &#39;findobj&#39;, &#39;finfo&#39;, &#39;fix&#39;, &#39;flag&#39;, &#39;flatiter&#39;, &#39;flatnonzero&#39;, &#39;flatten&#39;, &#39;flexible&#39;, &#39;flip&#39;, &#39;fliplr&#39;, &#39;flipud&#39;, &#39;float16&#39;, &#39;float32&#39;, &#39;float64&#39;, &#39;float_&#39;, &#39;float_power&#39;, &#39;floating&#39;, &#39;floor&#39;, &#39;floor_divide&#39;, &#39;fmax&#39;, &#39;fmin&#39;, &#39;fmod&#39;, &#39;format_float_positional&#39;, &#39;format_float_scientific&#39;, &#39;format_parser&#39;, &#39;frexp&#39;, &#39;frombuffer&#39;, &#39;fromfile&#39;, &#39;fromfunction&#39;, &#39;fromiter&#39;, &#39;frompyfunc&#39;, &#39;fromregex&#39;, &#39;fromstring&#39;, &#39;full&#39;, &#39;full_like&#39;, &#39;functools&#39;, &#39;fv&#39;, &#39;gamma&#39;, &#39;gca&#39;, &#39;gcd&#39;, &#39;gcf&#39;, &#39;gci&#39;, &#39;generic&#39;, &#39;genfromtxt&#39;, &#39;geometric&#39;, &#39;geomspace&#39;, &#39;get&#39;, &#39;get_array_wrap&#39;, &#39;get_backend&#39;, &#39;get_cmap&#39;, &#39;get_current_fig_manager&#39;, &#39;get_figlabels&#39;, &#39;get_fignums&#39;, &#39;get_include&#39;, &#39;get_plot_commands&#39;, &#39;get_printoptions&#39;, &#39;get_scale_names&#39;, &#39;get_state&#39;, &#39;getbufsize&#39;, &#39;geterr&#39;, &#39;geterrcall&#39;, &#39;geterrobj&#39;, &#39;getp&#39;, &#39;ginput&#39;, &#39;gradient&#39;, &#39;gray&#39;, &#39;greater&#39;, &#39;greater_equal&#39;, &#39;grid&#39;, &#39;gumbel&#39;, &#39;half&#39;, &#39;hamming&#39;, &#39;hanning&#39;, &#39;heaviside&#39;, &#39;helper&#39;, &#39;hexbin&#39;, &#39;hfft&#39;, &#39;hist&#39;, &#39;hist2d&#39;, &#39;histogram&#39;, &#39;histogram2d&#39;, &#39;histogram_bin_edges&#39;, &#39;histogramdd&#39;, &#39;hlines&#39;, &#39;hot&#39;, &#39;hsplit&#39;, &#39;hstack&#39;, &#39;hsv&#39;, &#39;hypergeometric&#39;, &#39;hypot&#39;, &#39;i0&#39;, &#39;identity&#39;, &#39;ifft&#39;, &#39;ifft2&#39;, &#39;ifftn&#39;, &#39;ifftshift&#39;, &#39;ihfft&#39;, &#39;iinfo&#39;, &#39;imag&#39;, &#39;importlib&#39;, &#39;imread&#39;, &#39;imsave&#39;, &#39;imshow&#39;, &#39;in1d&#39;, &#39;index_exp&#39;, &#39;indices&#39;, &#39;inexact&#39;, &#39;inf&#39;, &#39;inferno&#39;, &#39;info&#39;, &#39;infty&#39;, &#39;inner&#39;, &#39;insert&#39;, &#39;inspect&#39;, &#39;install_repl_displayhook&#39;, &#39;int0&#39;, &#39;int16&#39;, &#39;int32&#39;, &#39;int64&#39;, &#39;int8&#39;, &#39;int_&#39;, &#39;intc&#39;, &#39;integer&#39;, &#39;interactive&#39;, &#39;interp&#39;, &#39;intersect1d&#39;, &#39;intp&#39;, &#39;inv&#39;, &#39;invert&#39;, &#39;ioff&#39;, &#39;ion&#39;, &#39;ipmt&#39;, &#39;irfft&#39;, &#39;irfft2&#39;, &#39;irfftn&#39;, &#39;irr&#39;, &#39;is_busday&#39;, &#39;isclose&#39;, &#39;iscomplex&#39;, &#39;iscomplexobj&#39;, &#39;isfinite&#39;, &#39;isfortran&#39;, &#39;isin&#39;, &#39;isinf&#39;, &#39;isinteractive&#39;, &#39;isnan&#39;, &#39;isnat&#39;, &#39;isneginf&#39;, &#39;isposinf&#39;, &#39;isreal&#39;, &#39;isrealobj&#39;, &#39;isscalar&#39;, &#39;issctype&#39;, &#39;issubclass_&#39;, &#39;issubdtype&#39;, &#39;issubsctype&#39;, &#39;iterable&#39;, &#39;ix_&#39;, &#39;jet&#39;, &#39;kaiser&#39;, &#39;kron&#39;, &#39;lapack_lite&#39;, &#39;laplace&#39;, &#39;lcm&#39;, &#39;ldexp&#39;, &#39;left_shift&#39;, &#39;legend&#39;, &#39;less&#39;, &#39;less_equal&#39;, &#39;lexsort&#39;, &#39;linalg&#39;, &#39;linspace&#39;, &#39;little_endian&#39;, &#39;load&#39;, &#39;loads&#39;, &#39;loadtxt&#39;, &#39;locator_params&#39;, &#39;log&#39;, &#39;log10&#39;, &#39;log1p&#39;, &#39;log2&#39;, &#39;logaddexp&#39;, &#39;logaddexp2&#39;, &#39;logging&#39;, &#39;logical_and&#39;, &#39;logical_not&#39;, &#39;logical_or&#39;, &#39;logical_xor&#39;, &#39;logistic&#39;, &#39;loglog&#39;, &#39;lognormal&#39;, &#39;logseries&#39;, &#39;logspace&#39;, &#39;longcomplex&#39;, &#39;longdouble&#39;, &#39;longfloat&#39;, &#39;longlong&#39;, &#39;lookfor&#39;, &#39;lstsq&#39;, &#39;ma&#39;, &#39;mafromtxt&#39;, &#39;magma&#39;, &#39;magnitude_spectrum&#39;, &#39;margins&#39;, &#39;mask_indices&#39;, &#39;mat&#39;, &#39;math&#39;, &#39;matmul&#39;, &#39;matplotlib&#39;, &#39;matrix&#39;, &#39;matrix_power&#39;, &#39;matrix_rank&#39;, &#39;matshow&#39;, &#39;maximum&#39;, &#39;maximum_sctype&#39;, &#39;may_share_memory&#39;, &#39;mean&#39;, &#39;median&#39;, &#39;memmap&#39;, &#39;meshgrid&#39;, &#39;mgrid&#39;, &#39;min_scalar_type&#39;, &#39;minimum&#39;, &#39;minorticks_off&#39;, &#39;minorticks_on&#39;, &#39;mintypecode&#39;, &#39;mirr&#39;, &#39;mlab&#39;, &#39;mod&#39;, &#39;modf&#39;, &#39;moveaxis&#39;, &#39;mpl&#39;, &#39;msort&#39;, &#39;multi_dot&#39;, &#39;multinomial&#39;, &#39;multiply&#39;, &#39;multivariate_normal&#39;, &#39;mx2num&#39;, &#39;nan&#39;, &#39;nan_to_num&#39;, &#39;nanargmax&#39;, &#39;nanargmin&#39;, &#39;nancumprod&#39;, &#39;nancumsum&#39;, &#39;nanmax&#39;, &#39;nanmean&#39;, &#39;nanmedian&#39;, &#39;nanmin&#39;, &#39;nanpercentile&#39;, &#39;nanprod&#39;, &#39;nanquantile&#39;, &#39;nanstd&#39;, &#39;nansum&#39;, &#39;nanvar&#39;, &#39;nbytes&#39;, &#39;ndarray&#39;, &#39;ndenumerate&#39;, &#39;ndfromtxt&#39;, &#39;ndim&#39;, &#39;ndindex&#39;, &#39;nditer&#39;, &#39;negative&#39;, &#39;negative_binomial&#39;, &#39;nested_iters&#39;, &#39;new_figure_manager&#39;, &#39;newaxis&#39;, &#39;nextafter&#39;, &#39;nipy_spectral&#39;, &#39;noncentral_chisquare&#39;, &#39;noncentral_f&#39;, &#39;nonzero&#39;, &#39;norm&#39;, &#39;normal&#39;, &#39;not_equal&#39;, &#39;np&#39;, &#39;nper&#39;, &#39;npv&#39;, &#39;num2date&#39;, &#39;num2epoch&#39;, &#39;number&#39;, &#39;obj2sctype&#39;, &#39;object0&#39;, &#39;object_&#39;, &#39;ogrid&#39;, &#39;ones&#39;, &#39;ones_like&#39;, &#39;outer&#39;, &#39;packbits&#39;, &#39;pad&#39;, &#39;pareto&#39;, &#39;partition&#39;, &#39;pause&#39;, &#39;pcolor&#39;, &#39;pcolormesh&#39;, &#39;percentile&#39;, &#39;permutation&#39;, &#39;phase_spectrum&#39;, &#39;pi&#39;, &#39;pie&#39;, &#39;piecewise&#39;, &#39;pink&#39;, &#39;pinv&#39;, &#39;place&#39;, &#39;plasma&#39;, &#39;plot&#39;, &#39;plot_date&#39;, &#39;plotting&#39;, &#39;plt&#39;, &#39;pmt&#39;, &#39;poisson&#39;, &#39;polar&#39;, &#39;poly&#39;, &#39;poly1d&#39;, &#39;polyadd&#39;, &#39;polyder&#39;, &#39;polydiv&#39;, &#39;polyfit&#39;, &#39;polyint&#39;, &#39;polymul&#39;, &#39;polysub&#39;, &#39;polyval&#39;, &#39;positive&#39;, &#39;power&#39;, &#39;ppmt&#39;, &#39;printoptions&#39;, &#39;prism&#39;, &#39;prod&#39;, &#39;product&#39;, &#39;promote_types&#39;, &#39;psd&#39;, &#39;ptp&#39;, &#39;put&#39;, &#39;put_along_axis&#39;, &#39;putmask&#39;, &#39;pv&#39;, &#39;qr&#39;, &#39;quantile&#39;, &#39;quiver&#39;, &#39;quiverkey&#39;, &#39;r_&#39;, &#39;rad2deg&#39;, &#39;radians&#39;, &#39;rand&#39;, &#39;randint&#39;, &#39;randn&#39;, &#39;random&#39;, &#39;random_integers&#39;, &#39;random_sample&#39;, &#39;ranf&#39;, &#39;rate&#39;, &#39;ravel&#39;, &#39;ravel_multi_index&#39;, &#39;rayleigh&#39;, &#39;rc&#39;, &#39;rcParams&#39;, &#39;rcParamsDefault&#39;, &#39;rcParamsOrig&#39;, &#39;rc_context&#39;, &#39;rcdefaults&#39;, &#39;rcsetup&#39;, &#39;re&#39;, &#39;real&#39;, &#39;real_if_close&#39;, &#39;rec&#39;, &#39;recarray&#39;, &#39;recfromcsv&#39;, &#39;recfromtxt&#39;, &#39;reciprocal&#39;, &#39;record&#39;, &#39;register_cmap&#39;, &#39;relativedelta&#39;, &#39;remainder&#39;, &#39;repeat&#39;, &#39;require&#39;, &#39;reshape&#39;, &#39;resize&#39;, &#39;result_type&#39;, &#39;rfft&#39;, &#39;rfft2&#39;, &#39;rfftfreq&#39;, &#39;rfftn&#39;, &#39;rgrids&#39;, &#39;right_shift&#39;, &#39;rint&#39;, &#39;roll&#39;, &#39;rollaxis&#39;, &#39;roots&#39;, &#39;rot90&#39;, &#39;round_&#39;, &#39;row_stack&#39;, &#39;rrule&#39;, &#39;s_&#39;, &#39;safe_eval&#39;, &#39;sample&#39;, &#39;save&#39;, &#39;savefig&#39;, &#39;savetxt&#39;, &#39;savez&#39;, &#39;savez_compressed&#39;, &#39;sca&#39;, &#39;scatter&#39;, &#39;sci&#39;, &#39;sctype2char&#39;, &#39;sctypeDict&#39;, &#39;sctypeNA&#39;, &#39;sctypes&#39;, &#39;searchsorted&#39;, &#39;seed&#39;, &#39;select&#39;, &#39;semilogx&#39;, &#39;semilogy&#39;, &#39;set_cmap&#39;, &#39;set_loglevel&#39;, &#39;set_numeric_ops&#39;, &#39;set_printoptions&#39;, &#39;set_state&#39;, &#39;set_string_function&#39;, &#39;setbufsize&#39;, &#39;setdiff1d&#39;, &#39;seterr&#39;, &#39;seterrcall&#39;, &#39;seterrobj&#39;, &#39;setp&#39;, &#39;setxor1d&#39;, &#39;shape&#39;, &#39;shares_memory&#39;, &#39;short&#39;, &#39;show&#39;, &#39;show_config&#39;, &#39;shuffle&#39;, &#39;sign&#39;, &#39;signbit&#39;, &#39;signedinteger&#39;, &#39;silent_list&#39;, &#39;sin&#39;, &#39;sinc&#39;, &#39;single&#39;, &#39;singlecomplex&#39;, &#39;sinh&#39;, &#39;size&#39;, &#39;slogdet&#39;, &#39;solve&#39;, &#39;sometrue&#39;, &#39;sort&#39;, &#39;sort_complex&#39;, &#39;source&#39;, &#39;spacing&#39;, &#39;specgram&#39;, &#39;split&#39;, &#39;spring&#39;, &#39;spy&#39;, &#39;sqrt&#39;, &#39;square&#39;, &#39;squeeze&#39;, &#39;stack&#39;, &#39;stackplot&#39;, &#39;standard_cauchy&#39;, &#39;standard_exponential&#39;, &#39;standard_gamma&#39;, &#39;standard_normal&#39;, &#39;standard_t&#39;, &#39;std&#39;, &#39;stem&#39;, &#39;step&#39;, &#39;str0&#39;, &#39;str_&#39;, &#39;streamplot&#39;, &#39;string_&#39;, &#39;style&#39;, &#39;subplot&#39;, &#39;subplot2grid&#39;, &#39;subplot_mosaic&#39;, &#39;subplot_tool&#39;, &#39;subplots&#39;, &#39;subplots_adjust&#39;, &#39;subtract&#39;, &#39;sum&#39;, &#39;summer&#39;, &#39;suptitle&#39;, &#39;svd&#39;, &#39;swapaxes&#39;, &#39;switch_backend&#39;, &#39;sys&#39;, &#39;table&#39;, &#39;take&#39;, &#39;take_along_axis&#39;, &#39;tan&#39;, &#39;tanh&#39;, &#39;tensordot&#39;, &#39;tensorinv&#39;, &#39;tensorsolve&#39;, &#39;test&#39;, &#39;text&#39;, &#39;thetagrids&#39;, &#39;threading&#39;, &#39;tick_params&#39;, &#39;ticklabel_format&#39;, &#39;tight_layout&#39;, &#39;tile&#39;, &#39;time&#39;, &#39;timedelta64&#39;, &#39;title&#39;, &#39;trace&#39;, &#39;tracemalloc_domain&#39;, &#39;transpose&#39;, &#39;trapz&#39;, &#39;tri&#39;, &#39;triangular&#39;, &#39;tricontour&#39;, &#39;tricontourf&#39;, &#39;tril&#39;, &#39;tril_indices&#39;, &#39;tril_indices_from&#39;, &#39;trim_zeros&#39;, &#39;tripcolor&#39;, &#39;triplot&#39;, &#39;triu&#39;, &#39;triu_indices&#39;, &#39;triu_indices_from&#39;, &#39;true_divide&#39;, &#39;trunc&#39;, &#39;twinx&#39;, &#39;twiny&#39;, &#39;typeDict&#39;, &#39;typeNA&#39;, &#39;typecodes&#39;, &#39;typename&#39;, &#39;ubyte&#39;, &#39;ufunc&#39;, &#39;uint&#39;, &#39;uint0&#39;, &#39;uint16&#39;, &#39;uint32&#39;, &#39;uint64&#39;, &#39;uint8&#39;, &#39;uintc&#39;, &#39;uintp&#39;, &#39;ulonglong&#39;, &#39;unicode_&#39;, &#39;uniform&#39;, &#39;uninstall_repl_displayhook&#39;, &#39;union1d&#39;, &#39;unique&#39;, &#39;unpackbits&#39;, &#39;unravel_index&#39;, &#39;unsignedinteger&#39;, &#39;unwrap&#39;, &#39;ushort&#39;, &#39;vander&#39;, &#39;var&#39;, &#39;vdot&#39;, &#39;vectorize&#39;, &#39;violinplot&#39;, &#39;viridis&#39;, &#39;vlines&#39;, &#39;void&#39;, &#39;void0&#39;, &#39;vonmises&#39;, &#39;vsplit&#39;, &#39;vstack&#39;, &#39;waitforbuttonpress&#39;, &#39;wald&#39;, &#39;weibull&#39;, &#39;where&#39;, &#39;who&#39;, &#39;window_hanning&#39;, &#39;window_none&#39;, &#39;winter&#39;, &#39;xcorr&#39;, &#39;xkcd&#39;, &#39;xlabel&#39;, &#39;xlim&#39;, &#39;xscale&#39;, &#39;xticks&#39;, &#39;ylabel&#39;, &#39;ylim&#39;, &#39;yscale&#39;, &#39;yticks&#39;, &#39;zeros&#39;, &#39;zeros_like&#39;, &#39;zipf&#39;] . &lt;/div&gt; | |",
            "url": "https://jmmerrell.github.io/ws/2021/07/30/youtube_guerito_project.html",
            "relUrl": "/2021/07/30/youtube_guerito_project.html",
            "date": " • Jul 30, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "How to get started on Kaggle Competitions",
            "content": ". 1. Understand the Data . The first step when you face a new data set is to take some time to know the data. In Kaggle competitions, you&#39;ll come across something like the sample below. . . On the competition&#39;s page, you can check the project description on Overview and you&#39;ll find useful information about the data set on the tab Data. In Kaggle competitions, it&#39;s common to have the training and test sets provided in separate files. On the same tab, there&#39;s usually a summary of the features you&#39;ll be working with and some basic statistics. It&#39;s crucial to understand which problem needs to be addressed and the data set we have at hand. . You can use the Kaggle notebooks to execute your projects, as they are similar to Jupyter Notebooks. . 2. Import the necessary libraries and data set . 2.1. Libraries . The libraries used in this project are the following. . import pandas as pd # Data analysis tool import numpy as np # Package for scientific computing from sklearn.model_selection import train_test_split # Splits arrays or matrices into random train and test subsets from sklearn.model_selection import KFold # Cross-validator from sklearn.model_selection import cross_validate # Evaluate metrics by cross-validation from sklearn.model_selection import GridSearchCV # Search over specified parameter values for an estimator from sklearn.compose import ColumnTransformer # Applies transformers to columns of DataFrames from sklearn.pipeline import Pipeline # Helps building a chain of transforms and estimators from sklearn.impute import SimpleImputer # Imputation transformer for completing missing values from sklearn.preprocessing import OneHotEncoder # Encode categorical features from sklearn.metrics import mean_absolute_error # One of many statistical measures of error from xgboost import XGBRegressor # Our model estimator . 2.2. Data set . The next step is to read the data set into a pandas DataFrame and obtain target vector y, which will be the column SalePrice, and predictors X, which, for now, will be the remaining columns. . X_full = pd.read_csv(&#39;https://raw.githubusercontent.com/rmpbastos/data_sets/main/housing_price_train.csv&#39;, index_col=&#39;Id&#39;) X_test_full = pd.read_csv(&#39;https://raw.githubusercontent.com/rmpbastos/data_sets/main/housing_price_test.csv&#39;, index_col=&#39;Id&#39;) # Obtain target vectors and predictors X = X_full.copy() y = X.SalePrice X.drop([&#39;SalePrice&#39;], axis=1, inplace=True) . To get an overview of the data, let&#39;s check the first rows and the size of the data set. . X.head() . MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating HeatingQC CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition . Id . 1 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2003 | 2003 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 196.0 | Gd | TA | PConc | Gd | TA | No | GLQ | 706 | Unf | 0 | 150 | 856 | GasA | Ex | Y | SBrkr | 856 | 854 | 0 | 1710 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 8 | Typ | 0 | NaN | Attchd | 2003.0 | RFn | 2 | 548 | TA | TA | Y | 0 | 61 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | . 2 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | FR2 | Gtl | Veenker | Feedr | Norm | 1Fam | 1Story | 6 | 8 | 1976 | 1976 | Gable | CompShg | MetalSd | MetalSd | None | 0.0 | TA | TA | CBlock | Gd | TA | Gd | ALQ | 978 | Unf | 0 | 284 | 1262 | GasA | Ex | Y | SBrkr | 1262 | 0 | 0 | 1262 | 0 | 1 | 2 | 0 | 3 | 1 | TA | 6 | Typ | 1 | TA | Attchd | 1976.0 | RFn | 2 | 460 | TA | TA | Y | 298 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | . 3 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | CollgCr | Norm | Norm | 1Fam | 2Story | 7 | 5 | 2001 | 2002 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 162.0 | Gd | TA | PConc | Gd | TA | Mn | GLQ | 486 | Unf | 0 | 434 | 920 | GasA | Ex | Y | SBrkr | 920 | 866 | 0 | 1786 | 1 | 0 | 2 | 1 | 3 | 1 | Gd | 6 | Typ | 1 | TA | Attchd | 2001.0 | RFn | 2 | 608 | TA | TA | Y | 0 | 42 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | . 4 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | Crawfor | Norm | Norm | 1Fam | 2Story | 7 | 5 | 1915 | 1970 | Gable | CompShg | Wd Sdng | Wd Shng | None | 0.0 | TA | TA | BrkTil | TA | Gd | No | ALQ | 216 | Unf | 0 | 540 | 756 | GasA | Gd | Y | SBrkr | 961 | 756 | 0 | 1717 | 1 | 0 | 1 | 0 | 3 | 1 | Gd | 7 | Typ | 1 | Gd | Detchd | 1998.0 | Unf | 3 | 642 | TA | TA | Y | 0 | 35 | 272 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | . 5 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | FR2 | Gtl | NoRidge | Norm | Norm | 1Fam | 2Story | 8 | 5 | 2000 | 2000 | Gable | CompShg | VinylSd | VinylSd | BrkFace | 350.0 | Gd | TA | PConc | Gd | TA | Av | GLQ | 655 | Unf | 0 | 490 | 1145 | GasA | Ex | Y | SBrkr | 1145 | 1053 | 0 | 2198 | 1 | 0 | 2 | 1 | 4 | 1 | Gd | 9 | Typ | 1 | TA | Attchd | 2000.0 | RFn | 3 | 836 | TA | TA | Y | 192 | 84 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | . X.shape . (1460, 79) . y.shape . (1460,) . We have 1,460 rows and 79 columns. Later on, we&#39;ll check these columns to verify which of them will be meaningful to the model. . In the next step, we&#39;ll split the data into training and validation sets. . 3. Training and validation data . It is crucial to break our data into a set for training the model and another one to validate the results. It&#39;s worth mentioning that we should never use the test data here. Our test set stays untouched until we are satisfied with our model&#39;s performance. . What we&#39;re going to do is taking the predictors X and target vector y and breaking them into training and validation sets. For that, we&#39;ll use scikit-learn&#39;s train_test_split. . X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0) . Checking the shape of our training and validation sets, we get the following. . print(f&#39;Shape of X_train_full: {X_train_full.shape}&#39;) print(f&#39;Shape of X_valid_full: {X_valid_full.shape}&#39;) print(f&#39;Shape of y_train: {y_train.shape}&#39;) print(f&#39;Shape of y_valid: {y_valid.shape}&#39;) . Shape of X_train_full: (1168, 79) Shape of X_valid_full: (292, 79) Shape of y_train: (1168,) Shape of y_valid: (292,) . 4. Analyze and prepare the data . Now, we start analyzing the data by checking some information about the features. . X.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1460 entries, 1 to 1460 Data columns (total 79 columns): # Column Non-Null Count Dtype -- -- 0 MSSubClass 1460 non-null int64 1 MSZoning 1460 non-null object 2 LotFrontage 1201 non-null float64 3 LotArea 1460 non-null int64 4 Street 1460 non-null object 5 Alley 91 non-null object 6 LotShape 1460 non-null object 7 LandContour 1460 non-null object 8 Utilities 1460 non-null object 9 LotConfig 1460 non-null object 10 LandSlope 1460 non-null object 11 Neighborhood 1460 non-null object 12 Condition1 1460 non-null object 13 Condition2 1460 non-null object 14 BldgType 1460 non-null object 15 HouseStyle 1460 non-null object 16 OverallQual 1460 non-null int64 17 OverallCond 1460 non-null int64 18 YearBuilt 1460 non-null int64 19 YearRemodAdd 1460 non-null int64 20 RoofStyle 1460 non-null object 21 RoofMatl 1460 non-null object 22 Exterior1st 1460 non-null object 23 Exterior2nd 1460 non-null object 24 MasVnrType 1452 non-null object 25 MasVnrArea 1452 non-null float64 26 ExterQual 1460 non-null object 27 ExterCond 1460 non-null object 28 Foundation 1460 non-null object 29 BsmtQual 1423 non-null object 30 BsmtCond 1423 non-null object 31 BsmtExposure 1422 non-null object 32 BsmtFinType1 1423 non-null object 33 BsmtFinSF1 1460 non-null int64 34 BsmtFinType2 1422 non-null object 35 BsmtFinSF2 1460 non-null int64 36 BsmtUnfSF 1460 non-null int64 37 TotalBsmtSF 1460 non-null int64 38 Heating 1460 non-null object 39 HeatingQC 1460 non-null object 40 CentralAir 1460 non-null object 41 Electrical 1459 non-null object 42 1stFlrSF 1460 non-null int64 43 2ndFlrSF 1460 non-null int64 44 LowQualFinSF 1460 non-null int64 45 GrLivArea 1460 non-null int64 46 BsmtFullBath 1460 non-null int64 47 BsmtHalfBath 1460 non-null int64 48 FullBath 1460 non-null int64 49 HalfBath 1460 non-null int64 50 BedroomAbvGr 1460 non-null int64 51 KitchenAbvGr 1460 non-null int64 52 KitchenQual 1460 non-null object 53 TotRmsAbvGrd 1460 non-null int64 54 Functional 1460 non-null object 55 Fireplaces 1460 non-null int64 56 FireplaceQu 770 non-null object 57 GarageType 1379 non-null object 58 GarageYrBlt 1379 non-null float64 59 GarageFinish 1379 non-null object 60 GarageCars 1460 non-null int64 61 GarageArea 1460 non-null int64 62 GarageQual 1379 non-null object 63 GarageCond 1379 non-null object 64 PavedDrive 1460 non-null object 65 WoodDeckSF 1460 non-null int64 66 OpenPorchSF 1460 non-null int64 67 EnclosedPorch 1460 non-null int64 68 3SsnPorch 1460 non-null int64 69 ScreenPorch 1460 non-null int64 70 PoolArea 1460 non-null int64 71 PoolQC 7 non-null object 72 Fence 281 non-null object 73 MiscFeature 54 non-null object 74 MiscVal 1460 non-null int64 75 MoSold 1460 non-null int64 76 YrSold 1460 non-null int64 77 SaleType 1460 non-null object 78 SaleCondition 1460 non-null object dtypes: float64(3), int64(33), object(43) memory usage: 912.5+ KB . From the summary above, we can observe that some columns have missing values. Let&#39;s take a closer look. . 4.1. Missing Values . missing_values = X.isnull().sum() missing_values = missing_values[missing_values &gt; 0].sort_values(ascending=False) print(missing_values) . PoolQC 1453 MiscFeature 1406 Alley 1369 Fence 1179 FireplaceQu 690 LotFrontage 259 GarageYrBlt 81 GarageType 81 GarageFinish 81 GarageQual 81 GarageCond 81 BsmtFinType2 38 BsmtExposure 38 BsmtFinType1 37 BsmtCond 37 BsmtQual 37 MasVnrArea 8 MasVnrType 8 Electrical 1 dtype: int64 . Some features have missing values counting for the majority of their entries. Checking the competition page, we find more details about the values for each feature, which will help us handle missing data. . For instance, in the columns PoolQC, MiscFeature, Alley, Fence, and FireplaceQu, the missing values mean that the house doesn&#39;t count with that specific feature, so, we&#39;ll fill the missing values with &quot;NA&quot;. All the null values in columns starting with Garage and Bsmt are related to houses that don&#39;t have a garage or basement, respectively. We&#39;ll fill those and the remaining null values with &quot;NA&quot; or the mean value, considering if the features are categorical or numerical. . 4.2. Preprocessing the categorical variables . Most machine learning models only work with numerical variables. Therefore, if we feed the model with categorical variables without preprocessing them first, we&#39;ll get an error. . There are several ways to deal with categorical values. Here, we&#39;ll use One-Hot Encoding, which will create new columns indicating the presence or absence of each value in the original data. . One issue of One-Hot Encoding is dealing with variables with numerous unique categories since it will create a new column for each unique category. Thus, this project will only include categorical variables with no more than 15 unique values. . categorical_cols = [col for col in X_train_full.columns if X_train_full[col].nunique() &lt;= 15 and X_train_full[col].dtype == &#39;object&#39;] # Select numeric values numeric_cols = [col for col in X_train_full.columns if X_train_full[col].dtype in [&#39;int64&#39;, &#39;float64&#39;]] # Keep selected columns my_columns = categorical_cols + numeric_cols X_train = X_train_full[my_columns].copy() X_valid = X_valid_full[my_columns].copy() X_test = X_test_full[my_columns].copy() . 4.3. Create a pipeline . Pipelines are a great way to keep the data modeling and preprocessing more organized and easier to understand. Creating a pipeline, we&#39;ll handle the missing values and the preprocessing covered in the previous two steps. . As defined above, numerical missing entries will be filled with the mean value while missing categorical variables will be filled with &quot;NA&quot;. Furthermore, categorical columns will also be preprocessed with One-Hot Encoding. . We are using SimpleImputer to fill in missing values and ColumnTransformer will help us to apply the numerical and categorical preprocessors in a single transformer. . numerical_transformer = SimpleImputer(strategy=&#39;mean&#39;) # Preprocessing categorical values categorical_transformer = Pipeline(steps=[ (&#39;imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;NA&#39;)), (&#39;onehot&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;)) ]) # Pack the preprocessors together preprocessor = ColumnTransformer(transformers=[ (&#39;num&#39;, numerical_transformer, numeric_cols), (&#39;cat&#39;, categorical_transformer, categorical_cols) ]) . 5. Define a model . Now that we have bundled our preprocessors in a pipeline, we can define a model. In this article, we are working with XGBoost, one of the most effective machine learning algorithms, that presents great results in many Kaggle competitions. As a metric of evaluation, we are using the Mean Absolute Error. . model = XGBRegressor(verbosity=0, random_state=0) # Pack preprocessing and modeling together in a pipeline my_pipeline = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;model&#39;, model) ]) # Preprocessing of training data, fit model my_pipeline.fit(X_train, y_train) # Preprocessing of validation data, get predictions preds = my_pipeline.predict(X_valid) print(&#39;MAE:&#39;, mean_absolute_error(y_valid, preds)) . MAE: 16706.181988441782 . 6. Cross-validation . Using Cross-Validation can yield better results. Instead of simply using the training and test sets, cross-validation will run our model on different subsets of the data to get multiple measures of model quality. . We&#39;ll use the cross-validator KFold in its default setup to split the training data into 5 folds. Then, each fold will be used once as validation while the remaining folds will form the training set. After that, cross-validate will evaluate the metrics. In this case, we&#39;re using the Mean Absolute Error. . kfold = KFold(shuffle=True, random_state=0) # Evaluating the Mean Absolute Error scores = cross_validate(my_pipeline, X_train, y_train, scoring=&#39;neg_mean_absolute_error&#39;, cv=kfold) # Multiply by -1 since sklearn calculates negative MAE print(&#39;Average MAE score:&#39;, (scores[&#39;test_score&#39;] * -1).mean()) . Average MAE score: 16168.894833206665 . With cross-validation we could improve our score, reducing the error. In the next step, we&#39;ll try to further improve the model, optimizing some hyperparameters. . 7. Hyperparameter tuning . XGBoost in its default setup usually yields great results, but it also has plenty of hyperparameters that can be optimized to improve the model. Here, we&#39;ll use a method called GridSearchCV which will search over specified parameter values and return the best ones. Once again, we&#39;ll utilize the pipeline and the cross-validator KFold defined above. . GridSearchCV will perform an exhaustive search over parameters, which can demand a lot of computational power and take a lot of time to be finished. We can speed up the process a little bit by setting the parameter n_jobs to -1, which means that the machine will use all processors on the task. . &quot;&quot;&quot; To pass parameter in a pipeline, we should add the names of the steps and the parameter name separated by a ‘__’. Ex: Instead of &#39;n_estimators&#39;, we should set &#39;model__n_estimators&#39;. https://github.com/scikit-learn/scikit-learn/issues/18472 &quot;&quot;&quot; # parameters to be searched over param_grid = {&#39;model__n_estimators&#39;: [10, 50, 100, 200, 400, 600], &#39;model__max_depth&#39;: [2, 3, 5, 7, 10], &#39;model__min_child_weight&#39;: [0.0001, 0.001, 0.01], &#39;model__learning_rate&#39;: [0.01, 0.1, 0.5, 1]} # find the best parameter kfold = KFold(shuffle=True, random_state=0) grid_search = GridSearchCV(my_pipeline, param_grid, scoring=&#39;neg_mean_absolute_error&#39;, cv=kfold, n_jobs=-1) grid_result = grid_search.fit(X_train, y_train) . print(&#39;Best result:&#39;, round((grid_result.best_score_ * -1), 2), &#39;for&#39;, grid_result.best_params_) . Best result: 15750.17 for {&#39;model__learning_rate&#39;: 0.1, &#39;model__max_depth&#39;: 3, &#39;model__min_child_weight&#39;: 0.0001, &#39;model__n_estimators&#39;: 400} . 8. Generate test predictions . After tuning some hyperparameters, it&#39;s time to go over the modeling process again to make predictions on the test set. We&#39;ll define our final model based on the optimized values provided by GridSearchCV. . final_model = XGBRegressor(n_estimators=400, max_depth=3, min_child_weight=0.0001, learning_rate=0.1, verbosity=0, random_state=0 ) # Create a pipeline final_pipeline = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;final_model&#39;, final_model) ]) # Fit the model final_pipeline.fit(X_train, y_train) # Get predictions on the test set final_prediction = final_pipeline.predict(X_test) . 9. Submit your results . We&#39;re almost there! The machine learning modeling is done, but we still need to submit our results to have our score recorded. . This step is quite simple. We need to create a .csv file containing the predictions. This file consists of a DataFrame with two columns. In this case, one column for &quot;Id&quot; and the other one for the test predictions on the target feature. . output = pd.DataFrame({&#39;Id&#39;: X_test.index, &#39;SalePrice&#39;: final_prediction}) output.to_csv(&#39;submission.csv&#39;, index=False) . 10. Join the competition . Finally, we just need to join the competition. Please follow the steps below, according to Kaggle&#39;s instructions. . Start by accessing the competition page and clicking on Join Competition. | In your Kaggle notebook, click on the blue Save Version button in the top right corner of the window. | A pop-up window will show up. Select the option Save and Run All and then click on the blue Save button. | A new pop-up shows up in the bottom left corner while your notebook is running. When it stops running, click on the number to the right of the Save Version button. You should click on the ellipsis (...) to the right of the most recent notebook version, and select Open in Viewer. This brings you into view mode of the same page. | Now, click on the Output tab on the right of the screen. Then, click on the blue Submit button to submit your results to the leaderboard. | . After submitting, you can check your score and position on the leaderboard. . . Conclusion . This article was intended to be instructive, helping data science beginners to structure their first projects on Kaggle in simple steps. With this straightforward approach, I&#39;ve got a score of 14,778.87, which ranked this project in the Top 7%. . After further studying, you can go back on past projects and try to enhance their performance, using new skills you&#39;ve learned. To improve this project, we could investigate and treat the outliers more closely, apply a different approach to missing values, or do some feature engineering, for instance. . My advice to beginners is to keep it simple when starting out. Instead of aiming at the &quot;perfect&quot; model, focus on completing the project, applying your skills correctly, and learning from your mistakes, understanding where and why you messed things up. The data science community is on constant expansion and there&#39;s plenty of more experienced folks willing to help on websites like Kaggle or Stack Overflow. Try to learn from their past mistakes as well! With practice and discipline, it&#39;s just a matter of time to start building more elaborate projects and climb up the ranking of Kaggle&#39;s competitions. .",
            "url": "https://jmmerrell.github.io/ws/2021/07/30/Housing_Prices_Competition.html",
            "relUrl": "/2021/07/30/Housing_Prices_Competition.html",
            "date": " • Jul 30, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Churn Prediction with Machine Learning",
            "content": ". About the data . The dataset was provided by the IBM Developer Platform and is available here. Some information, such as the company name or private client data, was kept anonymous for the sake of confidentiality and will not affect the model&#39;s performance. . Let&#39;s import the necessary libraries and print the first five rows to start examining the dataset. . import pandas as pd import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import seaborn as sns; sns.set() from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from imblearn.under_sampling import RandomUnderSampler from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report from sklearn.svm import SVC from sklearn.linear_model import LogisticRegression from xgboost import XGBClassifier, plot_importance from sklearn.model_selection import cross_validate from sklearn.model_selection import GridSearchCV %matplotlib inline . COLOR = &#39;#ababab&#39; mpl.rcParams[&#39;figure.titlesize&#39;] = 16 mpl.rcParams[&#39;text.color&#39;] = &#39;black&#39; mpl.rcParams[&#39;axes.labelcolor&#39;] = COLOR mpl.rcParams[&#39;xtick.color&#39;] = COLOR mpl.rcParams[&#39;ytick.color&#39;] = COLOR mpl.rcParams[&#39;grid.color&#39;] = COLOR mpl.rcParams[&#39;grid.alpha&#39;] = 0.1 . DATA_PATH = &quot;https://raw.githubusercontent.com/carlosfab/dsnp2/master/datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv&quot; df = pd.read_csv(DATA_PATH) . df.head() . customerID gender SeniorCitizen Partner Dependents tenure PhoneService MultipleLines InternetService OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies Contract PaperlessBilling PaymentMethod MonthlyCharges TotalCharges Churn . 0 7590-VHVEG | Female | 0 | Yes | No | 1 | No | No phone service | DSL | No | Yes | No | No | No | No | Month-to-month | Yes | Electronic check | 29.85 | 29.85 | No | . 1 5575-GNVDE | Male | 0 | No | No | 34 | Yes | No | DSL | Yes | No | Yes | No | No | No | One year | No | Mailed check | 56.95 | 1889.5 | No | . 2 3668-QPYBK | Male | 0 | No | No | 2 | Yes | No | DSL | Yes | Yes | No | No | No | No | Month-to-month | Yes | Mailed check | 53.85 | 108.15 | Yes | . 3 7795-CFOCW | Male | 0 | No | No | 45 | No | No phone service | DSL | Yes | No | Yes | Yes | No | No | One year | No | Bank transfer (automatic) | 42.30 | 1840.75 | No | . 4 9237-HQITU | Female | 0 | No | No | 2 | Yes | No | Fiber optic | No | No | No | No | No | No | Month-to-month | Yes | Electronic check | 70.70 | 151.65 | Yes | . We have a row for each customer and the features&#39; categories are described below: . Demographic customer information . gender | SeniorCitizen | Partner | Dependents | . Services that each customer has signed up for . PhoneService | MultipleLines | InternetService | OnlineSecurity | OnlineBackup | DeviceProtection | TechSupport | StreamingTV | StreamingMovies | . Customer account information . tenure | Contract | PaperlessBilling | PaymentMethod | MonthlyCharges | TotalCharges | . Customers who left within the last month (This is the feature our model is going to predict) . Churn | . Data Analysis . We will start the analysis by taking a look at the size of the dataset and examining the features. . print(f&quot;Rows: &quot;, df.shape[0]) print(f&quot;Columns: &quot;, df.shape[1]) . Rows: 7043 Columns: 21 . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 7043 entries, 0 to 7042 Data columns (total 21 columns): # Column Non-Null Count Dtype -- -- 0 customerID 7043 non-null object 1 gender 7043 non-null object 2 SeniorCitizen 7043 non-null int64 3 Partner 7043 non-null object 4 Dependents 7043 non-null object 5 tenure 7043 non-null int64 6 PhoneService 7043 non-null object 7 MultipleLines 7043 non-null object 8 InternetService 7043 non-null object 9 OnlineSecurity 7043 non-null object 10 OnlineBackup 7043 non-null object 11 DeviceProtection 7043 non-null object 12 TechSupport 7043 non-null object 13 StreamingTV 7043 non-null object 14 StreamingMovies 7043 non-null object 15 Contract 7043 non-null object 16 PaperlessBilling 7043 non-null object 17 PaymentMethod 7043 non-null object 18 MonthlyCharges 7043 non-null float64 19 TotalCharges 7043 non-null object 20 Churn 7043 non-null object dtypes: float64(1), int64(2), object(18) memory usage: 1.1+ MB . df.nunique() . customerID 7043 gender 2 SeniorCitizen 2 Partner 2 Dependents 2 tenure 73 PhoneService 2 MultipleLines 3 InternetService 3 OnlineSecurity 3 OnlineBackup 3 DeviceProtection 3 TechSupport 3 StreamingTV 3 StreamingMovies 3 Contract 3 PaperlessBilling 2 PaymentMethod 4 MonthlyCharges 1585 TotalCharges 6531 Churn 2 dtype: int64 . Observe that we have 21 features for 7043 clients. Among all features, 3 of them are numerical, 10 are categorical and the remaining can be considered binary. . The feature customerID represents a unique value for each row. Eliminating it will help us clean the dataset without spoiling the model. . df.drop(&#39;customerID&#39;, axis=1, inplace=True) . Also, notice that the variable TotalCharges is wrongly classified as a string. . Before moving further, we need to convert TotalCharges to float. However, when doing so the following error is raised: Unable to parse string &quot; &quot; at position 488. . Although all features were displayed above as non-null, we need to verify TotalCharges for whitespaces counting as strings. . print(f&quot;Cells filled with whitespace in &#39;TotalCharges&#39; (Before): &quot;, len(df[df[&#39;TotalCharges&#39;] == &#39; &#39;])) . Cells filled with whitespace in &#39;TotalCharges&#39; (Before): 11 . We should deal with these 11 blank fields before converting the whole column to numerical. First, the blank cells will be replaced by null values. In the following step, they will be filled with the median value of TotalCharges. . df.loc[df[&#39;TotalCharges&#39;] == &#39; &#39;, &#39;TotalCharges&#39;] = np.nan print(f&quot;Cells filled with whitespace in &#39;TotalCharges&#39; (After): &quot;, len(df[df[&#39;TotalCharges&#39;] == &#39; &#39;])) . Cells filled with whitespace in &#39;TotalCharges&#39; (After): 0 . TotalCharges_median = df[&#39;TotalCharges&#39;].median() df[&#39;TotalCharges&#39;].fillna(TotalCharges_median, inplace=True) . Now we can proceed and convert the TotalCharges column to float. . df[&#39;TotalCharges&#39;] = df[&#39;TotalCharges&#39;].astype(float) . Now that we have dealt with TotalCharges, let&#39;s check some statistical details of the numerical features and see if there is any evidence of outliers. . df.describe() . SeniorCitizen tenure MonthlyCharges TotalCharges . count 7043.000000 | 7043.000000 | 7043.000000 | 7043.000000 | . mean 0.162147 | 32.371149 | 64.761692 | 2281.916928 | . std 0.368612 | 24.559481 | 30.090047 | 2265.270398 | . min 0.000000 | 0.000000 | 18.250000 | 18.800000 | . 25% 0.000000 | 9.000000 | 35.500000 | 402.225000 | . 50% 0.000000 | 29.000000 | 70.350000 | 1397.475000 | . 75% 0.000000 | 55.000000 | 89.850000 | 3786.600000 | . max 1.000000 | 72.000000 | 118.750000 | 8684.800000 | . fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(9, 7)) fig.suptitle(&quot;Boxplot of &#39;Monthly Charges&#39; and &#39;Total Charges&#39;&quot;) boxprops = whiskerprops = capprops = medianprops = dict(linewidth=1) sns.boxplot(df[&#39;MonthlyCharges&#39;], orient=&#39;v&#39;, color=&#39;#488ab5&#39;, ax=ax[0], boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops, medianprops=medianprops) ax[0].set_facecolor(&#39;#f5f5f5&#39;) ax[0].set_yticks([20, 70, 120]) sns.boxplot(df[&#39;TotalCharges&#39;], orient=&#39;v&#39;, color=&#39;#488ab5&#39;, ax=ax[1], boxprops=boxprops, whiskerprops=whiskerprops, capprops=capprops, medianprops=medianprops) ax[1].set_facecolor(&#39;#f5f5f5&#39;) ax[1].set_yticks([0, 4000, 8000]) plt.tight_layout(pad=4.0); . At first glance, everything looks ok with our numerical features. We didn&#39;t spot any outlier in MonthlyCharges nor TotalCharges. . Before starting to apply some feature engineering, let&#39;s check the value distribution for our target variable Churn. . print(df[&#39;Churn&#39;].value_counts()) print(&#39; nTotal Churn Rate: {:.2%}&#39;.format(df[df[&#39;Churn&#39;] == &#39;Yes&#39;].shape[0] / df.shape[0])) . No 5174 Yes 1869 Name: Churn, dtype: int64 Total Churn Rate: 26.54% . We are dealing with an unbalanced dataset. Observe that the churn rate is 26.5%, meaning that the quantity of &quot;No&quot; values is substantially higher than that of &quot;Yes&quot; values. . Data Preparation . Ok, we have already made some minor adjustments to the dataset, such as eliminating customerID and converting TotalCharges to numerical. . Now, we are going to perform a closer examination of the features to identify if any further adjustments can be made to improve our machine learning model. Let&#39;s check the unique values for the categorical features. . def unique_values(): cat_columns = np.unique(df.select_dtypes(&#39;object&#39;).columns) for i in cat_columns: print(i, df[i].unique()) unique_values() . Churn [&#39;No&#39; &#39;Yes&#39;] Contract [&#39;Month-to-month&#39; &#39;One year&#39; &#39;Two year&#39;] Dependents [&#39;No&#39; &#39;Yes&#39;] DeviceProtection [&#39;No&#39; &#39;Yes&#39; &#39;No internet service&#39;] InternetService [&#39;DSL&#39; &#39;Fiber optic&#39; &#39;No&#39;] MultipleLines [&#39;No phone service&#39; &#39;No&#39; &#39;Yes&#39;] OnlineBackup [&#39;Yes&#39; &#39;No&#39; &#39;No internet service&#39;] OnlineSecurity [&#39;No&#39; &#39;Yes&#39; &#39;No internet service&#39;] PaperlessBilling [&#39;Yes&#39; &#39;No&#39;] Partner [&#39;Yes&#39; &#39;No&#39;] PaymentMethod [&#39;Electronic check&#39; &#39;Mailed check&#39; &#39;Bank transfer (automatic)&#39; &#39;Credit card (automatic)&#39;] PhoneService [&#39;No&#39; &#39;Yes&#39;] StreamingMovies [&#39;No&#39; &#39;Yes&#39; &#39;No internet service&#39;] StreamingTV [&#39;No&#39; &#39;Yes&#39; &#39;No internet service&#39;] TechSupport [&#39;No&#39; &#39;Yes&#39; &#39;No internet service&#39;] gender [&#39;Female&#39; &#39;Male&#39;] . Some of the columns with 3 unique values could be treated as binary. To illustrate, the columns StreamingTV and TechSupport have values &quot;No&quot;, &quot;Yes&quot;, and &quot;No internet service&quot;. In these cases, &quot;No internet service&quot; could be considered as &quot;No&quot;. . to_binary = [&#39;DeviceProtection&#39;, &#39;OnlineBackup&#39;, &#39;OnlineSecurity&#39;, &#39;StreamingMovies&#39;, &#39;StreamingTV&#39;, &#39;TechSupport&#39;] for i in to_binary: df.loc[df[i].isin([&#39;No internet service&#39;]), i] = &#39;No&#39; . Let&#39;s check it out to see how the categorical features look like now. . unique_values() . Churn [&#39;No&#39; &#39;Yes&#39;] Contract [&#39;Month-to-month&#39; &#39;One year&#39; &#39;Two year&#39;] Dependents [&#39;No&#39; &#39;Yes&#39;] DeviceProtection [&#39;No&#39; &#39;Yes&#39;] InternetService [&#39;DSL&#39; &#39;Fiber optic&#39; &#39;No&#39;] MultipleLines [&#39;No phone service&#39; &#39;No&#39; &#39;Yes&#39;] OnlineBackup [&#39;Yes&#39; &#39;No&#39;] OnlineSecurity [&#39;No&#39; &#39;Yes&#39;] PaperlessBilling [&#39;Yes&#39; &#39;No&#39;] Partner [&#39;Yes&#39; &#39;No&#39;] PaymentMethod [&#39;Electronic check&#39; &#39;Mailed check&#39; &#39;Bank transfer (automatic)&#39; &#39;Credit card (automatic)&#39;] PhoneService [&#39;No&#39; &#39;Yes&#39;] StreamingMovies [&#39;No&#39; &#39;Yes&#39;] StreamingTV [&#39;No&#39; &#39;Yes&#39;] TechSupport [&#39;No&#39; &#39;Yes&#39;] gender [&#39;Female&#39; &#39;Male&#39;] . As an example, let&#39;s plot graphs to see the churn distribution for some of the features. . First, we need to convert Churn values to numerical. . df.loc[df[&#39;Churn&#39;] == &#39;No&#39;,&#39;Churn&#39;] = 0 df.loc[df[&#39;Churn&#39;] == &#39;Yes&#39;,&#39;Churn&#39;] = 1 df[&#39;Churn&#39;] = df[&#39;Churn&#39;].astype(int) . fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(18,10)) sns.barplot(df[&#39;gender&#39;], df[&#39;Churn&#39;], color=&#39;#488ab5&#39;, ci=None, ax=ax[0][0]) ax[0][0].set_facecolor(&#39;#f5f5f5&#39;) ax[0][0].set_ylim(0,1) ax[0][0].set_xlabel(None) ax[0][0].set_title(&#39;Gender&#39;) sns.barplot(df[&#39;Dependents&#39;], df[&#39;Churn&#39;], color=&#39;#488ab5&#39;, ci=None, ax=ax[0][1]) ax[0][1].set_facecolor(&#39;#f5f5f5&#39;) ax[0][1].tick_params(labelleft=False) ax[0][1].set_ylim(0,1) ax[0][1].set_ylabel(None) ax[0][1].set_xlabel(None) ax[0][1].set_title(&#39;Dependents&#39;) sns.barplot(df[&#39;InternetService&#39;], df[&#39;Churn&#39;], color=&#39;#488ab5&#39;, ci=None, ax=ax[0][2]) ax[0][2].set_facecolor(&#39;#f5f5f5&#39;) ax[0][2].tick_params(labelleft=False) ax[0][2].set_ylim(0,1) ax[0][2].set_ylabel(None) ax[0][2].set_xlabel(None) ax[0][2].set_title(&#39;Internet Service&#39;) sns.barplot(df[&#39;DeviceProtection&#39;], df[&#39;Churn&#39;], color=&#39;#488ab5&#39;, ci=None, ax=ax[1][0]) ax[1][0].set_facecolor(&#39;#f5f5f5&#39;) ax[1][0].set_ylim(0,1) ax[1][0].set_xlabel(None) ax[1][0].set_title(&#39;Device Protection&#39;) sns.barplot(df[&#39;OnlineSecurity&#39;], df[&#39;Churn&#39;], color=&#39;#488ab5&#39;, ci=None, ax=ax[1][1]) ax[1][1].set_facecolor(&#39;#f5f5f5&#39;) ax[1][1].tick_params(labelleft=False) ax[1][1].set_ylim(0,1) ax[1][1].set_ylabel(None) ax[1][1].set_xlabel(None) ax[1][1].set_title(&#39;Online Security&#39;) sns.barplot(df[&#39;Contract&#39;], df[&#39;Churn&#39;], color=&#39;#488ab5&#39;, ci=None, ax=ax[1][2]) ax[1][2].set_facecolor(&#39;#f5f5f5&#39;) ax[1][2].tick_params(labelleft=False) ax[1][2].set_ylim(0,1) ax[1][2].set_ylabel(None) ax[1][2].set_xlabel(None) ax[1][2].set_title(&#39;Contract&#39;) plt.tight_layout(pad=4.0) . Looking at the example above, we can interpret that Gender probably won&#39;t be a meaningful variable to the model, as the churn rate is quite similar for both male and female customers. On the other hand, clients with dependents are less prone to stop doing business with the company. . As for internet service, customers with fiber optic plans are more likely to quit. Their churn rate is more than double that of DSL and no internet users. . Turning to protection services, clients with device protection and online security plans are more likely to maintain their contracts. . Lastly, the type of contract might be a valuable feature for the model. Notice that the churn rate for month-to-month contracts is considerably higher than that of one year and two-year contracts. . Considering that most Machine Learning algorithms work better with numerical inputs, we&#39;ll preprocess our data using the following techniques to convert categorical features into numerical values: . LabelEncoder for binary | get_dummies for other categorical variables | . bin_var = [col for col in df.columns if len(df[col].unique()) == 2 and col != &#39;Churn&#39;] # list of categorical variables cat_var = [col for col in df.select_dtypes([&#39;object&#39;]).columns.tolist() if col not in bin_var] # apply Label Encoding for binaries le = LabelEncoder() for col in bin_var: df[col] = le.fit_transform(df[col]) # apply get_dummies for categorical df = pd.get_dummies(df, columns=cat_var) df.head() . gender SeniorCitizen Partner Dependents tenure PhoneService OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies PaperlessBilling MonthlyCharges TotalCharges Churn MultipleLines_No MultipleLines_No phone service MultipleLines_Yes InternetService_DSL InternetService_Fiber optic InternetService_No Contract_Month-to-month Contract_One year Contract_Two year PaymentMethod_Bank transfer (automatic) PaymentMethod_Credit card (automatic) PaymentMethod_Electronic check PaymentMethod_Mailed check . 0 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 29.85 | 29.85 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . 1 1 | 0 | 0 | 0 | 34 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 56.95 | 1889.50 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | . 2 1 | 0 | 0 | 0 | 2 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 53.85 | 108.15 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | . 3 1 | 0 | 0 | 0 | 45 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 42.30 | 1840.75 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 70.70 | 151.65 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | . Now that we have our features preprocessed, the data is ready for the machine learning model. . Machine Learning Models . The first thing we need to do is splitting the data into training and test sets. . X = df.drop(&#39;Churn&#39;, axis=1) # target vector y = df[&#39;Churn&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y) . Remember that we are dealing with an unbalanced dataset, as we determined a few steps above. . To manage the situation, we&#39;ll standardize the features of the training set using StardardScaler and then apply RandomUnderSampler, which is a &quot;way to balance the data by randomly selecting a subset of data for the targeted classes&quot;, according to the official documentation. . scaler = StandardScaler().fit(X_train) X_train = scaler.transform(X_train) rus = RandomUnderSampler() X_train_rus, y_train_rus = rus.fit_sample(X_train, y_train) . /usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) . Now that the data is standardized and balanced, the following models will be used and we&#39;ll determine which one shows the better results: . SVC (Support Vector Classifier) | Logistic Regression | XGBoost | . To evaluate the effectiveness of these models, we could use Precision or Recall. Precision will give us the proportion of positive identifications that were indeed correct, while recall will determine the proportion of real positives that were correctly identified. . Considering the problem we are trying to solve, Recall will be more suitable for this study, as the objective here is to identify the maximum number of clients that are actually prone to stop doing business with the company, even if some &quot;non-churners&quot; are wrongly identified as &quot;churners&quot;. That is to say, in our case, it is better to pursue a smaller number of False Negatives possible. . svc = SVC() lr = LogisticRegression() xgb = XGBClassifier() model = [] cross_val = [] recall = [] for i in (svc, lr, xgb): model.append(i.__class__.__name__) cross_val.append(cross_validate(i, X_train_rus, y_train_rus, scoring=&#39;recall&#39;)) for d in range(len(cross_val)): recall.append(cross_val[d][&#39;test_score&#39;].mean()) . We used Cross-Validation to get better results. Instead of simply splitting the data into a train and test set, the cross_validate method splits our training data into k number of Folds, making better use of the data. In our case, we performed a 5-fold cross-validation, as we let the default k value. . model_recall = pd.DataFrame pd.DataFrame(data=recall, index=model, columns=[&#39;Recall&#39;]) . Recall . SVC 0.798727 | . LogisticRegression 0.803062 | . XGBClassifier 0.807402 | . Notice that all 3 models provided similar results, with a recall rate of about 80%. We&#39;ll now tune some hyperparameters on the models to see if we can achieve higher recall values. The method utilized here is GridSearchCV, which will search over specified parameter values for each estimator. Each model has a variety of parameters that can be tuned, but we are only adjusting those with more potential to impact the prediction (specified in the param_grid parameter), while the remainder can be left to their default values. . # parameters to be searched param_grid = {&#39;kernel&#39;: [&#39;linear&#39;, &#39;poly&#39;, &#39;rbf&#39;, &#39;sigmoid&#39;], &#39;C&#39;: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]} # find the best parameters grid_search = GridSearchCV(svc, param_grid, scoring=&#39;recall&#39;) grid_result = grid_search.fit(X_train_rus, y_train_rus) print(f&#39;Best result: {grid_result.best_score_} for {grid_result.best_params_}&#39;) . Best result: 0.961784795989923 for {&#39;C&#39;: 0.01, &#39;kernel&#39;: &#39;poly&#39;} . Notice how effective hyperparameter tuning can be. We searched over different values for C and kernel and we got an increased recall of 96%, for C = 0.01 and kernel type &quot;poly&quot;. . # parameters to be searched param_grid = {&#39;solver&#39;: [&#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;liblinear&#39;], &#39;C&#39;: [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]} # find the best parameters grid_search = GridSearchCV(lr, param_grid, scoring=&#39;recall&#39;) grid_result = grid_search.fit(X_train_rus, y_train_rus) print(f&#39;Best result: {grid_result.best_score_} for {grid_result.best_params_}&#39;) . Best result: 0.8809833000025972 for {&#39;C&#39;: 0.0001, &#39;solver&#39;: &#39;liblinear&#39;} . Turning to the Logistic Regression, we achieved a better recall as well, with 88% for C = 0.0001 and solver = &quot;liblinear&quot;. . Finally, let&#39;s make some adjustments to the XGBoost estimator. XGBoost is known for being one of the most effective Machine Learning algorithms, due to its great performance on structured and tabular datasets on classification and regression predictive modeling problems. It is highly customizable and counts with a higher range of parameters to be tuned. . In the first step, we are going to determine the optimal number of trees in the XGBoost model, searching over values for the n_estimators argument. . # parameter to be searched param_grid = {&#39;n_estimators&#39;: range(0,1000,25)} # find the best parameter grid_search = GridSearchCV(xgb, param_grid, scoring=&#39;recall&#39;) grid_result = grid_search.fit(X_train_rus, y_train_rus) print(f&#39;Best result: {grid_result.best_score_} for {grid_result.best_params_}&#39;) . Best result: 0.8276030439186558 for {&#39;n_estimators&#39;: 25} . We can already notice an improvement in the recall rate. Now that we have determined the better n_estimators value, we can move on and search over two relevant parameters, max_depth, and min_child_weight. . xgb = XGBClassifier(n_estimators=25) # parameters to be searched param_grid = {&#39;max_depth&#39;: range(1,8,1), &#39;min_child_weight&#39;: np.arange(0.0001, 0.5, 0.001)} # find the best parameters grid_search = GridSearchCV(xgb, param_grid, scoring=&#39;recall&#39;, n_jobs=-1) grid_result = grid_search.fit(X_train_rus, y_train_rus) print(f&#39;Best result: {grid_result.best_score_} for {grid_result.best_params_}&#39;) . Best result: 0.8679998961119912 for {&#39;max_depth&#39;: 1, &#39;min_child_weight&#39;: 0.0001} . In the following step, we&#39;ll determine the best value for gamma, an important parameter used to control the model&#39;s tendency to overfit. . xgb = XGBClassifier(n_estimators=25, max_depth=1, min_child_weight=0.0001) # parameter to be searched param_grid = {&#39;gama&#39;: np.arange(0.0,20.0,0.05)} # find the best parameters grid_search = GridSearchCV(xgb, param_grid, scoring=&#39;recall&#39;, n_jobs=-1) grid_result = grid_search.fit(X_train_rus, y_train_rus) print(f&#39;Best result: {grid_result.best_score_} for {grid_search.best_params_}&#39;) . Best result: 0.8679998961119912 for {&#39;gama&#39;: 0.0} . Finally, we&#39;ll search for the optimal learning_rate value. . xgb = XGBClassifier(n_estimators=25, max_depth=1, min_child_weight=0.0001, gama=0.0) # parameter to be searched param_grid = {&#39;learning_rate&#39;: [0.0001, 0.01, 0.1, 1]} # find the best parameter grid_search = GridSearchCV(xgb, param_grid, scoring=&#39;recall&#39;) grid_result = grid_search.fit(X_train_rus, y_train_rus) print(f&#39;Best result: {grid_search.best_score_} for {grid_search.best_params_}&#39;) . Best result: 0.8889073578682181 for {&#39;learning_rate&#39;: 0.0001} . After tuning parameters for SVC, Logistic Regression, and XGBoost, we noticed improvements in all three models. Now, let&#39;s draw a confusion matrix for each of these algorithms to visualize their performance on the test set. . svc = SVC(kernel=&#39;poly&#39;, C=1.0) svc.fit(X_train_rus, y_train_rus) # prediction X_test_svc = scaler.transform(X_test) y_pred_svc = svc.predict(X_test_svc) # classification report print(classification_report(y_test, y_pred_svc)) # confusion matrix fig, ax = plt.subplots() sns.heatmap(confusion_matrix(y_test, y_pred_svc, normalize=&#39;true&#39;), annot=True, ax=ax) ax.set_title(&#39;Confusion Matrix&#39;) ax.set_ylabel(&#39;Real Value&#39;) ax.set_xlabel(&#39;Predicted Value&#39;) plt.show() . precision recall f1-score support 0 0.91 0.69 0.78 1278 1 0.49 0.81 0.62 483 accuracy 0.72 1761 macro avg 0.70 0.75 0.70 1761 weighted avg 0.79 0.72 0.74 1761 . lr = LogisticRegression(solver=&#39;liblinear&#39;, C=0.0001) lr.fit(X_train_rus, y_train_rus) # prediction X_test_lr = scaler.transform(X_test) y_pred_lr = lr.predict(X_test_lr) # classification report print(classification_report(y_test, y_pred_lr)) # confusion matrix fig, ax = plt.subplots() sns.heatmap(confusion_matrix(y_test, y_pred_lr, normalize=&#39;true&#39;), annot=True, ax=ax) ax.set_title(&#39;Confusion Matrix&#39;) ax.set_ylabel(&#39;Real Value&#39;) ax.set_xlabel(&#39;Predicted Value&#39;) plt.show() . precision recall f1-score support 0 0.93 0.61 0.74 1278 1 0.46 0.87 0.60 483 accuracy 0.68 1761 macro avg 0.69 0.74 0.67 1761 weighted avg 0.80 0.68 0.70 1761 . xgb = XGBClassifier(learning_rate=0.0001, n_estimators=25, max_depth=1, min_child_weight=0.0001, gamma=0) xgb.fit(X_train_rus, y_train_rus) # prediction X_test_xgb = scaler.transform(X_test) y_pred_xgb = xgb.predict(X_test_xgb) # classification report print(classification_report(y_test, y_pred_xgb)) # confusion matrix fig, ax = plt.subplots() sns.heatmap(confusion_matrix(y_test, y_pred_xgb, normalize=&#39;true&#39;), annot=True, ax=ax) ax.set_title(&#39;Confusion Matrix&#39;) ax.set_ylabel(&#39;Real Value&#39;) ax.set_xlabel(&#39;Predicted Value&#39;) plt.show() . precision recall f1-score support 0 0.92 0.57 0.70 1278 1 0.43 0.88 0.58 483 accuracy 0.65 1761 macro avg 0.68 0.72 0.64 1761 weighted avg 0.79 0.65 0.67 1761 . After running the algorithms on the test set, we have a display on how the model&#39;s performance can be improved when we adjust some parameters. The three models had gains in recall rate after tuning, with XGBoost presenting the best recall rate among them. . Conclusion . The purpose of this project was to develop a model that would be able to determine churning clients from a telecom company, as efficiently as possible. . Being able to identify potential churners in advance allows the company to develop strategies to prevent customers from leaving the client base. With this data in hand, companies can offer incentives, like discounts or loyalty programs, or provide additional services in an attempt to reduce the churn rate. . Another point that is worth mentioning is the importance of tuning hyperparameters, adjusting the ML algorithm to achieve better results. All three models improved their recall rate after parameter tuning. . XGBoost has been proving its effectiveness on Data Science projects for a while, and, in this project, it provided the best results among the models. For that reason, XGBoost algorithm would be our choice for the problem presented here. .",
            "url": "https://jmmerrell.github.io/ws/2021/07/30/Churn_Prediction.html",
            "relUrl": "/2021/07/30/Churn_Prediction.html",
            "date": " • Jul 30, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jmmerrell.github.io/ws/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jmmerrell.github.io/ws/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jmmerrell.github.io/ws/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jmmerrell.github.io/ws/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}